[{"categories":["command"],"content":"CFSSL 简介 CFSSL 是 CloudFlare 开源的一款 PKI/TLS 瑞士军刀工具。 CFSSL 既是命令行工具，又是用于签名，验证和捆绑 TLS 证书的 HTTP API 服务器。 使用 Go 1.12+ 语言编写。 官方源码仓库: https://github.com/cloudflare/cfssl ","date":"2021-04-22","objectID":"/posts/cfssl/:1:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["command"],"content":"安装 cfssl wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -O /usr/local/bin/cfssl-json wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -O /usr/local/bin/cfssl wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -O /usr/local/bin/cfssl-certinfo chmod +x /usr/local/bin/cfssl* ","date":"2021-04-22","objectID":"/posts/cfssl/:2:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["command"],"content":"自签证书 ","date":"2021-04-22","objectID":"/posts/cfssl/:3:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["command"],"content":"签发 CA 证书 生成 CA 证书签名请求文件 ca-csr.json mkdir certs cd certs/ cat \u003e ca-csr.json \u003c\u003cEOF { \"CN\": \"CA\", \"hosts\": [ ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"BeiJing\", \"O\": \"BJ\", \"ST\": \"BeiJing\", \"OU\": \"CA\" } ] } EOF 证书签名请求文件可以使用 cfssl print-defaults csr 创建，然后在进行相应的修改 生成 CA 证书 cfssl gencert -initca ca-csr.json | cfssl-json -bare ca ","date":"2021-04-22","objectID":"/posts/cfssl/:3:1","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["command"],"content":"签发域名证书 自签发一个域名证书，以 host.com 域名为例 生成证书配置文件 默认配置可以使用 cfssl print-defaults config 命令生成 cat \u003e config.json \u003c\u003cEOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"www\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] }, \"client\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ] } } } } EOF 生成 host.com 域名证书签名请求文件 host-csr.json cat \u003e host-csr.json \u003c\u003cEOF { \"CN\": \"host.com\", \"hosts\": [ \"host.com\", \"*.host.com\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"BeiJing\", \"O\": \"BJ\", \"ST\": \"BeiJing\", \"OU\": \"HOST\" } ] } EOF 签发 host.com 域名证书 cfssl gencert -ca ca.pem -ca-key ca-key.pem -config config.json -profile www host-csr.json | cfssl-json -bare host 使用 cfssl-certinfo 命令查看证书信息 root@10-7-79-148:~/certs# cfssl-certinfo -cert host.pem { \"subject\": { \"common_name\": \"host.com\", \"country\": \"CN\", \"organization\": \"BJ\", \"organizational_unit\": \"HOST\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"BJ\", \"HOST\", \"host.com\" ] }, \"issuer\": { \"common_name\": \"CA\", \"country\": \"CN\", \"organization\": \"BJ\", \"organizational_unit\": \"CA\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"BJ\", \"CA\", \"CA\" ] }, \"serial_number\": \"50106944723092673296745532281502755453871335123\", \"sans\": [ \"host.com\", \"*.host.com\" ], \"not_before\": \"2021-04-22T13:28:00Z\", \"not_after\": \"2031-04-20T13:28:00Z\", \"sigalg\": \"SHA256WithRSA\", \"authority_key_id\": \"46:6C:D3:F9:1A:89:A0:B6:11:82:DA:E2:8B:8D:00:24:3E:8F:9E:3D\", \"subject_key_id\": \"6A:E8:F5:D9:E5:14:C0:2E:AE:53:DF:41:AF:9E:FF:A7:9B:D4:6A:80\", \"pem\": \"-----BEGIN CERTIFICATE-----\\nMIID3jCCAsagAwIBAgIUCMbfhCW8BG+QACtbh8V8YVcoJtMwDQYJKoZIhvcNAQEL\\nBQAwWDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl\\naUppbmcxCzAJBgNVBAoTAkJKMQswCQYDVQQLEwJDQTELMAkGA1UEAxMCQ0EwHhcN\\nMjEwNDIyMTMyODAwWhcNMzEwNDIwMTMyODAwWjBgMQswCQYDVQQGEwJDTjEQMA4G\\nA1UECBMHQmVpSmluZzEQMA4GA1UEBxMHQmVpSmluZzELMAkGA1UEChMCQkoxDTAL\\nBgNVBAsTBEhPU1QxETAPBgNVBAMTCGhvc3QuY29tMIIBIjANBgkqhkiG9w0BAQEF\\nAAOCAQ8AMIIBCgKCAQEA3ZfbPOW2hzTi3Ec/gpufnhaOkRiCZYIcGe5BJx+cip8c\\nh553anDZts2i1ZTYMeTjwtgHbojHqgqGgcF3xsCHQidRwoOhp7UHRgwfAacfmv0U\\nF5qmoPfNcbQzyZXhDJZAZqWLGqDBhCR/hVVugahXmZb8XzkpreTYTGHAiwAgUKXq\\nDEtEDr0D6LRw27+dR/1bwFs0ad2aEeJxvdH5Y40hO796VoPbX6PCI/TPkMnUsdTF\\nL51Ge+WEKk4TwEEghV1fl6+gGg3dmTcHpb8S5/zhe1bDI7Zs9/ErTAxd1HDdlPxt\\n66HtiygfKEjy8qVtsCIz+hzCxn9bZsmwNRdvV0QitQIDAQABo4GXMIGUMA4GA1Ud\\nDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0G\\nA1UdDgQWBBRq6PXZ5RTALq5T30Gvnv+nm9RqgDAfBgNVHSMEGDAWgBRGbNP5Gomg\\nthGC2uKLjQAkPo+ePTAfBgNVHREEGDAWgghob3N0LmNvbYIKKi5ob3N0LmNvbTAN\\nBgkqhkiG9w0BAQsFAAOCAQEAluByuUmRaPi1+SxjosQI8w6CvJC0N5XbAjsyXrDo\\netwpKKty0745aKyCtkFu6KW7bQohoX4JBdSrqve9V1Psm7Iwh6P8LKBRckBn6lMq\\ndavsgoGkyD/RwRMLUpi0TW8bvd0m+BOO2iHb+BSID7C+WPxflZb2Z8z1ljyzFaM6\\nmfevfYMqUiiRP/ztHvrHcZnk9pQi3kserPJg5DIzNvsvMd1T8IwJg36iIt6j4pi1\\nbtmXSWssMSR1vc7ZPWjS3Jc+2nDVjyPvARJsoAy6BBg07Pd41FhgKPgQE8il1oxc\\n3ep1OXlIC5IjfoZWrp80kznOaj++cOzl1Mg3k+eVyKmx1w==\\n-----END CERTIFICATE-----\\n\" } ","date":"2021-04-22","objectID":"/posts/cfssl/:3:2","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["elasticstack"],"content":"安装配置 filebeat ","date":"2021-04-14","objectID":"/posts/filebeat/:1:0","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"安装 root@ubuntu:/opt# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.12.0-amd64.deb root@ubuntu:/opt# dpkg -i filebeat-7.12.0-amd64.deb ","date":"2021-04-14","objectID":"/posts/filebeat/:1:1","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"配置 filebeat.yml root@ubuntu:/etc/filebeat# cat filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /usr/local/nginx/logs/access.log json.keys_under_root: true json.overwrite_keys: true #filebeat.config.modules: # path: ${path.config}/modules.d/*.yml # reload.enabled: true setup.template.settings: index.number_of_shards: 3 # 配置索引分片数 # #setup.kibana: # output.elasticsearch: hosts: [\"192.168.16.102:9200\",\"192.168.16.103:9200\",\"192.168.16.104:9200\"] # 配置索引名为 nginx-日期，用于区分应用 index: \"nginx-%{+YYYY-MM}\" setup.template.enable: true setup.template.name: \"nginx\" setup.template.pattern: \"nginx-*\" setup.ilm.enabled: false #setup.ilm.rollover_alias: \"nginx\" #setup.ilm.pattern: \"{now/d}-000001\" # #processors: # - add_host_metadata: # when.not.contains.tags: forwarded # - add_cloud_metadata: ~ # - add_docker_metadata: ~ # - add_kubernetes_metadata: ~ ","date":"2021-04-14","objectID":"/posts/filebeat/:1:2","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"配置 nginx 日志格式 root@nginx-1:~# cat /etc/nginx/log-json log_format json '{\"remote_addr\":\"$remote_addr\", \"time_local\": \"$time_local\", \"domain\":\"$host\", \"request\":\"$request\", ' '\"status\":\"$status\", \"body_bytes_sent\":\"$body_bytes_sent\", \"method\":\"$request_method\", ' '\"http_referer\":\"$http_referer\", \"request_time\":\"$request_time\", ' '\"http_user_agent\":\"$http_user_agent\", \"http_x_forwarded_for\":\"$http_x_forwarded_for\", ' '\"upstream_addr\":\"$upstream_addr\", \"upstream_response_time\":\"$upstream_response_time\"}'; # 在 nginx 配置文件中引入，并指定 access_log 使用 json 格式记录日志 root@nginx-1:~# vim /etc/nginx/nginx.conf include /etc/nginx/log-json; access_log /var/log/nginx/access.log json; ","date":"2021-04-14","objectID":"/posts/filebeat/:2:0","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"环境准备 本文使用 Ubuntu 20.04 安装 elasticsearch 集群，准备三台机。 192.168.16.102 192.168.16.103 192.168.16.104 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:1:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"安装配置 elasticsearch 集群 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"安装 elasticsearch root@ubuntu:/opt# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-linux-x86_64.tar.gz root@ubuntu:/opt# tar xzf elasticsearch-7.12.0-linux-x86_64.tar.gz root@ubuntu:/opt# ln -s elasticsearch-7.12.0 elasticsearch ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:1","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"配置 elasticsearch root@ubuntu:/opt# cd elasticsearch/config root@ubuntu:/opt/elasticsearch/config# cat \u003e elasticsearch.yml \u003c\u003cEOF # 集群名 cluster.name: my-application # 集群内同时启动的数据任务个数，默认是2个 cluster.routing.allocation.cluster_concurrent_rebalance: 16 # 添加或删除节点为及负载均衡时并发恢复的线程个数，默认是4个 cluster.routing.allocation.node_concurrent_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个 cluster.routing.allocation.node_initial_primaries_recoveries: 16 # 节点名 node.name: node-1 # 是否有资格成为主节点 node.master: true # 是否为数据节点 node.data: true path.data: /data/elasticsearch/data path.logs: /data/elasticsearch/logs # 监听的网络地址 network.host: 0.0.0.0 network.tcp.keep_alive: true network.tcp.no_delay: true transport.tcp.compress: true gateway.recover_after_nodes: 2 # 用于 HTTP 客户端通信的端口 http.port: 9200 # 用于节点之间通信的端口 transport.port: 9300 # head 管理插件需要打开跨域配置 http.cors.allow-origin: \"*\" http.cors.enabled: true http.max_content_length: 200mb # 节点发现 discovery.seed_hosts: [\"192.168.16.102:9300\",\"192.168.16.103:9300\",\"192.168.16.104:9300\"] # 初始化一个集群时需要此配置来选举 master cluster.initial_master_nodes: [\"node-1\"] EOF # 配置 jvm 堆内存大小 root@ubuntu:/opt/elasticsearch/config# grep '^-Xm' jvm.options -Xms4g -Xmx4g ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:2","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"系统配置 root@ubuntu:/opt/elasticsearch# useradd -m -s /bin/bash elasticsearch root@ubuntu:/opt/elasticsearch# mkdir -p /data/elasticsearch/{data,logs} root@ubuntu:/opt/elasticsearch# chown -R elasticsearch.elasticsearch /data/elasticsearch root@ubuntu:/opt/elasticsearch# chown -R elasticsearch.elasticsearch /opt/elasticsearch-7.12.0 root@ubuntu:/opt/elasticsearch/config# cat \u003e\u003e /etc/security/limits.conf \u003c\u003cEOF * soft nproc unlimited * hard nproc unlimited * soft core unlimited * soft nofile 65535 * hard nofile 65535 EOF root@ubuntu:/opt/elasticsearch/config# echo 'vm.max_map_count=262144' \u003e\u003e /etc/sysctl.conf root@ubuntu:/opt/elasticsearch/config# sysctl -p ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:3","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"启动 elasticsearch 服务 root@ubuntu:~# su - elasticsearch elasticsearch@ubuntu:~$ cd /opt/elasticsearch elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch elasticsearch 默认不允许使用 root 用户启动， 需要切换至 elasticsearch 用户启动； -d 选项可以将程序设置为守护进程 提示：以相同的操作方法配置另两台节点，注意节点名不要重复 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:4","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"elasticsearch 安全验证 elasticsearch 7.7 以后的版本将安全认证功能免费开放了。 并将 X-pack 插件集成了到了开源的 ElasticSearch 版本中。下面介绍如何利用 X-pack 给 ElasticSearch 相关组件设置用户名和密码 # 配置 xpack, 启用验证功能 root@ubuntu:/opt/elasticsearch# cat \u003e\u003econfig/elasticsearch.yml \u003c\u003cEOF xpack.security.enabled: true xpack.security.transport.ssl.enabled: true EOF # 重启 elasticsearch 服务 root@ubuntu:/opt/elasticsearch# kill \u003celasticsearch-pid\u003e root@ubuntu:/opt/elasticsearch# su - elasticsearch elasticsearch@ubuntu:~$ cd /opt/elasticsearch elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch -d # 使用交互命令行模式配置验证密码 elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch-setup-passwords interactive Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user. You will be prompted to enter passwords as the process progresses. Please confirm that you would like to continue [y/N]y Enter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana_system]: Reenter password for [kibana_system]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: 到此已经完成ES及相关组件的加密了, 后续访问和使用相关组件都需要验证用户名和密码了 (请记好你配置的密码) 验证密码信息存储在 .security-7 索引中 不带密码访问时 root@ubuntu:~# curl -I localhost:9200 HTTP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"security\" charset=\"UTF-8\" content-type: application/json; charset=UTF-8 content-length: 381 带密码访问 root@ubuntu:~# curl -i localhost:9200 -u elastic:JEd01cn6hj0qm2mO HTTP/1.1 200 OK content-type: application/json; charset=UTF-8 content-length: 530 { \"name\" : \"node-1\", \"cluster_name\" : \"my-application\", \"cluster_uuid\" : \"G6jxloWFR1SpCJ5cqb4EKA\", \"version\" : { \"number\" : \"7.12.0\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\", \"build_date\" : \"2021-03-18T06:17:15.410153305Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } ","date":"2021-04-10","objectID":"/posts/elasticsearch/:3:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"重置 elasticsearch 密码 官方文档: https://www.elastic.co/guide/en/elasticsearch/reference/7.4/security-api-change-password.html 修改 elastic 用户的密码 root@ubuntu:~# curl -X POST localhost:9200/_security/user/elastic/_password \\ -d '{\"password\":\"123456\"}' \\ -u elastic:JEd01cn6hj0qm2mO \\ -H 'content-type: application/json' 也可以使用 ./bin/elasticsearch-setup-passwords interactive 命令重新设置 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:4:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["nginx"],"content":"alias 与 root 区别 nginx 是通过 alias 设置虚拟目录，在 nginx 的配置中， alias 目录和 root 目录是有区别的 alias 指定的目录是准确的，即 location 匹配访问的 path 目录下的文件直接是在 alias 目录下查找的； root 指定的目录是 location 匹配访问的 path 目录的上一级目录, 这个 path 目录一定要是真实存在 root 指定目录下的； 使用 alias 标签的目录块中不能使用 rewrite 的 break（具体原因不明）；另外， alias 指定的目录后面必须要加上 \"/\" 符号！ alias 虚拟目录配置中，location 匹配的 path 目录如果后面不带 \"/\" ，那么访问的url地址中这个 path 目录后面加不加 \"/\" 不影响访问，访问时它会自动加上 \"/\" ； 但是如果 location 匹配的 path 目录后面加上 \"/\"，那么访问的 url 地址中这个 path 目录必须要加上 \"/\"，访问时它不会自动加上 \"/\"。如果不加上 \"/\"，访问就会失败！ root 目录配置中，location 匹配的 path 目录后面带不带 \"/\"，都不会影响访问。 ","date":"2021-04-01","objectID":"/posts/nginx-alias-root/:1:0","tags":["nginx"],"title":"Nginx alias 与 root 区别","uri":"/posts/nginx-alias-root/"},{"categories":["nginx"],"content":"举例说明 比如 nginx 配置的域名是 liwanggui.com location /huan/ { alias /home/www/huan/; } 在上面 alias 虚拟目录配置下，访问 http://liwanggui.com/huan/a.html 实际指定的是 /home/www/huan/a.html。 注意: alias 指定的目录后面必须要加上 \"/\"，即 /home/www/huan/ 不能改成 /home/www/huan 上面的配置也可以改成 root 目录配置，如下 这样 nginx 就会去 /home/www/huan 下寻找 http://liwanggui.com/huan/a.html 资源，两者配置后的访问效果是一样的！ location /huan/ { root /home/www/; } ","date":"2021-04-01","objectID":"/posts/nginx-alias-root/:2:0","tags":["nginx"],"title":"Nginx alias 与 root 区别","uri":"/posts/nginx-alias-root/"},{"categories":["nginx"],"content":"域名重定向 当用户访问 http://liwanggui.com 时 url 重定向至 https://liwanggui.com， 实现 http -\u003e https 重定向，实现方式有两种： 通过 rewrite 模块的 permanent 参数实现永久重定向的 http 状态 301 通过 return 指令实现 （推荐） rewrite 实现 server { listen 80; server_name liwanggui.com; access_log off; rewrite ^/(.*)$ https://$host/$1 permanent; # 匹配以斜杠开头之后的所有字符 $1 表示小括号内匹配的字符 permanent 表示永久301跳转 } return 实现: 推荐做法 server { listen 80; server_name liwanggui.com; access_log off; return 301 https://$host$request_uri; } $host: 表示 HTTP 请求头中的 Host 值 $request_uri: 表示 HTTP 请求 uri ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:1:0","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"多域名跳转应用实例 使用 nginx 做反向代理，当用户访问 www.liwanggui.com 时就代理到 192.168.1.100:8080 的 web 目录下， 当用户访问 http://www.liwanggui.com/admin 时就代理到 192.168.1.100:8080 的 admin 目录下， 当用户访问 wap.liwanggui.com 时就代理到 192.168.1.100:8080 的 wap 目录下 server_name www.liwanggui.com; location / { proxy_pass http://192.168.1.100:8080/web/; } location /admin { proxy_pass http://192.168.1.100:8080/admin; } server_name wap.liwanggui.com; location / { proxy_pass http://192.168.1.100:8080/wap/; } 注意：在 proxy_pass 配置两个代理目录 web 和 wap 后面必须加一个斜杠，否则 nginx 会报错，仔细看上面代理配置中两种写法的区别就明白了 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:2:0","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"nginx 常用指令 nginx 的 URL 重写模块是用得比较多的模块之一，常用的 URL 重写模块命令有 if 、rewrite、 set、 break ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:0","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"if 命令 语法： if (condition) {....} 默认值： none 使用字段: server 、location 默认情况下，if 命令默认值为空，可在 nginx 配置文件的 server、location 部分使用，另外，if 命令可以在在判断语句中指定正则或匹配条件等，相关匹配条件如下： if 与小括号之间有一个空格 正则表达式匹配 ~ 表示区分大小写匹配 ~* 表示不区分大小写匹配 !~ 表示区分大小写不匹配， !~* 表示不区分大小写不匹配 文件及目录匹配 -f 和 !-f 用来判断是否存在文件 -d 和 !-d 用来判断是否存在目录 -e 和 !-e 用来判断是否存在文件和目录 -x 和 !-x 用来判断文件是否可执行 nginx 配置文件中有很多内置变量，这些变量经常和if命令一起使用。常见的内置变量有如下几种: $args, 此变量与请求行中的参数相等 $document_root， 此变量等同于当前请求的 root 命令指定的值 $uri, 此变量等同于当前 request 中的 uri $document_uri， 此变量与 $uri 含义一样 $host， 此变量与请求头部中的 “Host” 行指定的值一致 $limit_rate, 此变量用来设置限制连接的速率 $request_method, 此变量等同于 request 的 method，通常是’GET',‘POST’ $remote_addr, 此变量表示客户端的IP地址 $remote_port, 此变量表示客户端端口 $remote_user, 此变量等同于用户名，由 ngx_http_auth_basic_module 认证 $request_filename, 此变量表示当前请求的文件的路径名，由 root 或 alias 与 URI request 组合而成 $request_uri, 此变量表示含有参数的完整的初始 URI $query_string, 此变量与$args含义一致 $server_name, 此变量表示请求到达的服务器名 $server_port, 此变量表示请示到达的服务器端口 例：uri为：http://localhost:88/test1/test2/test.php 各变量值如下： $host： localhost $server_port： 88 $request_uri： http://localhost:88/test1/test2/test.php $document_uri： /test1/test2/test.php $document_root： /var/www/html $request_filename： /var/www/html/test1/test2/test.php 配置实例 server { listen 80; server_name www.liwanggui.com; access_log logs/host.access.log main; index index.html index.htm; root /var/www/html; location ~*\\.(gif|jpg|jpeg|png|bmp|swf|htm|html|css|js)$ { root /usr/local/nginx/www/img; if (!-f $request_filename){ root /var/www/html/img; } if (!-f $request_filename){ root /apps/images; } } location ~*\\.(jsp)${ root /webdata/webapp/www/ROOT; if (!-f $request_filename){ root /usr/local/nginx/www/jsp; } proxy_pass http://127.0.0.1:8888; } } 这段代码主要完成对 www.liwanggui.com 这个域名的资源访问配置， www.liwanggui.com 这个域名的根目录为 /var/www/html, 而静态资源分别位于 /usr/local/nginx/www/img, /var/www/html/img, /apps/images 三个目录下， 请求静态资源的方式依次在三个目录中找，如果第一个目录找不到，就找第二目录，以此类推，如果都找不到，将提示404错误； 动态资源分别位于 /webdata/webapp/www/ROOT,/usr/local/nginx/www/jsp, 两个目录下，如果客户端请求的的资源是以 .jsp 结尾的，那么将依次在这两个动态程序目录下查找资源。 而于没有在这两个目录中定义的资源，将全部从根目录 /var/www/html 进行查找。 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:1","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"rewrite 命令 nginx 通过 ngx_http_rewrite_module 模块支持URL重写和if条件判断，但要使用 rewrite 功能，需要 pcre 支持，应在编译 nginx 时指定 pcre 源码目录. rewrite 的使用语法如下： 语法： rewrite regex flag 默认值： none 使用字段： server location if 在默认情况下，rewrite 命令默认值为空，可以 nginx 配置文件的 server,location,if 部分使用，rewrite 命令的最后一项参数为 flag 标记,其支持的 flag 标记主要有以下几种： last, 相当于 apache 里的 L 标记，表示完成 rewrite 之后搜索相应的 uri 或 location break, 表示终止匹配，不再匹配后面的规则 redirect, 将返回 302 临时重定向，在浏览器地址会显示跳转后的 URL 地址。 permanent, 将返回 301 永久重定向，在浏览器地址会显示跳转后的 URL 地址。 last 一般写在 server 和 if 中，而 break 一般使用在 location 中 last 不终止重写后的 url 匹配，即新的 url 会再从 server 走一遍匹配流程，而 break 终止重写后的匹配 break 和 last 都能组织继续执行后面的 rewrite 指令 其中 last 和 break 用来实现 URL 重写，浏览器地址不变。下面是一个示例配置： location ~ ^/best/ { rewrite ^/best/(.*)$ /best/$1 break; proxy_pass http://www.liwanggui.com; } 这个例子使用了 break 标记，可实现将请求为 http://www.lwg.com/best/webinfo.html 的页面重定向到 http://www.liwanggui.com/best/webinfo.html 页面而不引起浏览器地址栏中 URL 的变化。 这个功能在新旧网站交替的时候非常有用（最好实践下，感觉有问题） ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:2","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"set 命令 通过 set 命令可以设置一个变量并为其赋值，其值可以是文本、变量或他们的组合。也可以使用set定义一个新的变量，但是不能使用 set 设置 $http_xxx 头部变量 set 的使用方法如下： 语法： set variable value 默认值： none 使用字段： server location if 在默认情况下，set 命令默认值为空，可以 nginx 配置文件的 server location if 部分使用，下面是一个示例配置 location / { proxy_pass http://127.0.0.1:8080/; set $query $query_string; rewrite /dede /wordpress?$query?; } 在这个例子中，要实现将请求 http://www.liwanggui.com/dede/wp?p=160 的页面，重写到地址 http://www.liwanggui.com/wordpress/?p=160, 也就是重写带参数的 URL. 这里涉及 $query_string 变量，这个变量相当于请求行中的参数，也就是？ 后面的内容。也可以用 $args 代替 $query_string 变量 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:3","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"break 命令 break 的用法在前面的介绍中其实已经出现过，它表示完成当前设置的规则后，不再匹配后面的重写规则。 break的使用语法如下： 语法： break 默认值： none 使用字段： server lcoation if 在默认情况下，break 命令的值为空，可以 nginx 配置文件的 server lcoation if 部分使用，下面是一个示例配置 server { listen 80; server_name www.lwg.com www.liwanggui.com; if ($host != 'www.wb.com'){ rewrite ^/(.*)$ http://www.lwg.com/error.txt break; rewrite ^/(.*)$ http://www.lwg.com/$1 permanent; } } 这个例子定义了两个域名 www.lwg.com 和 www.liwanggui.com, 当通过域名 www.liwanggui.com 访问网站时，会将请求重定向到 http://www.lwg.com/error.txt 页面，由于设置了 break 命令，因此下面的 rewrite 规则不再执行，直接退出。 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:4","tags":["nginx"],"title":"Nginx 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"准备编译环境 本文使用 Ubuntu 20.04.2 LTS 编译安装. root@ubuntu:~# apt install gcc make openssl libssl-dev libpcre3-dev zlib1g-dev root@ubuntu:~# cd /usr/local/src root@ubuntu:/usr/local/src# wget http://nginx.org/download/nginx-1.18.0.tar.gz root@ubuntu:/usr/local/src# tar xzf nginx-1.18.0.tar.gz ","date":"2021-04-01","objectID":"/posts/nginx-install/:1:0","tags":["nginx"],"title":"Nginx 源码编译安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"编译安装 root@ubuntu:/usr/local/src# cd nginx-1.18.0 root@ubuntu:/usr/local/src/nginx-1.18.0# useradd -r www root@ubuntu:/usr/local/src/nginx-1.18.0# ./configure --prefix=/usr/local/nginx \\ --user=www \\ --group=www \\ --with-threads \\ --with-file-aio \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_auth_request_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_degradation_module \\ --with-http_slice_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --with-stream_ssl_preread_module root@ubuntu:/usr/local/src/nginx-1.18.0# make \u0026\u0026 make install ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:0","tags":["nginx"],"title":"Nginx 源码编译安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"Nginx 服务管理 启动 nginx 服务 root@ubuntu:/usr/local/nginx# /usr/local/nginx/sbin/nginx root@ubuntu:/usr/local/nginx# curl -I localhost HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Thu, 01 Apr 2021 08:16:00 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Thu, 01 Apr 2021 08:07:38 GMT Connection: keep-alive ETag: \"60657f4a-264\" Accept-Ranges: bytes 管理 nginx 服务 # 测试 nginx 配置文件语法 root@ubuntu:~# /usr/local/nginx/sbin/nginx -t nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful # 重载 nginx 配置 root@ubuntu:~# /usr/local/nginx/sbin/nginx -s reload # 停止 nginx 服务 root@ubuntu:~# /usr/local/nginx/sbin/nginx -s stop ","date":"2021-04-01","objectID":"/posts/nginx-install/:3:0","tags":["nginx"],"title":"Nginx 源码编译安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"Systemd 配置 root@ubuntu:~# cat \u003e /usr/lib/systemd/system/nginx.service \u003c\u003cEOF [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/usr/local/nginx/logs/nginx.pid ExecStartPre=/usr/local/nginx/sbin/nginx -t ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target EOF ","date":"2021-04-01","objectID":"/posts/nginx-install/:4:0","tags":["nginx"],"title":"Nginx 源码编译安装","uri":"/posts/nginx-install/"},{"categories":["command"],"content":"回顾下安装软件的三种方式： 编译安装软件，优点是可以定制化安装目录、按需开启功能等，缺点是需要查找并实验出适合的编译参数，诸如 MySQL 之类的软件编译耗时过长。 yum 安装软件，优点是全自动化安装，不需要为依赖问题发愁了，缺点是自主性太差，软件的功能、存放位置都已经固定好了，不易变更。 编译源码，根据自己的需求做成定制 RPM 包 –\u003e 搭建内网yum仓库 –\u003e yum安装。结合前两者的优点，暂未发现什么缺点。可能的缺点就是 RPM 包的通用性差，只能适用于本公司的环境。另外一般人不会定制 RPM 包。这是中大型互联网企业运维自动化的必要技能。 这里也不介绍 rpm 的概念，想了解的朋友可以查看 http://www.ibm.com/developerworks/cn/linux/l-rpm/ 这里也不介绍 rpmbuild 这个打包工具了，想了解的朋友自行谷歌百度。但我不建议大家花太多的时间去学习这个命令，比较晦涩，而且我会在下面介绍更简单的命令。 ","date":"2021-04-01","objectID":"/posts/fpm/:0:0","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"FPM打包工具 FPM 的作者是 jordansissel FPM 的 github FPM 功能简单说就是将一种类型的包转换成另一种类型。 ","date":"2021-04-01","objectID":"/posts/fpm/:1:0","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"1. 支持的源类型包 dir 将目录打包成所需要的类型，可以用于源码编译安装的软件包 rpm 对rpm进行转换 gem 对rubygem包进行转换 python 将python模块打包成相应的类型 ","date":"2021-04-01","objectID":"/posts/fpm/:1:1","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"2. 支持的目标类型包 rpm 转换为rpm包 deb 转换为deb包 solaris 转换为solaris包 puppet 转换为puppet模块 ","date":"2021-04-01","objectID":"/posts/fpm/:1:2","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"3. FPM 安装 fpm 是 ruby 写的，因此系统环境需要 ruby，且 ruby 版本号大于1.8.5。rpm 包太旧，安装后会提示版本问题。所以采用编译安装方式： # 下载 ruby 源码包 wget -c https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.4.tar.gz tar xzf ruby-2.3.4.tar.gz cd ruby-2.3.4 ./configure \u0026\u0026 make \u0026\u0026 make install # 添加阿里云的Rubygems仓库，外国的源慢 gem sources -a http://mirrors.aliyun.com/rubygems/ # 移除原生的Ruby仓库 gem sources --remove http://rubygems.org/ # 安装fpm gem install fpm ","date":"2021-04-01","objectID":"/posts/fpm/:1:3","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"4. FPM参数 详细使用见 fpm --help 常用参数 -s 指定源类型 -t 指定目标类型，即想要制作为什么包 -n 指定包的名字 -v 指定包的版本号 -C 指定打包的相对路径 Change directory to here before searching forfiles -d 指定依赖于哪些包 -f 第二次打包时目录下如果有同名安装包存在，则覆盖它 -p 输出的安装包的目录，不想放在当前目录下就需要指定 --post-install 软件包安装完成之后所要运行的脚本；同--after-install --pre-install 软件包安装完成之前所要运行的脚本；同--before-install --post-uninstall 软件包卸载完成之后所要运行的脚本；同--after-remove --pre-uninstall 软件包卸载完成之前所要运行的脚本；同--before-remove ","date":"2021-04-01","objectID":"/posts/fpm/:1:4","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"使用实例–实战定制nginx的RPM包 ","date":"2021-04-01","objectID":"/posts/fpm/:2:0","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"1. 安装nginx yum -y install pcre-devel openssl-devel useradd -M -s /sbin/nologin nginx tar xf nginx-1.6.2.tar.gz cd nginx-1.6.2 ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module make \u0026\u0026 make install ","date":"2021-04-01","objectID":"/posts/fpm/:2:1","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"2. 编写脚本 [root@oldboy ~]# cd /server/scripts/ [root@oldboy scripts]# vim nginx_rpm.sh # 这是安装完rpm包要执行的脚本 #!/bin/bash useradd -M -s /sbin/nologin nginx ","date":"2021-04-01","objectID":"/posts/fpm/:2:2","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"3. 打包 [root@oldboy ~]# fpm -s dir -t rpm -n nginx -v 1.6.2 \\ -d 'pcre-devel,openssl-devel' \\ --post-install /server/scripts/nginx_rpm.sh \\ -f /usr/local/nginx no value for epoch is set, defaulting to nil {:level=\u003e:warn} no value for epoch is set, defaulting to nil {:level=\u003e:warn} Created package {:path=\u003e\"nginx-1.6.2-1.x86_64.rpm\"} [root@oldboy ~]# ll -h nginx-1.6.2-1.x86_64.rpm -rw-r--r-- 1 root root 6.7M Nov 1 10:02 nginx-1.6.2-1.x86_64.rpm ","date":"2021-04-01","objectID":"/posts/fpm/:2:3","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"4. 安装rpm包 安装rpm包的三种方法： ①rpm命令安装 [root@LB-nginx-01 ~]# rpm -ivh nginx-1.6.2-1.x86_64.rpm error: Failed dependencies: pcre-devel is needed by nginx-1.6.2-1.x86_64 openssl-devel is needed by nginx-1.6.2-1.x86_64 但会报如上依赖错误，需要先yum安装依赖才能安装 rpm 包 yum 命令安装rpm包 yum -y localinstall nginx-1.6.2-1.x86_64.rpm 这个命令会自动先安装 rpm 包的依赖，然后再安装 rpm 包。可以配合自建的 yum 本地仓库使用 ","date":"2021-04-01","objectID":"/posts/fpm/:2:4","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["command"],"content":"注意事项 相对路径问题 [root@oldboy nginx]# fpm -s dir -t rpm -n nginx -v 1.6.2 . no value for epoch is set, defaulting to nil {:level=\u003e:warn} no value for epoch is set, defaulting to nil {:level=\u003e:warn} Created package {:path=\u003e\"nginx-1.6.2-1.x86_64.rpm\"} [root@oldboy nginx]# rpm -qpl nginx-1.6.2-1.x86_64.rpm /client_body_temp /conf/extra/dynamic_pools /conf/extra/static_pools ………… # 绝对路径 [root@oldboy ~]# fpm -s dir -t rpm -n nginx -v 1.6.2 /usr/local/nginx no value for epoch is set, defaulting to nil {:level=\u003e:warn} no value for epoch is set, defaulting to nil {:level=\u003e:warn} Created package {:path=\u003e\"nginx-1.6.2-1.x86_64.rpm\"} [root@oldboy ~]# rpm -qpl nginx-1.6.2-1.x86_64.rpm /usr/local/nginx/client_body_temp /usr/local/nginx/conf/extra/dynamic_pools /usr/local/nginx/conf/extra/static_pools /usr/local/nginx/conf/fastcgi.conf /usr/local/nginx/conf/fastcgi.conf.default ………… 注：fpm 类似 tar 打包一样，只是 fpm 打的包能够被 yum 命令识别而已。 ","date":"2021-04-01","objectID":"/posts/fpm/:3:0","tags":["fpm"],"title":"RPM 快速打包","uri":"/posts/fpm/"},{"categories":["linux"],"content":"yum 主要用于自动安装、升级 rpm 软件包，它能自动查找并解决 rpm 包之间的依赖关系。要成功的使用 yum 工具安装更新软件或系统，就需要有一个包含各种rpm软件包的 repository（软件仓库），这个软件仓库我们习惯称为 yum 源。网络上有大量的 yum 源，但由于受到网络环境的限制，导致软件安装耗时过长甚至失败。特别是当有大量服务器大量软件包需要安装时，缓慢的进度条令人难以忍受。因此我们在优化系统时，都会更换国内的源。 相比较而言，本地 yum 源服务器最大优点是局域网的快速网络连接和稳定性。有了局域网中的 yum 源服务器，即便在 Internet 连接中断的情况下，也不会影响其他 yum 客户端的软件安装和升级。 ","date":"2021-04-01","objectID":"/posts/yum-repo/:0:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"创建 yum 仓库目录 mkdir -p /data/yum/centos/{6,7}/x86_64 上传 rpm 包到 /data/yum/centos/6/x86_64 和 /data/yum/centos/7/x86_64 目录 ","date":"2021-04-01","objectID":"/posts/yum-repo/:1:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"安装 createrepo 软件 yum install createrepo ","date":"2021-04-01","objectID":"/posts/yum-repo/:2:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"初始化 repodata 索引文件 createrepo -pdo /data/yum/centos/6/x86_64/ /data/yum/centos/6/x86_64/ createrepo -pdo /data/yum/centos/7/x86_64/ /data/yum/centos/7/x86_64/ ","date":"2021-04-01","objectID":"/posts/yum-repo/:3:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"提供 yum 服务 提供 yum 服务很简单，只需要使用 nginx 开启目录浏览器功能即可, 测试时可以使用 python 模块实现 # python 2.x python2 -m SimpleHTTPServer 80 # python 3.x python3 -m http.server 80 ","date":"2021-04-01","objectID":"/posts/yum-repo/:4:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"添加新 rpm 包 每当添加新的 rpm 包时都需要执行以下命令, 为了方便可以将以下加入计划任务中 createrepo --update /data/yum/centos/6/x86_64/ createrepo --update /data/yum/centos/7/x86_64/ ","date":"2021-04-01","objectID":"/posts/yum-repo/:5:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["linux"],"content":"客户端配置 客户端需要将 yum 仓库地址写成 yum 源配置文件，并放入 /etc/yum.repos.d 目录中 cat \u003e /etc/yum.repos.d/devops.repo \u003c\u003c REPO [devops] name=CentOS-$releasever - DEVOPS baseurl=http://your_domain_name/centos/$releasever/x86_64/ enable=1 gpgcheck=0 REPO 之后就可以使用 yum 安装 devops 仓库中的 rpm 包了 ","date":"2021-04-01","objectID":"/posts/yum-repo/:6:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["command"],"content":" 参考文档 - 1 参考文档 - 2 软件编译安装可以很大程序上定制符合实际需求的软件包，但由于编译时间过长依赖关系复杂常常会耽误太多的时间，为了达到快速部署安装的需求我们需要定制符合需求的 rpm 包， rpm 默认是通过 rpmbuild 工具配合 spec 配置文件生成。下面将介绍如何使用 rpmbild 工具生成定制 rpm 包， 以 nginx 为例 ","date":"2021-04-01","objectID":"/posts/rpmbuild/:0:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["command"],"content":"环境准备 安装所需工具 yum install gcc rpm-build rpm-devel rpmlint make python bash coreutils diffutils patch rpmdevtools 准备制作环境 [root@build ~]# useradd -m build [root@build ~]# su - build [build@build ~]# rpmdev-setuptree [build@build ~]# ls -R rpmbuild/ rpmbuild/: BUILD RPMS SOURCES SPECS SRPMS rpmbuild/BUILD: # 源码编译工作目录 rpmbuild/RPMS: # 最终 rpm 包生成目录 rpmbuild/SOURCES: # 源码包及附加文件放置目录 rpmbuild/SPECS: # spec 配置文件目录 rpmbuild/SRPMS: # 最终端 srpm 包生成目录 rpmbuild/BUILDROOT: rpm 打包工作目录 生成 nginx.spec 生成 nginx.spec 配置文件，并根据情况进行修改 [build@build ~]# cd rpmbuild/SPECS [build@SPECS ~]# rpmdev-newspec nginx [build@SPECS ~]# cat nginx.spec Name: nginx Version: 1.14.2 Release: 1%{?dist} Summary: A high performance web server and reverse proxy server Group: System Environment/Daemons License: GPLv2 URL: https://nginx.org # 制作 rpm 包所需文件 Source0: nginx-1.14.2.tar.gz Source1: nginx.conf Source2: limit.conf Source3: proxy.conf Source4: pathinfo.conf Source5: enable-php.conf Source6: geoip2.conf Source7: upstream.conf.example Source8: enable-ssl.conf.example Source9: nginx-status.conf.example Source10: nginx.init Source11: nginx.logrotate # 编译时需要的依赖包 BuildRequires: gcc BuildRequires: gcc-c++ BuildRequires: make BuildRequires: libmaxminddb-devel # rpm 安装时需要的依赖包 Requires: libmaxminddb-devel %description Nginx is a web server and a reverse proxy server for HTTP, SMTP, POP3 and IMAP protocols, with a strong focus on high concurrency, performance and low memory usage. # 制作前准备，解包和路径切换工具 %prep %setup -q # 软件包编译过程 %build ./configure --prefix=/usr/local/nginx / --user=www --group=www / --with-http_stub_status_module / --with-http_sub_module / --with-http_ssl_module / --with-http_v2_module / --with-http_realip_module / --with-openssl=./openssl-1.1.1b / --with-pcre=./pcre-8.42 / --with-zlib=./zlib-1.2.11 / --add-module=./nginx-sticky-module-ng-1.2.6/ / --add-module=./nginx-upstream-check-module/ / --add-module=./ngx-http-geoip2-module-3.2/ make %{?_smp_mflags} # 编译完安装软件包至指定目录等待打包 %install rm -rf $RPM_BUILD_ROOT make install DESTDIR=$RPM_BUILD_ROOT %{__install} -p -D -m 0644 %{SOURCE1} %{buildroot}/usr/local/nginx/conf/nginx.conf %{__install} -p -D -m 0644 %{SOURCE2} %{buildroot}/usr/local/nginx/conf/limit.conf %{__install} -p -D -m 0644 %{SOURCE3} %{buildroot}/usr/local/nginx/conf/proxy.conf %{__install} -p -D -m 0644 %{SOURCE4} %{buildroot}/usr/local/nginx/conf/pathinfo.conf %{__install} -p -D -m 0644 %{SOURCE5} %{buildroot}/usr/local/nginx/conf/enable-php.conf %{__install} -p -D -m 0644 %{SOURCE6} %{buildroot}/usr/local/nginx/conf/geoip2.conf %{__install} -p -D -m 0644 %{SOURCE7} %{buildroot}/usr/local/nginx/conf/upstream.conf.example %{__install} -p -D -m 0644 %{SOURCE8} %{buildroot}/usr/local/nginx/conf/enable-ssl.conf.example %{__install} -p -D -m 0644 %{SOURCE9} %{buildroot}/usr/local/nginx/conf/nginx-status.conf.example %{__install} -p -D -m 0755 %{SOURCE10} %{buildroot}/etc/init.d/nginx %{__install} -p -D -m 0644 %{SOURCE11} %{buildroot}/etc/logrotate.d/nginx # 清理工作 %clean rm -rf $RPM_BUILD_ROOT # 安装前执行的命令 %pre if ! id www \u0026\u003e/dev/null; then useradd -r -M -s /sbin/nologin www fi # 安装后 %post /sbin/chkconfig --add %{name} /sbin/chkconfig %{name} on # 卸载前 %preun /etc/init.d/nginx stop /sbin/chkconfig --del %{name} # rpm 打包的文件列表 %files %defattr(-,root,root,-) /usr/local/nginx/ /etc/logrotate.d/nginx %attr(0755,root,root) /etc/init.d/nginx %config(noreplace) /usr/local/nginx/conf/nginx.conf %config(noreplace) /usr/local/nginx/conf/limit.conf %config(noreplace) /usr/local/nginx/conf/geoip2.conf %config(noreplace) /usr/local/nginx/conf/enable-php.conf # 更新日志 %changelog 准备 nginx 源码包及相关文件 [build@build ~]$ cd rpmbuild/SOURCES/ [build@build SOURCES]$ ls -l total 12356 -rw-r--r-- 1 build build 207 Mar 17 17:32 enable-php.conf -rw-r--r-- 1 build build 1137 Mar 17 18:11 enable-ssl.conf.example -rw-r--r-- 1 build build 1077 Mar 17 21:05 geoip2.conf -rw-r--r-- 1 build build 1488 Mar 17 17:53 limit.conf -rw-rw-r-- 1 build build 12589977 Mar 17 20:19 nginx-1.14.2.tar.gz -rw-r--r-- 1 ","date":"2021-04-01","objectID":"/posts/rpmbuild/:1:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["command"],"content":"制作 rpm 包 [build@build ~]$ cd rpmbuild/SPECS/ [build@SPECS ~]# rpmbuild -ba nginx.spec rpmbuild -bp nginx.spec # 制作到%prep段 rpmbuild -bc nginx.spec # 制作到%build段 rpmbuild -bi nginx.spec # 执行 spec 文件的 “%install” 阶段 (在执行了 %prep 和 %build 阶段之后)。这通常等价于执行了一次 “make install” rpmbuild -bb nginx.spec # 制作二进制包 rpmbuild -ba nginx.spec # 表示既制作二进制包又制作src格式包 Tips: 更新多选项说明使用 rpmbuild -h ","date":"2021-04-01","objectID":"/posts/rpmbuild/:2:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["command"],"content":"linux 下最受欢迎的多线程归档器是 pigz（而不是gzip）和 pbzip2（而不是bzip2) ","date":"2021-04-01","objectID":"/posts/pigz/:0:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["command"],"content":"开始使用 pigz 可以当做是 gzip 高级版，可以执行 gzip 的工作，但是在压缩时会将工作分散到多个处理器和内核上。 pbzip2 可以当做是 bzip2 高级版, 在和 tar 命令一起使用时需要手动使用的压缩程序, 信息如下: -I, --use-compress-program=PROG 默认情况系统并没有安装 pigz，pbzip2, 需要手动安装执行下以下命令安装 $ yum install -y pigz pbzip2 ","date":"2021-04-01","objectID":"/posts/pigz/:1:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["command"],"content":"示例 $ tar -I pbzip2 -cf OUTPUT_FILE.tar.bz2 paths_to_archive $ tar --use-compress-program=pigz -cf OUTPUT_FILE.tar.gz paths_to_archive Archiver 必须接受 -d 如果替换实用程序没有此参数和/或您需要指定其他参数，则使用管道（如有必要，添加参数）： $ tar cf - paths_to_archive | pbzip2 \u003e OUTPUT_FILE.tar.gz $ tar cf - paths_to_archive | pigz \u003e OUTPUT_FILE.tar.gz 解压只需要将 -c 替换为 -x 即可. ","date":"2021-04-01","objectID":"/posts/pigz/:2:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["redis"],"content":"Redis Replication ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:0","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Replication 原理 副本库通过 slaveof 127.0.0.1 6380 命令, 连接主库,并发送 SYNC 给主库 主库收到 SYNC,会立即触发 BGSAVE,后台保存 RDB,发送给副本库 副本库接收后会应用 RDB 快照 主库会陆续将中间产生的新的操作,保存并发送给副本库 到此,我们主复制集就正常工作了 再此以后,主库只要发生新的操作,都会以命令传播的形式自动发送给副本库. 所有复制相关信息,从 info 信息中都可以查到. 即使重启任何节点, 他的主从关系依然都在. 如果发生主从关系断开时,从库数据没有任何损坏, 在下次重连之后, 从库发送 PSYNC 给主库 主库只会将从库缺失部分的数据同步给从库应用,达到快速恢复主从的目的 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:1","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"准备多实例 Redis 服务 实验采用单机多实例的方式进行，创建三个实例。端口 6380-6382, 可以使用 redis 源码包中的脚本（install_server.sh）创建 [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6380 Please select the redis config file name [/data/redis/etc/6380.conf] Selected default - /data/redis/etc/6380.conf Please select the redis log file name [/data/redis/log/redis_6380.log] Selected default - /data/redis/log/redis_6380.log Please select the data directory for this instance [/data/redis/6380] Selected default - /data/redis/6380 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6380 Config file : /data/redis/etc/6380.conf Log file : /data/redis/log/redis_6380.log Data dir : /data/redis/6380 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6380.conf =\u003e /etc/init.d/redis_6380 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6381 Please select the redis config file name [/data/redis/etc/6381.conf] Selected default - /data/redis/etc/6381.conf Please select the redis log file name [/data/redis/log/redis_6381.log] Selected default - /data/redis/log/redis_6381.log Please select the data directory for this instance [/data/redis/6381] Selected default - /data/redis/6381 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6381 Config file : /data/redis/etc/6381.conf Log file : /data/redis/log/redis_6381.log Data dir : /data/redis/6381 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6381.conf =\u003e /etc/init.d/redis_6381 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6382 Please select the redis config file name [/data/redis/etc/6382.conf] Selected default - /data/redis/etc/6382.conf Please select the redis log file name [/data/redis/log/redis_6382.log] Selected default - /data/redis/log/redis_6382.log Please select the data directory for this instance [/data/redis/6382] Selected default - /data/redis/6382 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6382 Config file : /data/redis/etc/6382.conf Log file : /data/redis/log/redis_6382.log Data dir : /data/redis/6382 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6382.conf =\u003e /etc/init.d/redis_6382 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! 本例为了使用自定义的目录存放 redis 数据及配置文件对 install_server.sh 脚本中的路径做了修改 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:2","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Replication 配置 6380 为主库，6381-6382 为从库，配置如下: 主库配置 [root@localhost log]# redis-cli -p 6380 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE 从库配置 [root@localhost log]# redis-cli -p 6381 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE OK 127.0.0.1:6381\u003e slaveof 127.0.0.1 6380 OK [root@localhost log]# redis-cli -p 6382 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE OK 127.0.0.1:6381\u003e slaveof 127.0.0.1 6380 OK 如果想解除主从关系可以使用 slaveof no one 指令 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:3","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"检查 Replication 状态 [root@localhost etc]# redis-cli -p 6380 -a 123 info replication # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=6381,state=online,offset=3442,lag=1 slave1:ip=127.0.0.1,port=6382,state=online,offset=3442,lag=1 master_repl_offset:3575 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:3574 [root@localhost etc]# redis-cli -p 6381 -a 123 info replication # Replication role:slave master_host:127.0.0.1 master_port:6380 master_link_status:up master_last_io_seconds_ago:0 master_sync_in_progress:0 slave_repl_offset:6291 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 可以在主库写入数据然后到从库读取测试同步 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:4","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Redis Sentinel 高可用 由于 redis replication 默认状态下如果主库宕机，是不会自动切换主从身份的，需要手动干预，这是生产环境下不允许的。为了解决此问题 redis 引入哨兵模式。 redis sentinel 有以下作用： 监控节点 自动选主，切换主从身份 从库自动指向新的主库 对应用透明 自动处理故障节点 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:0","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"配置 sentinel 准备 sentinel 配置文件 源码包中有示例配置文件可用，直接复制修改即可 mkdir /data/sentinel cat \u003e /data/sentinel/26379.conf \u003c\u003cEOF protected-mode no daemonize yes logfile \"/data/sentinel/26379.log\" port 26379 dir \"/tmp\" sentinel myid 994ea01af0f13692c13eeda116a0668084bb5e68 # mymaster 是一个自定义名称，客户端通过 sentinel 连接 redis 时需要使用 # 127.0.0.1 6380 是主节点的 ip 和 端口号 # 1 是 sentinel failover 投票数，默认为 sentinel 节点数/2 + 1, 1个节点时为1 # 为了能正常投票得出结果 sentinel 节点数得为奇数个 sentinel monitor mymaster 127.0.0.1 6380 1 sentinel down-after-milliseconds mymaster 5000 sentinel auth-pass mymaster 123 sentinel config-epoch mymaster 3 sentinel leader-epoch mymaster 3 sentinel known-slave mymaster 127.0.0.1 6382 sentinel known-slave mymaster 127.0.0.1 6381 sentinel current-epoch 3 EOF 启动 sentinel 服务 /usr/local/bin/redis-sentinel /data/sentinel/26379.conf redis-sentinel 是 redis-server 的软链接 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:1","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"连接测试 sentinel sentinel 管理命令 [root@localhost sentinel]# redis-cli -p 26379 # 列出所有被监视的主服务器 127.0.0.1:26379\u003e SENTINEL masters # 列出所有被监视的从服务器 127.0.0.1:26379\u003e SENTINEL slaves \u003cmaster name\u003e # 强制开启 主从切换, 危险 127.0.0.1:26379\u003e SENTINEL failover \u003cmaster name\u003e 测试主从切换 模拟故障停止主库 [root@localhost etc]# redis-cli -p 6380 -a 123 shutdown 查看 sentinel 日志 3963:X 28 Mar 15:31:13.864 # +sdown master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +odown master mymaster 127.0.0.1 6380 #quorum 1/1 3963:X 28 Mar 15:31:13.864 # +new-epoch 6 3963:X 28 Mar 15:31:13.864 # +try-failover master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +vote-for-leader 994ea01af0f13692c13eeda116a0668084bb5e68 6 3963:X 28 Mar 15:31:13.864 # +elected-leader master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +failover-state-select-slave master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.948 # +selected-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.948 * +failover-state-send-slaveof-noone slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.007 * +failover-state-wait-promotion slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.943 # +promoted-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.943 # +failover-state-reconf-slaves master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.004 * +slave-reconf-sent slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.994 * +slave-reconf-inprog slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.994 * +slave-reconf-done slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:16.055 # +failover-end master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:16.055 # +switch-master mymaster 127.0.0.1 6380 127.0.0.1 6382 3963:X 28 Mar 15:31:16.055 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:31:16.055 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:31:21.064 # +sdown slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 可以看到 sentinel 检测到主库服务停止，将 6382 选为主库，进行了切换操作， 6381 也自动与 6382 建立主从关系 6380 修复好后，重新启动服务 sentinel 会自动检测到并 6380 加入主从环境中 [root@localhost etc]# /etc/init.d/redis_6380 start Starting Redis server... 此时可以从 sentinel 日志中看到以下信息 3963:X 28 Mar 15:35:05.234 # -sdown slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:35:15.229 * +convert-to-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 sentinel 会自动在 6380 实例的配置文件最后一行加入一行配置 [root@localhost etc]# tail -n 3 6380.conf requirepass \"123\" masterauth \"123\" slaveof 127.0.0.1 6382 配置多节点 sentinel 此时 redis 的高可用解决了，但 sentinel 只有一个节点存在单点故障，为了解决这个问题，可以通过部署多个 sentinel 节点解决。 配置多个 sentinel 节点时需要注意节点数据及配置文件中的 sentinel monitor mymaster 127.0.0.1 6380 1 配置项的最后一个值的配置。 客户端连接 redis 以 python 客户端为例说明, 参考文档: https://pypi.org/project/redis/ 安装 redis 库 pip install redis 连接至单实例 reids \u003e\u003e\u003e import redis \u003e\u003e\u003e r = redis.Redis(host='localhost', port=6379, db=0) \u003e\u003e\u003e r.set('foo', 'bar') True \u003e\u003e\u003e r.get('foo') b'bar' Sentinel 模式 \u003e\u003e\u003e from redis.sentinel import Sentinel \u003e\u003e\u003e sentinel = Sentinel([('localhost', 26379)], socket_timeout=0.1) \u003e\u003e\u003e sentinel.discover_master('mymaster') ('127.0.0.1', 6379) \u003e\u003e\u003e sentinel.discover_slaves('mymaster') [('127.0.0.1', 6380)] \u003e\u003e\u003e master = sentinel.master_for('mymaster', socket_timeout=0.1, password='123') \u003e\u003e\u003e slave = sentinel.slave_for('mymaster', socket_timeout=0.1, password='123') \u003e\u003e\u003e master.set('foo', 'bar') \u003e\u003e\u003e slave.get('foo') b'bar' ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:2","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Redis 命令 ","date":"2021-03-28","objectID":"/posts/redis-operating/:1:0","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"管理命令 info: 查看 Redis 当前状态信息 127.0.0.1:6379\u003e info # Server redis_version:3.2.9 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:f0e03a357ae83877 redis_mode:standalone os:Linux 3.10.0-862.el7.x86_64 x86_64 arch_bits:64 multiplexing_api:epoll gcc_version:4.8.5 process_id:2502 run_id:c913615b9c199a0d7ba4454c38e1a65427525c60 tcp_port:6379 uptime_in_seconds:2047 uptime_in_days:0 hz:10 lru_clock:6276784 executable:/usr/local/bin/redis-server config_file:/etc/redis/6379.conf # Clients connected_clients:1 client_longest_output_list:0 client_biggest_input_buf:0 blocked_clients:0 ...(略) # 只查看特定的信息 127.0.0.1:6379\u003e info Memory # Memory used_memory:821088 used_memory_human:801.84K used_memory_rss:7888896 used_memory_rss_human:7.52M used_memory_peak:822064 used_memory_peak_human:802.80K total_system_memory:1021902848 total_system_memory_human:974.56M used_memory_lua:37888 used_memory_lua_human:37.00K maxmemory:128000000 maxmemory_human:122.07M maxmemory_policy:noeviction mem_fragmentation_ratio:9.61 mem_allocator:jemalloc-4.0.3 client list: 查看当前连接的客户端列表 client kill \u003cip:port\u003e: 结束客户端连接 127.0.0.1:6379\u003e CLIENT LIST id=3 addr=127.0.0.1:39676 fd=7 name= age=1340 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client # 危险，不要轻易操作 127.0.0.1:6379\u003e CLIENT KILL 127.0.0.1:39676 OK config get/set/rewrite: 在线获取/修改配置信息 config resetstat: 重置统计信息 dbsize: 查看键值对数量 flushall: 清空所有数据，危险操作 flushdb: 清空当前库数据，危险操作 select \u003c序号\u003e: 默认有16个库，切换库 127.0.0.1:6379\u003e select 1 OK 127.0.0.1:6379[1]\u003e select 15 OK monitor: 实时监控操作指令 shutdown: 停止 redis 服务 ","date":"2021-03-28","objectID":"/posts/redis-operating/:1:1","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"key 通用操作命令 keys: 查看所有 key 127.0.0.1:6379\u003e KEYS * 1) \"user.age\" 2) \"user.email\" 3) \"user.name\" 127.0.0.1:6379\u003e KEYS *n* 1) \"user.name\" 127.0.0.1:6379\u003e KEYS user* 1) \"user.age\" 2) \"user.email\" 3) \"user.name\" 注意: 不建议直接使用 keys * 直接查看，数量多的情况下会影响 redis 性能 type: 查看 key 的类型 127.0.0.1:6379\u003e TYPE user.name string 127.0.0.1:6379\u003e TYPE hbb hash expire/pexpire: 以秒/毫秒设置 key 的存活时间 ttl/pttl: 以秒/毫秒返回 key 的存活时间 persist: 取消生存时间设置 127.0.0.1:6379\u003e set te 123 OK 127.0.0.1:6379\u003e EXPIRE te 20 (integer) 1 127.0.0.1:6379\u003e ttl te (integer) 16 127.0.0.1:6379\u003e persist te (integer) 1 127.0.0.1:6379\u003e ttl te (integer) -1 del: 删除一个 key exists: 检查 key 是否存在 rename: 重命名 key 127.0.0.1:6379\u003e exists te (integer) 1 127.0.0.1:6379\u003e del te (integer) 1 127.0.0.1:6379\u003e exists te (integer) 0 127.0.0.1:6379\u003e set te adf OK 127.0.0.1:6379\u003e rename te tem OK 127.0.0.1:6379\u003e get tem \"adf\" 127.0.0.1:6379\u003e exists tem (integer) 1 ","date":"2021-03-28","objectID":"/posts/redis-operating/:1:2","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"Redis 数据类型 redis 支持的数据类型如下 String： 字符类型 Hash：字典类型 List：列表 Set：集合 Sorted set：有序集合 ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:0","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"string 应用场景: session 共享 计数器：微博数，粉丝数，订阅、礼物 字符串类型操作命令 # 设置键的字符串值 127.0.0.1:6379\u003e set mykey 0 OK # 获取键的值 127.0.0.1:6379\u003e get mykey \"0\" # 设置键的字符串值并返回其旧值 127.0.0.1:6379\u003e getset key2 2 (nil) # 设置键的值和有效期(单位秒) 127.0.0.1:6379\u003e setex time 10 10 OK 127.0.0.1:6379\u003e ttl time (integer) 7 # 仅当密钥不存在时设置密钥的值 127.0.0.1:6379\u003e setnx k3 3 (integer) 1 127.0.0.1:6379\u003e get k3 \"3\" # 将键的整数值加1 127.0.0.1:6379\u003e incr sar (integer) 1 # 将键的整数值减1 127.0.0.1:6379\u003e decr sar (integer) 0 # 将键的整数值增加给定的数量 127.0.0.1:6379\u003e incrby sar 100 (integer) 100 # 将键的整数值减少给定的数量 127.0.0.1:6379\u003e decrby sar 15 (integer) 85 # 同时为多个键设置多个值 127.0.0.1:6379\u003e mset k1 1 k2 2 k3 3 OK # 同时获取多个键的值 127.0.0.1:6379\u003e mget k1 k2 k3 1) \"1\" 2) \"2\" 3) \"3\" # 检测键是否存在 127.0.0.1:6379\u003e exists k1 (integer) 1 # 获取键值的长度 127.0.0.1:6379\u003e strlen k2 (integer) 1 ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:1","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"hash 应用场景: 存储部分变更的数据，如用户信息等, 最接近 mysql 表结构的一种类型,主要是可以做数据库缓存 hash 字典类型操作命令 # 设置字典 127.0.0.1:6379\u003e hmset stu id 101 name zhangsan age 20 gender m OK # 获取字典多个字段值 127.0.0.1:6379\u003e hmget stu name age gender 1) \"zhangsan\" 2) \"20\" 3) \"m\" # 获取 stu 键的字段数量 127.0.0.1:6379\u003e hlen stu (integer) 4 # 判断 stu 键中是否存在 name 的字段 127.0.0.1:6379\u003e hexists stu name (integer) 1 # 返回 stu 键的所有字段和值 127.0.0.1:6379\u003e hgetall stu 1) \"id\" 2) \"101\" 3) \"name\" 4) \"zhangsan\" 5) \"age\" 6) \"20\" 7) \"gender\" 8) \"m\" # 获取 stu 所有字段名称 127.0.0.1:6379\u003e hkeys stu 1) \"id\" 2) \"name\" 3) \"age\" 4) \"gender\" # 获取 stu 所有字段的值 127.0.0.1:6379\u003e hvals stu 1) \"101\" 2) \"zhangsan\" 3) \"20\" 4) \"m\" ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:2","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"list 应用场景: 消息队列系统，社交类朋友圈 创建一个列表 每次插入的数据都会放在最前面，第一个索引号为 0 127.0.0.1:6379\u003e lpush wechat \"today is nice day !\" (integer) 1 127.0.0.1:6379\u003e lpush wechat \"today is bad day !\" (integer) 2 127.0.0.1:6379\u003e lpush wechat \"today is good day !\" (integer) 3 127.0.0.1:6379\u003e lpush wechat \"today is rainy day !\" (integer) 4 127.0.0.1:6379\u003e lpush wechat \"today is friday day !\" (integer) 5 获取列表中的数据 # 取最新1条数据 127.0.0.1:6379\u003e lrange wechat 0 0 1) \"today is friday day !\" # 取所有数据 127.0.0.1:6379\u003e lrange wechat 0 -1 1) \"today is friday day !\" 2) \"today is rainy day !\" 3) \"today is good day !\" 4) \"today is bad day !\" 5) \"today is nice day !\" # 取最新的前3条数据 127.0.0.1:6379\u003e lrange wechat 0 2 1) \"today is friday day !\" 2) \"today is rainy day !\" 3) \"today is good day !\" # 取最后2条数据 127.0.0.1:6379\u003e lrange wechat -2 -1 1) \"today is bad day !\" 2) \"today is nice day !\" 列表的 增、删、改、查 操作命令 # 增 lpush mykey a b 若key不存在,创建该键及与其关联的List,依次插入a ,b， 若List类型的key存在,则插入value中 lpushx mykey2 e 若key不存在,此命令无效， 若key存在,则插入value中 linsert mykey before a a1 在 a 的前面插入新元素 a1 linsert mykey after e e2 在e 的后面插入新元素 e2 rpush mykey a b 在链表尾部先插入b,在插入a rpushx mykey e 若key存在,在尾部插入e, 若key不存在,则无效 rpoplpush mykey mykey2 将mykey的尾部元素弹出,再插入到mykey2 的头部(原子性的操作) # 删 del mykey 删除已有键 lrem mykey 2 a 从头部开始找,按先后顺序,值为a的元素,删除数量为2个,若存在第3个,则不删除 ltrim mykey 0 2 从头开始,索引为0,1,2的3个元素,其余全部删除 # 改 lset mykey 1 e 从头开始, 将索引为1的元素值,设置为新值 e,若索引越界,则返回错误信息 rpoplpush mykey mykey 将 mykey 中的尾部元素移到其头部 # 查 lrange mykey 0 -1 取链表中的全部元素，其中0表示第一个元素,-1表示最后一个元素。 lrange mykey 0 2 从头开始,取索引为0,1,2的元素 lrange mykey 0 0 从头开始,取第一个元素,从第0个开始,到第0个结束 lpop mykey 获取头部元素,并且弹出头部元素,出栈 lindex mykey 6 从头开始,获取索引为6的元素 若下标越界,则返回nil ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:3","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"set 案例：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合中。 Redis 还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能， 对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 127.0.0.1:6379\u003e sadd lxl pg1 jnl baoqiang gsy alexsb (integer) 5 127.0.0.1:6379\u003e sadd jnl baoqiang ms bbh yf wxg (integer) 5 # 求并集 127.0.0.1:6379\u003e sunion lxl jnl 1) \"bbh\" 2) \"baoqiang\" 3) \"pg1\" 4) \"alexsb\" 5) \"gsy\" 6) \"ms\" 7) \"yf\" 8) \"jnl\" 9) \"wxg\" # 求交集 127.0.0.1:6379\u003e sinter lxl jnl 1) \"baoqiang\" # 求差集 127.0.0.1:6379\u003e sdiff lxl jnl 1) \"alexsb\" 2) \"pg1\" 3) \"gsy\" 4) \"jnl\" 增、删、改、查 # 增 sadd myset a b c 若 key 不存在, 创建该键及与其关联的 set, 依次插入a ,b, 若key存在, 则插入value中, 若 a 在myset中已经存在,则插入了 d 和 e 两个新成员。 # 删 spop myset 尾部的 b 被移出, 事实上 b 并不是之前插入的第一个或最后一个成员 srem myset a d f 若 f 不存在, 移出 a、d ,并返回2 # 改 smove myset myset2 a 将a从 myset 移到 myset2， # 查 sismember myset a 判断 a 是否已经存在，返回值为 1 表示存在。 smembers myset 查看 set 中的内容 scard myset 获取 set 集合中元素的数量 srandmember myset 随机的返回某一成员 sdiff myset1 myset2 myset3 1和2得到一个结果,拿这个集合和3比较,获得每个独有的值 sdiffstore diffkey myset myset2 myset3 3个集和比较,获取独有的元素,并存入diffkey 关联的Set中 sinter myset myset2 myset3 获得3个集合中都有的元素 sinterstore interkey myset myset2 myset3 把交集存入interkey 关联的Set中 sunion myset myset2 myset3 获取3个集合中的成员的并集 sunionstore unionkey myset myset2 myset3 把并集存入unionkey 关联的Set中 ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:4","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["redis"],"content":"sorted set 应用场景： 排行榜应用，取 TOP N 操作 这个需求与上面需求的不同之处在于，前面操作以时间为权重，这个是以某个条件为权重，比如按顶的次数排序， 这时候就需要我们的 sorted set 出马了，将你要排序的值设置成 sorted set 的score，将具体的数据设置成相应的 value，每次只需要执行一条 ZADD 命令即可。 127.0.0.1:6379\u003e zadd topN 0 smlt 0 fskl 0 fshkl 0 lzlsfs 0 wdhbx 0 wxg (integer) 6 127.0.0.1:6379\u003e ZINCRBY topN 100000 smlt \"100000\" 127.0.0.1:6379\u003e ZINCRBY topN 10000 fskl \"10000\" 127.0.0.1:6379\u003e ZINCRBY topN 1000000 fshkl \"1000000\" 127.0.0.1:6379\u003e ZINCRBY topN 100 lzlsfs \"100\" 127.0.0.1:6379\u003e ZINCRBY topN 100000000 wxg \"100000000\" 127.0.0.1:6379\u003e ZREVRANGE topN 0 2 1) \"wxg\" 2) \"fshkl\" 3) \"smlt\" 127.0.0.1:6379\u003e ZREVRANGE topN 0 2 withscores 1) \"wxg\" 2) \"100000000\" 3) \"fshkl\" 4) \"1000000\" 5) \"smlt\" 6) \"100000\" 增、删、改、查 # 增 zadd myzset 2 \"two\" 3 \"three\" 添加两个分数分别是 2 和 3 的两个成员 # 删 zrem myzset one two 删除多个成员变量,返回删除的数量 # 改 zincrby myzset 2 one 将成员 one 的分数增加 2，并返回该成员更新后的分数 # 查 zrange myzset 0 -1 WITHSCORES 返回所有成员和分数,不加WITHSCORES,只返回成员 zrank myzset one 获取成员one在Sorted-Set中的位置索引值。0表示第一个位置 zcard myzset 获取 myzset 键中成员的数量 zcount myzset 1 2 获取分数满足表达式 1 \u003c= score \u003c= 2 的成员的数量 zscore myzset three 获取成员 three 的分数 zrangebyscore myzset 1 2 获取分数满足表达式 1 \u003c score \u003c= 2 的成员 #-inf 表示第一个成员，+inf最后一个成员 #limit限制关键字 #2 3 是索引号 zrangebyscore myzset -inf +inf limit 2 3 返回索引是2和3的成员 zremrangebyscore myzset 1 2 删除分数 1\u003c= score \u003c= 2 的成员，并返回实际删除的数量 zremrangebyrank myzset 0 1 删除位置索引满足表达式 0 \u003c= rank \u003c= 1 的成员 zrevrange myzset 0 -1 WITHSCORES 按位置索引从高到低,获取所有成员和分数 #原始成员:位置索引从小到大 one 0 two 1 #执行顺序:把索引反转 位置索引:从大到小 one 1 two 0 #输出结果: two one zrevrange myzset 1 3 获取位置索引,为1,2,3的成员 #相反的顺序:从高到低的顺序 zrevrangebyscore myzset 3 0 获取分数 3\u003e=score\u003e=0的成员并以相反的顺序输出 zrevrangebyscore myzset 4 0 limit 1 2 获取索引是1和2的成员,并反转位置索引 ","date":"2021-03-28","objectID":"/posts/redis-operating/:2:5","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-operating/"},{"categories":["kubernetes","traefik"],"content":"Traefik 灰度发布概述 Traefik2.0 的一个更强大的功能就是灰度发布，灰度发布我们有时候也会称为金丝雀发布（Canary），主要就是让一部分测试的服务也参与到线上去，经过测试观察看是否符号上线要求 ","date":"2021-03-10","objectID":"/posts/traefik-canary/:1:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"Traefik 灰度发布","uri":"/posts/traefik-canary/"},{"categories":["kubernetes","traefik"],"content":"测试灰度发布 比如现在我们有两个名为 appv1 和 appv2 的服务，我们希望通过 Traefik 来控制我们的流量，将 3⁄4 的流量路由到 appv1，1/4 的流量路由到 appv2 去，这个时候就可以利用 Traefik2.0 中提供的带权重的轮询（WRR）来实现该功能，首先在 Kubernetes 集群中部署上面的两个服务。为了对比结果我们这里提供的两个服务一个是 whoami，一个是 nginx，方便测试。 appv1 服务的资源清单如下所示：（appv1.yaml） apiVersion:apps/v1kind:Deploymentmetadata:name:appv1spec:selector:matchLabels:app:appv1template:metadata:labels:use:testapp:appv1spec:containers:- name:whoamiimage:traefik/whoamiports:- containerPort:80name:portv1---apiVersion:v1kind:Servicemetadata:name:appv1spec:selector:app:appv1ports:- name:httpport:80targetPort:portv1 appv2 服务的资源清单如下所示：（appv2.yaml） apiVersion:apps/v1kind:Deploymentmetadata:name:appv2spec:selector:matchLabels:app:appv2template:metadata:labels:use:testapp:appv2spec:containers:- name:nginximage:nginxports:- containerPort:80name:portv2---apiVersion:v1kind:Servicemetadata:name:appv2spec:selector:app:appv2ports:- name:httpport:80targetPort:portv2 直接创建上面两个服务： kubectl apply -f appv1.yaml kubectl apply -f appv2.yaml 在 Traefik 2.1 中新增了一个 TraefikService 的 CRD 资源，我们可以直接利用这个对象来配置 WRR，之前的版本需要通过 File Provider，比较麻烦，新建一个描述 WRR 的资源清单：(wrr.yaml) apiVersion:traefik.containo.us/v1alpha1kind:TraefikServicemetadata:name:app-wrrspec:weighted:services:- name:appv1weight:3# 定义权重port:80kind:Service # 可选，默认就是 Service- name:appv2weight:1port:80 然后为我们的灰度发布的服务创建一个 IngressRoute 资源对象：(ingressroute.yaml) apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:wrringressroutenamespace:defaultspec:entryPoints:- webroutes:- match:Host(`wrr.wglee.cn`)kind:Ruleservices:- name:app-wrrkind:TraefikService 不过需要注意的是现在我们配置的 Service 不再是直接的 Kubernetes 对象了，而是上面我们定义的 TraefikService 对象，直接创建上面的两个资源对象，这个时候我们对域名 wrr.wglee.cn 做上解析，去浏览器中连续访问 4 次，我们可以观察到 appv1 这应用会收到 3 次请求，而 appv2 这个应用只收到 1 次请求，符合上面我们的 3:1 的权重配置。 ","date":"2021-03-10","objectID":"/posts/traefik-canary/:2:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"Traefik 灰度发布","uri":"/posts/traefik-canary/"},{"categories":["kubernetes","traefik"],"content":"部署测试 web 应用 使用 Deployment 部署 demoapp， 启动两个 pod 实例， 资源配置清单 demoapp.yaml 如下： apiVersion:apps/v1kind:Deploymentmetadata:labels:app:demoappname:demoappspec:replicas:2selector:matchLabels:app:demoapptemplate:metadata:labels:app:demoappspec:containers:- image:ikubernetes/demoapp:v1.1name:demoapp---apiVersion:v1kind:Servicemetadata:labels:app:demoappname:demoappspec:ports:- name:wwwport:80protocol:TCPtargetPort:80selector:app:demoapptype:ClusterIP 在集群中应用 demoapp.yaml kubectl apply -f demoapp.yaml ","date":"2021-03-10","objectID":"/posts/traefik-ingressroute/:1:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"使用 Traefik 暴露内部服务","uri":"/posts/traefik-ingressroute/"},{"categories":["kubernetes","traefik"],"content":"配置 traefik IngressRoute ingress-route.yaml apiVersion:traefik.containo.us/v1alpha1# 使用 ingress routekind:IngressRoutemetadata:name:demo-ingressnamespace:defaultspec:# 指定使用的入口，由于是 http 流量，我们使用 web 入口（入口在 traefik 命令行配置参数自行配置）entryPoints:- webroutes:# 指定匹配规则- match:Host(`d.wglee.cn`)kind:Ruleservices:- name:demoappport:80 在集群中应用 ingress-route.yaml kubectl apply -f ingress-route.yaml ","date":"2021-03-10","objectID":"/posts/traefik-ingressroute/:2:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"使用 Traefik 暴露内部服务","uri":"/posts/traefik-ingressroute/"},{"categories":["kubernetes","traefik"],"content":"配置负载均衡反向代理 我们把只要是 wglee.cn 这个域名的流量（配置中使用通配符）统一反代到 traefik 的 web 入口 upstream k8s_services { server 192.168.31.21:80; server 192.168.31.22:80; } server { listen 80; server_name *.wglee.cn; location / { proxy_pass http://k8s_services; include proxy.conf; } } ","date":"2021-03-10","objectID":"/posts/traefik-ingressroute/:3:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"使用 Traefik 暴露内部服务","uri":"/posts/traefik-ingressroute/"},{"categories":["kubernetes","traefik"],"content":"查看及测试 打开 traefik-dashboard 页面，查看配置的规则是否生效，使用域名访问此 web 服务。 ","date":"2021-03-10","objectID":"/posts/traefik-ingressroute/:4:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"使用 Traefik 暴露内部服务","uri":"/posts/traefik-ingressroute/"},{"categories":["kubernetes"],"content":"Dashboard 介绍 Kubernetes Dashboard 是 Kubernetes 集群的基于 Web 的通用 UI。 它允许用户管理群集中运行的应用程序并对其进行故障排除，以及管理群集本身。 Dashboard 的 Github 主页：https://github.com/kubernetes/dashboard ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:1:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"部署 Dashboard Dashboard 默认的 YAML 配置文件中是采用 HTTPS 方式访问，本实验中前端 SLB 采用 HTTP 与后端集群中 Ingress 通信，因此需要对该 YAML 文件进行改造，否则会出现以客户端 HTTP 方式请求 HTTPS 接口的报错。 由于 Dashboard 使用自签发的证书, 在连接及配置反向代理多有不便，建议将其改为 HTTP 方式，然后在 nginx 反代处加证书。 从官网下载 Dashboard YAML 配置文件，进行修改，配置如下: # Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion:v1kind:Namespacemetadata:name:kubernetes-dashboard---apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:ServiceapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:ports:# 配置关联 HTTP 服务端口- port:80targetPort:8080#- port: 443# targetPort: 8443selector:k8s-app:kubernetes-dashboard---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-certsnamespace:kubernetes-dashboardtype:Opaque---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-csrfnamespace:kubernetes-dashboardtype:Opaquedata:csrf:\"a36ff54b93460ed9c987ca71a618659e\"---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-key-holdernamespace:kubernetes-dashboardtype:Opaque---kind:ConfigMapapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-settingsnamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardrules:# Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups:[\"\"]resources:[\"secrets\"]resourceNames:[\"kubernetes-dashboard-key-holder\",\"kubernetes-dashboard-certs\",\"kubernetes-dashboard-csrf\"]verbs:[\"get\",\"update\",\"delete\"]# Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups:[\"\"]resources:[\"configmaps\"]resourceNames:[\"kubernetes-dashboard-settings\"]verbs:[\"get\",\"update\"]# Allow Dashboard to get metrics.- apiGroups:[\"\"]resources:[\"services\"]resourceNames:[\"heapster\",\"dashboard-metrics-scraper\"]verbs:[\"proxy\"]- apiGroups:[\"\"]resources:[\"services/proxy\"]resourceNames:[\"heapster\",\"http:heapster:\",\"https:heapster:\",\"dashboard-metrics-scraper\",\"http:dashboard-metrics-scraper\"]verbs:[\"get\"]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\"metrics.k8s.io\"]resources:[\"pods\",\"nodes\"]verbs:[\"get\",\"list\",\"watch\"]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:DeploymentapiVersion:apps/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:k8s-app:kubernetes-dashboardtemplate:metadata:labels:k8s-app:kubernetes-dashboardspec:containers:- name:kubernetes-dashboardimage:kubernetesui/dashboard:v2.0.3","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:2:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"使用 Ingress 暴露 Dashboard 服务 使用 traefik 2.x 版本实现 Ingress 功能，准备 ingressroute.yaml 资源配置清单 apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:dashboard-ingressnamespace:kubernetes-dashboardspec:entryPoints:- webroutes:- match:Host(`dashboard.wglee.cn`)kind:Ruleservices:- name:kubernetes-dashboardport:80 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:3:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"配置 nginx 反代 由于 dashboard 不允许使用 http 登录，所以需要在 nginx 反向代理配置文件中配置 tls 证书 upstream www_pools { server 192.168.31.21:80; server 192.168.31.22:80; } server { listen 443 ssl; server_name dashboard.wglee.cn; ssl_certificate ssl/wglee.cn.crt; ssl_certificate_key ssl/wglee.cn.key; ssl_ciphers \"TLS13-AES-256-GCM-SHA384:TLS13-CHACHA20-POLY1305-SHA256:TLS13-AES-128-GCM-SHA256:TLS13-AES-128-CCM-8-SHA256:TLS13-AES-128-CCM-SHA256:EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5\"; ssl_session_cache builtin:1000 shared:SSL:10m; location / { proxy_pass http://www_pools; include proxy.conf; } } server { listen 80; server_name dashboard.wglee.cn; access_log off; return 301 https://dashboard.wglee.cn$request_uri; } 此时我们可以通过域名访问 kubernetes-dashboard 页面 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:4:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"创建集群管理员账号 由于 kubernetes-dashboard 需要使用集群用户验证，我们先准备集群管理员账号配置清单 admin-user.yaml ---apiVersion:v1kind:ServiceAccountmetadata:name:adminnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:adminroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:adminnamespace:kubernetes-dashboard 应用 admin-user.yaml 配置，在 kubernetes-dashboard 名称空间里创建集群管理员账号。 获取 admin 管理员用户的 token kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" 使用 admin 管理员 token 登录 dashboard 管理页面 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:5:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"部署 Metrics Server 在 kubernetes 中 HPA 自动伸缩指标依据，kubectl top 命令的资源使用率，可以通过 metrics-server 来获取。 dashboard 也会引用 metrics-server 展示资源负载情况图表。但是官方明确表示，该指标不应该用于监控指标采集。 官方主页: https://github.com/kubernetes-sigs/metrics-server 在大部分情况下，使用 deployment 部署一个副本即可，最多支持5000个 node，每个 node 消耗 3m CPU 和 3M 内存 可以从官方主页获取资源配置清单文件，也可以直接使用下面的配置文件。 metrics-server 配置参数 - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --kubelet-insecure-tls 部署 metrics-server kubectl apply -f metrics-server.yaml 由于国内环境导致无法下载 metrics-server 镜像，在网上也没有找到可用的代理的服务器。请自行解决此问题。 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:6:0","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"Dashboard 页面效果展示 metrics-server 安装完成后可以在 dashboard 页面中看到效果 dashboard metrics ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:6:1","tags":["kubernetes","dashboard","k8s"],"title":"部署 Kubernetes Dashboard","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes","traefik"],"content":"Traefik 简介 Traefik 是一个开源的可以使服务发布变得轻松有趣的边缘路由器。它负责接收你系统的请求，然后使用合适的组件来对这些请求进行处理。 除了众多的功能之外，Traefik 的与众不同之处还在于它会自动发现适合你服务的配置。当 Traefik 在检查你的服务时，会找到服务的相关信息并找到合适的服务来满足对应的请求。 Traefik 兼容所有主流的集群技术，比如 Kubernetes，Docker，Docker Swarm，AWS，Mesos，Marathon，等等；并且可以同时处理多种方式。（甚至可以用于在裸机上运行的比较旧的软件。） 使用 Traefik，不需要维护或者同步一个独立的配置文件：因为一切都会自动配置，实时操作的（无需重新启动，不会中断连接）。使用 Traefik，你可以花更多的时间在系统的开发和新功能上面，而不是在配置和维护工作状态上面花费大量时间。 官方文档 – https://doc.traefik.io/traefik/ ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:1:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"Kubernetes 部署 Traefik 2.x","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes","traefik"],"content":"核心概念 Traefik 是一个边缘路由器，是你整个平台的大门，拦截并路由每个传入的请求：它知道所有的逻辑和规则，这些规则确定哪些服务处理哪些请求；传统的反向代理需要一个配置文件，其中包含路由到你服务的所有可能路由，而 Traefik 会实时检测服务并自动更新路由规则，可以自动服务发现。 首先，当启动 Traefik 时，需要定义 entrypoints（入口点），然后，根据连接到这些 entrypoints 的路由来分析传入的请求，来查看他们是否与一组规则相匹配，如果匹配，则路由可能会将请求通过一系列中间件转换过后再转发到你的服务上去。在了解 Traefik 之前有几个核心概念我们必须要了解： Providers 用来自动发现平台上的服务，可以是编排工具、容器引擎或者 key-value 存储等，比如 Docker、Kubernetes、File Entrypoints 监听传入的流量（端口等…），是网络入口点，它们定义了接收请求的端口（HTTP 或者 TCP）。 Routers 分析请求（host, path, headers, SSL, …），负责将传入请求连接到可以处理这些请求的服务上去。 Services 将请求转发给你的应用（load balancing, …），负责配置如何获取最终将处理传入请求的实际服务。 Middlewares 中间件，用来修改请求或者根据请求来做出一些判断（authentication, rate limiting, headers, …），中间件被附件到路由上，是一种在请求发送到你的服务之前（或者在服务的响应发送到客户端之前）调整请求的一种方法。 ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:2:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"Kubernetes 部署 Traefik 2.x","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes","traefik"],"content":"安装 Traefik 2 由于 Traefik 2.X 版本和之前的 1.X 版本不兼容，我们这里选择功能更加强大的 2.X 版本来和大家进行讲解，我们这里使用的镜像是 traefik:2.4。 在 Traefik 中的配置可以使用两种不同的方式： 动态配置：完全动态的路由配置 静态配置：启动配置 静态配置中的元素（这些元素不会经常更改）连接到 providers 并定义 Treafik 将要监听的 entrypoints。 在 Traefik 中有三种方式定义静态配置：在配置文件中、在命令行参数中、通过环境变量传递 动态配置包含定义系统如何处理请求的所有配置内容，这些配置是可以改变的，而且是无缝热更新的，没有任何请求中断或连接损耗。 安装 Traefik 到 Kubernetes 集群中的资源清单文件可以到官方文档中找到，链接: https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/ 将配置资源清单保存到本地，资源清单文件我这里准备好了（做了些修改，traefik 容器网络模式改为与共用宿主机网络，详细查看配置文件），通过以下命令应用。 请根据自己的需求修改资源配置清单 kubectl apply -f https://liwanggui.com/files/k8s/traefik2.x/crd.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2.x/rbac.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2.x/traefik.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2.x/dashboard.yaml 其中 traefik.yaml 使用的是 DaemonSet 部署方式，如果你需要修改可以下载下来做相应的修改即可。我们这里是通过命令行参数来做的静态配置： args: - --log.level=INFO - --accesslog - --api=true # 开启 api/dashboard 会创建一个名为 api@internal 的特殊 service，在 dashboard 中可以直接使用这个 service 来访问 - --api.insecure - --entrypoints.web.address=:80 # 定义名为 web 的入口 - --entryPoints.websecure.address=:443 # 定义名为 websecure 的入口 - --entrypoints.tcpep.address=:8000 # 定义名为 tcpep 的入口 - --entrypoints.web.forwardedheaders.insecure # 信任 web 入口 所有转发的 header 头信息，可以用于获取客户端真实 IP 地址 - --entrypoints.websecure.forwardedheaders.insecure # 信任 websecure 入口 所有转发的 header 头信息，可以用于获取客户端真实 IP 地址 - --providers.kubernetescrd - --providers.kubernetesingress dashboard.yaml 中定义的是访问 dashboard 的资源清单文件，可以根据自己的需求修改。 $ kubectl get pods -n kube-system -l app=traefik NAME READY STATUS RESTARTS AGE traefik-7s6wk 1/1 Running 0 7m27s traefik-bx6m8 1/1 Running 0 7m27s 部署完成后我们可以通过域名 traefik.wglee.cn 访问 Traefik 的 Dashboard 页面了 关于更多关于 traefik 功能配置可以参考 https://www.qikqiak.com/post/traefik-2.1-101/ ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:3:0","tags":["kubernetes","k8s","traefik","traefik2"],"title":"Kubernetes 部署 Traefik 2.x","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes"],"content":"主机环境 本示例中的 Kubernetes 集群部署将基于以下环境进行。 OS: Ubuntu 18.04.5 Kubernetes：v1.18.1 Container Runtime: Docker CE 19.03.15 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:1:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"环境说明 测试使用的 kubernetes 集群可由一个 master 主机及一个以上（建议至少两个）node 主机组成，这些主机可以是物理服务器，也可以运行于 vmware、virtualbox 或 kvm 等虚拟化平台上的虚拟机，甚至是公有云上的VPS主机。 本测试环境将由 master1.host.com、node1.host.com、node2.host.com 3个独立的主机组成，它们分别拥有4核心的CPU及4G的内存资源，操作系统环境均为仅小化部署的 Ubuntu Server 18.04.5 LTS，启用了 SSH 服务，域名为 host.com。此外，各主机需要预设的系统环境如下： 借助于 chronyd 服务（程序包名称 chrony）设定各节点时间精确同步； 通过 DNS 完成各节点的主机名称解析； 各节点禁用所有的 Swap 设备； 各节点禁用默认配置的 iptables 防火墙服务； 注意：为了便于操作，后面将在各节点直接以系统管理员 root 用户进行操作。若用户使用了普通用户，建议将如下各命令以 sudo 方式运行。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:2:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"设定时钟同步 若节点可直接访问互联网，安装 chrony 程序包后，可直接启动 chronyd 系统服务，并设定其随系统引导而启动。随后，chronyd 服务即能够从默认的时间服务器同步时间。 apt install chrony systemctl start chronyd.service 不过，建议用户配置使用本地的的时间服务器，在节点数量众多时尤其如此。存在可用的本地时间服务器时，修改节点的 /etc/chrony/chrony.conf 配置文件，并将时间服务器指向相应的主机即可，配置格式如下： server CHRONY-SERVER-NAME-OR-IP iburst ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:3:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"主机名称解析 使用 bind9 提供 dns 服务, 本环境直接在主节点安装 bind9 apt install bind9 更新多详细配置参考 Ubuntu Server 安装配置 bind9 host.com zone 配置文件示例 ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA host.com. root.host.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS ns.host.com. @ IN A 192.168.31.11 ns IN A 192.168.31.11 master1 IN A 192.168.31.11 node1 IN A 192.168.31.21 node2 IN A 192.168.31.22 apiserver IN A 192.168.31.11 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:4:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"禁用 Swap 设备 部署集群时，kubeadm 默认会预先检查当前主机是否禁用了 Swap 设备，并在未禁用时强制终止部署过程。因此，在主机内存资源充裕的条件下，需要禁用所有的 Swap 设备，否则，就需要在后文的 kubeadm init 及 kubeadm join 命令执行时额外使用相关的选项忽略检查错误。 关闭 Swap 设备，需要分两步完成。首先是关闭当前已启用的所有 Swap 设备： swapoff -a 而后编辑 /etc/fstab 配置文件，注释用于挂载 Swap 设备的所有行。 另外，若确需在节点上使用 Swap 设备，也可选择让 kubeam 忽略 Swap 设备的相关设定。我们编辑 kubelet 的配置文件 /etc/default/kubelet，设置其忽略 Swap 启用的状态错误即可，文件内容如下： KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\" ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:5:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"禁用默认的防火墙服务 Ubuntu 和 Debian 等 Linux 发行版默认使用 ufw（Uncomplicated FireWall）作为前端来简化 iptables 的使用，处于启用状态时，它默认会生成一些规则以加强系统安全。出于降低配置复杂度之目的，本文选择直接将其禁用。 ufw disable ufw status ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:6:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装程序包 提示：以下操作需要在本示例中的所有三台主机上分别进行 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:7:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装 docker 此时我们需要安装指定版本的 docker，docker 安装请参考 Docker 快速安装 kubelet 需要让 docker 容器引擎使用 systemd 作为 CGroup 的驱动，其默认值为 cgroupfs，因而，我们还需要编辑 docker 的配置文件 /etc/docker/daemon.json，添加如下内容 { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"], # 镜像加速器 \"insecure-registries\":[\"harbor.host.com\"], # 第三方仓库或自建仓库地址，可以配置为 http \"data-root\": \"/data/docker\", # docker 数据存储目录 \"exec-opts\": [\"native.cgroupdriver=systemd\"], # 额外参数,部署 k8s 时需要指定此选项 \"bip\": \"10.10.0.1/16\", # 配置 docker 网桥 ip \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\"}, \"live-restore\": true } 配置完成后即可启动 docker 服务，并将其设置为随系统启动而自动引导： systemctl daemon-reload systemctl start docker.service systemctl enable docker.service ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:7:1","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装配置 kubelet 和 kubeadm 首先，在各主机上生成 kubelet 和 kubeadm 等相关程序包的仓库，这里以阿里云的镜像服务为例： apt update \u0026\u0026 apt install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003e/etc/apt/sources.list.d/kubernetes.list\u003c\u003cEOF deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update 接着，在各主机安装 kubelet、kubeadm 和 kubectl 程序包，并将其设置为随系统启动而自动引导： apt install -y kubelet=1.18.1-00 kubeadm=1.18.1-00 kubectl=1.18.1-00 systemctl enable kubelet 安装完成后，要确保 kubeadm 等程序文件的版本，这将也是后面初始化 Kubernetes 集群时需要明确指定的版本号 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:7:2","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化第一个主节点 该步骤开始尝试构建 Kubernetes 集群的 master 节点，配置完成后，各 worker 节点直接加入到集群中的即可。需要特别说明的是，由 kubeadm 部署的 Kubernetes 集群上，集群核心组件 kube-apiserver、kube-controller-manager、kube-scheduler``和etcd 等均会以静态 Pod 的形式运行，它们所依赖的镜像文件默认来自于 gcr.io 这一 Registry 服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种，本示例将选择使用更易于使用的后一种方式。 使用能够到达该服务的代理服务； 使用国内的镜像服务器上的服务，例如 gcr.azk8s.cn/google_containers 和 registry.aliyuncs.com/google_containers 等。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化 master 节点 在 master1.host.com 上完成如下操作 在运行初始化命令之前先运行如下命令单独获取相关的镜像文件，而后再运行后面的 kubeadm init 命令，以便于观察到镜像文件的下载过程。 # 查看镜像列表 kubeadm config images list --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.1 # 获取镜像文件至本地 kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.1 而后即可进行 master 节点初始化。kubeadm init 命令支持两种初始化方式，一是通过命令行选项传递关键的部署设定，另一个是基于 yaml 格式的专用配置文件，后一种允许用户自定义各个部署参数。下面分别给出了两种实现方式的配置步骤，建议采用第二种方式进行。 初始化方式一 运行如下命令完成 master 节点的初始化： kubeadm init \\ --image-repository registry.aliyuncs.com/google_containers \\ --control-plane-endpoint apiserver.host.com \\ --kubernetes-version v1.18.1 \\ --apiserver-advertise-address 192.168.31.11 \\ --service-cidr 10.10.0.0/16 \\ --pod-network-cidr 172.16.0.0/16 \\ --token-ttl 0 \\ --upload-certs 命令中的各选项简单说明如下： --image-repository： 指定要使用的镜像仓库，默认为 gcr.io； --kubernetes-version：kubernetes 程序组件的版本号，它必须要与安装的 kubelet 程序包的版本号相同； --control-plane-endpoint：控制平面的固定访问端点，可以是 IP 地址或 DNS 名称，会被用于集群管理员及集群组件的 kubeconfig 配置文件的 API Server 的访问地址；单控制平面部署时可以不使用该选项； --pod-network-cidr：Pod 网络的地址范围，其值为 CIDR 格式的网络地址，通常，Flannel 网络插件的默认为 10.244.0.0/16, Calico 插件的默认值为 192.168.0.0/16； --service-cidr：Service 的网络地址范围，其值为 CIDR 格式的网络地址，默认为 10.96.0.0/12；通常，仅 Flannel一类的网络插件需要手动指定该地址； --apiserver-advertise-address：apiserver 通告给其他组件的IP地址，一般应该为 Master 节点的用于集群内部通信的IP地址，0.0.0.0 表示节点上所有可用地址； --upload-certs: 将控制平面证书上传到 kubeadm-certs secret。 --token-ttl：共享令牌（token）的过期时长，默认为 24小时，0 表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在 token 过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建 token，并生成节点加入命令。 kubeadm token create --print-join-command 需要注意的是，若各节点未禁用Swap设备，还需要附加选项 “–ignore-preflight-errors=Swap”，从而让kubeadm忽略该错误设定。 初始化方式二 kubeadm 也可通过配置文件加载配置，以定制更丰富的部署选项。以下是个符合前述命令设定方式的使用示例，不过，它明确定义了 kubeProxy 的模式为 ipvs，并支持通过修改 imageRepository 的值修改获取系统镜像时使用的镜像仓库。 默认配置可以通过 kubeadm config print init-defaults 获取 ---apiVersion:kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:- system:bootstrappers:kubeadm:default-node-tokenttl:24h0m0susages:- signing- authenticationkind:InitConfigurationlocalAPIEndpoint:# 这里的地址即为初始化的控制平面第一个节点的IP地址；advertiseAddress:192.168.31.11bindPort:6443nodeRegistration:criSocket:/var/run/dockershim.sock# 第一个控制平面节点的主机名称；name:master1.host.comtaints:- effect:NoSchedulekey:node-role.kubernetes.io/master---apiServer:timeoutForControlPlane:4m0sapiVersion:kubeadm.k8s.io/v1beta2# 控制平面的接入端点，我们这里选择适配到 apiserver.host.com 这一域名上controlPlaneEndpoint:\"apiserver.host.com:6443\"certificatesDir:/etc/kubernetes/pkiclusterName:kubernetescontrollerManager:{}dns:type:CoreDNSetcd:local:# 配置 etcd 数据存储路径dataDir:/data/etcd# 配置镜像拉取站点imageRepository:registry.aliyuncs.com/google_containerskind:ClusterConfiguration# 版本号要与部署的目标版本保持一致kubernetesVersion:v1.18.1networking:# 要使用的集群域名，默认为 cluster.localdnsDomain:cluster.local# Pod 的网络地址段podSubnet:172.16.0.0/16# Service 的网络地址段serviceSubnet:10.10.0.0/16scheduler:{}---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfiguration# 用于配置 kube-proxy 上为 Service 指定的代理模式，默认为 iptablesmode:\"ipvs\"ipvs:scheduler:\"nq\" 将上面的内容保存于配置文件中，例如 kubeadm-config.yaml，而后执行如下命令即能实现类似前一种初始化方式中的集群初始配置，但这里将 Service 的代理模式设定为了 ipvs。 初始化命令 kubeadm init --config kubeadm-config.yaml kubeadm init 命令完整参考指南请移步官方文档，地址为 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:1","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化完成后的操作步骤 注意：对于 Kubernetes 系统的新用户来说，无论使用上述哪种方法，命令运行结束后，请记录最后的 kubeadm join 命令输出的最后提示的操作步骤。下面的内容是需要用户记录的一个命令输出示例，它提示了后续需要的操作步骤： [init] Using Kubernetes version: v1.18.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [master1.host.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local apiserver.host.com] and IPs [10.10.0.1 192.168.31.11] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [master1.host.com localhost] and IPs [192.168.31.11 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [master1.host.com localhost] and IPs [192.168.31.11 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" W0309 06:48:52.172442 48909 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" W0309 06:48:52.173496 48909 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 26.002622 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.18\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master1.host.com as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node master1.host.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: x3oo6y.ytmywnftdx6khuh5 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [boot","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:2","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"向集群添加额外的控制平面节点 在添加额外主节点之前我们需要将集群证书上传到集群中以便向其它主节点共享证书并生成证书密钥，使用此密钥可以解密由 init 上载的证书。 使用如下命令命令完成。 kubeadm init phase upload-certs --upload-certs I0309 07:16:31.594295 62834 version.go:252] remote version is much newer: v1.20.4; falling back to: stable-1.18 W0309 07:16:35.449971 62834 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace [upload-certs] Using certificate key: 7c3c34ac69d980bdaf28eb42e38186c71245cef7c5926d0fb252b7200aad05bf 也可以直接拷贝 /etc/kubernetes/pki 至另一台主节点上的 /etc/kubernetes 目录下 集群添加额外的主节点，使用如下命令完成。 kubeadm join apiserver.host.com:6443 --token x3oo6y.ytmywnftdx6khuh5 \\ --discovery-token-ca-cert-hash sha256:7708b5166572b6f33094b27d3c457d080213ec3bd701161d4d648367c53f1013 \\ --control-plane --certificate-key 7c3c34ac69d980bdaf28eb42e38186c71245cef7c5926d0fb252b7200aad05bf 本次实验由于资源有限只部署一台主节点，实际生产环境中建议部署多台，避免单点故障。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:9:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"添加节点到集群中 下面的两个步骤，需要分别在 node1.host.com 和 node2.host.com 上完成。 1、若未禁用 Swap 设备，编辑 kubelet 的配置文件 /etc/default/kubelet，设置其忽略 Swap 启用的状态错误，内容如下： KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\" 2、将节点加入 master 的集群中，要使用主节点初始化过程中记录的 kubeadm join 命令，并且在未禁用 Swap 设备的情况下，额外附加 “--ignore-preflight-errors=Swap” 选项； kubeadm join apiserver.host.com:6443 --token x3oo6y.ytmywnftdx6khuh5 \\ --discovery-token-ca-cert-hash sha256:7708b5166572b6f33094b27d3c457d080213ec3bd701161d4d648367c53f1013 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:10:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"验证节点添加结果 在每个节点添加完成后，即可通过 kubectl 验正添加结果。下面的命令及其输出是在 node1 和 node2 均添加完成后运行的，其输出结果表明两个 Node 已经准备就绪。 kubectl get nodes NAME STATUS ROLES AGE VERSION master1.host.com Ready master 41m v1.18.1 node1.host.com Ready \u003cnone\u003e 51s v1.18.1 node2.host.com Ready \u003cnone\u003e 43s v1.18.1 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:10:1","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"测试应用编排及服务访问 到此为止，一个 master，并附带有二个 node 的 kubernetes 集群基础设施已经部署完成，用户随后即可测试其核心功能。例如，下面的命令可将 demoapp 以 Pod 的形式编排运行于集群之上，并通过在集群外部进行访问： kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 kubectl scale deployment/demoapp --replicas=2 kubectl create service nodeport demoapp --tcp=80:80 而后，使用如下命令了解 Service 对象 demoapp 使用的 NodePort，以便于在集群外部进行访问 kubectl get svc -l app=demoapp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demoapp NodePort 10.10.149.76 \u003cnone\u003e 80:32002/TCP 10s demoapp 是一个 web 应用，因此，用户可以于集群外部通过 http://NodeIP:32002 这个 URL 访问 demoapp 上的应用，例如于集群外通过浏览器访问 http://192.168.31.22:32002。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:11:0","tags":["kubernetes","kubeadm","k8s"],"title":"Kubernetes 集群部署 (kubeadm)","uri":"/posts/kubernetes-install/"},{"categories":["docker"],"content":" docker 安装请参考: – Docker 快速安装 ","date":"2021-03-06","objectID":"/posts/docker-cli/:0:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"镜像管理 ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"1. 获取镜像 # 默认从 dockerhub 拉取最新版本镜像 [root@localhost ~]# docker pull busybox Using default tag: latest latest: Pulling from library/busybox add3ddb21ede: Pull complete Digest: sha256:b82b5740006c1ab823596d2c07f081084ecdb32fd258072707b99f52a3cb8692 Status: Downloaded newer image for busybox:latest # 拉取指定版本的镜像 [root@localhost ~]# docker pull ubuntu:14.04 14.04: Pulling from library/ubuntu 48f0413f904d: Downloading [======\u003e ] 8.925MB/67.12MB 2bd2b2e92c5f: Download complete 06ed1e3efabb: Download complete a220dbf88993: Waiting 57c164185602: Waiting ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:1","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"2. 列出镜像 [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB ubuntu 14.04 c69811d4e993 4 weeks ago 188MB ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:2","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"3. 删除镜像 [root@localhost ~]# docker rmi ubuntu:14.04 Untagged: ubuntu:14.04 Untagged: ubuntu@sha256:6a3e01207b899a347115f3859cf8a6031fdbebb6ffedea6c2097be40a298c85d Deleted: sha256:c69811d4e9931740c0a490f74fafb566bb520b945f6e62cab96f6faecd750b95 Deleted: sha256:5294610fabc319f443fc036f7bf5c02299f2614d4b0f79c87529bb9aef46ce4e Deleted: sha256:a783f54895fb2d76726d8b4fbbb263bcffc0cbb7fe858450a39d21b7f4de1df6 Deleted: sha256:e11129e7baf41455394f83970e3e232fa7c33a87948b76ca1c121942c4f0403f Deleted: sha256:38c3fb0ca70b3e0444085376821508b758e4f30b290a0016c4b044b9f46bddf8 Deleted: sha256:826fc2344fbbc40cf9f2714c831a0d3ff88596e471f71c33b1055f3913d829d4 ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:3","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"4. 保存镜像 [root@localhost ~]# docker save ubuntu:latest -o ubuntu-latest.tar [root@localhost ~]# ls -lh total 44M -rw-r--r-- 1 root root 72.9M Sep 9 19:20 ubuntu-latest.tar 保存并压缩 [root@localhost ~]# docker save ubuntu:latest | gzip \u003e ubuntu-latest.tar.gz [root@localhost ~]# ls -lh total 44M -rw-r--r-- 1 root root 44M Sep 9 19:20 ubuntu-latest.tar.gz ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:4","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"5. 载入镜像 [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB [root@localhost ~]# docker load -i ubuntu-latest.tar.gz 8aa4fcad5eeb: Loading layer [==================================================\u003e] 124.1MB/124.1MB 25e0901a71b8: Loading layer [==================================================\u003e] 15.87kB/15.87kB 625c7a2a783b: Loading layer [==================================================\u003e] 11.78kB/11.78kB 9c42c2077cde: Loading layer [==================================================\u003e] 5.632kB/5.632kB a09947e71dc0: Loading layer [==================================================\u003e] 3.072kB/3.072kB Loaded image: ubuntu:latest [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB PS: 结合以上命令可以使用 shell 命令完成镜像迁移工作 docker save \u003c镜像名\u003e | bzip2 | pv | ssh \u003c用户名\u003e@\u003c主机名\u003e 'cat | docker load' ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:5","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"使用容器 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"1. 启动/停止/重启容器 # -d 参数表示守护进程方式运行 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 727dbeacc1f5 ubuntu:latest \"/bin/bash\" 3 seconds ago Up 2 seconds inspiring_goldberg # 停止容器，重启，启动指令为 restart, start [root@localhost ~]# docker container stop inspiring_goldberg inspiring_goldberg [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 727dbeacc1f5 ubuntu:latest \"/bin/bash\" About a minute ago Exited (0) 1 second ago inspiring_goldberg ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:1","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"2. 删除容器 [root@localhost ~]# docker container rm 727dbeacc1f5 727dbeacc1f5 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 容器删除前需要先停止 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:2","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"3. 导出容器 [root@localhost ~]# docker container export 84bc66973544 \u003e ubuntu-latest.tar ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:3","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"4. 导入容器为镜像 [root@localhost ~]# cat ubuntu-latest.tar | docker import - test/ubuntu:latest sha256:389b10ce91abd80cc9b306cbc02cfc74b5089ba60766d8fd66af48691ab9d6fc [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE test/ubuntu latest 389b10ce91ab 5 seconds ago 97.9MB busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以 使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容 器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状 态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入 时可以重新指定标签等元数据信息。 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:4","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"5. 进入容器 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 84bc66973544 ubuntu:latest \"/bin/bash\" 12 minutes ago Up 12 minutes hungry_bartik [root@localhost ~]# docker exec -ti 84bc66973544 bash root@84bc66973544:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"16.04.3 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.3 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:5","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"harbor 使用 docker 容器的方式部署，所以在部署 harbor 前需要安装好 docker 及单机编排工具 docker-compose ","date":"2021-03-06","objectID":"/posts/docker-harbor/:0:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"下载 harbor 离线安装包 harbor 托管于 Github，在 Github 上有提供完整的离线安装直接下载即可。 Github 地址 root@ops:/opt# wget https://github.com/goharbor/harbor/releases/download/v2.1.0/harbor-offline-installer-v2.1.0.tgz ","date":"2021-03-06","objectID":"/posts/docker-harbor/:1:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"安装 docker-compose 由于 harbor 依赖于 docker-compose 完成单机编排工作，需要先安装好 root@ops:/opt# curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose root@ops:/opt# chmod +x /usr/local/bin/docker-compose ","date":"2021-03-06","objectID":"/posts/docker-harbor/:2:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"部署 harbor root@ops:/opt# tar xzf harbor-offline-installer-v2.1.0.tgz root@ops:/opt# cd harbor # 复制一份harbor配置文件 root@ops:/opt/harbor# cp harbor.yml.tmpl harbor.yml root@ops:/opt/harbor# egrep -v '^$|#' harbor.yml hostname: harbor.lwg.com # harbor 站点域名 http: port: 801 # 修改端口 harbor_admin_password: Harbor12345 # harbor 管理员默认密码 database: password: root123 max_idle_conns: 50 max_open_conns: 100 data_volume: /data/harbor # harbor 数据存储路径 clair: updaters_interval: 12 trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /data/harbor/logs # harbor 日志文件路径 _version: 2.0.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - clair - trivy root@ops:/opt/harbor# ./install # 开始部署 harbor 部署完成后可以配置 nginx 反代对外提供服务，nginx 配置注意加大 client_max_body_size 参数值. ","date":"2021-03-06","objectID":"/posts/docker-harbor/:3:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":" Dockerfile 是一个文本格式的配置文件，用户可以使用 Dockerfile 快速创建自定义的镜像。 Dockerfile 由一行行命令语句组成，并且支持以 # 开头注释行。 Dockerfile 一般分为四部分： 基础镜像信息，维护者信息、镜像操作指令和容器启动时执行指令。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:0:0","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"指令 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:0","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"FROM 格式: FROM \u003cimage\u003e 或者 FROM \u003cimage\u003e:\u003ctag\u003e Dockerfile 的第一条指令必须是 FROM ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:1","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"MAINTAINER - 弃用 格式: MAINTAINER \u003cname\u003e，指定维护者信息 推荐使用 LABEL maintainer=\"SvenDowideit@home.org.au\" ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:2","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"LABLE 格式: LABEL \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e ... 该指令将元数据添加到 docker 镜像中 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:3","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"USER 格式: USER \u003cusername\u003e 指定容器内进程使用的用户名或 UID, 当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:4","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"WORKDIR 格式: WORKDIR /path/to/workdir 指定容器的当前工作路径，为后续的 RUN、CMD、ENTRYPINT 指令配置工作路径 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:5","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ENV 格式: ENV \u003ckey\u003e \u003cvalue\u003e 指定一个环境变量，可以被 RUN 指令使用，并在容器运行时保持存在 ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 可以同时指定多个变量，推荐使用这种方式 ENV PG_MAJOR=9.3 PG_VERSION=9.3.4 RUN curl -SL http://example.com/postgres-${PG_VERSION}.tar.gz | tar -xzC /usr/src \u0026\u0026 ... ENV PATH=/usr/local/postgres-$PG_MAJOR/bin:$PATH ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:6","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ARG 格式: ARG \u003ckey\u003e=\u003cvalue\u003e ... 该指令定义了一个变量，只在构建镜像时生效。可以只定义变量名(或者默认值)，然后使用命令 docker build --build-arg 参数, 传递变量值的给构建者。 如果用户指定了Dockerfile中未定义的构建参数，则该构建会输出警告。ARGdocker build--build-arg \u003cvarname\u003e=\u003cvalue\u003e ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:7","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ADD 格式: ADD \u003csrc\u003e \u003cdest\u003e 该指令将复制指定的 \u003csrc\u003e 到容器中的 \u003cdest\u003e. 其中 \u003csrc\u003e 可以是 Dockerfile 所在目录的一个相对路径(文件和目录)； 也可以是一个 url；还可以是一个 tar 文件（自动解压为目录） ADD hom* /mydir/ ADD hom?.txt /mydir/ ADD --chown=55:mygroup files* /somedir/ ADD --chown=bin files* /somedir/ ADD --chown=1 files* /somedir/ ADD --chown=10:11 files* /somedir/ ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:8","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"COPY 格式: COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] 该指令复制文件或目录，并将它们添加到容器的文件系统路径中; 可以指定多个资源，但文件和目录的路径将基于构建路径。 COPY hom* /mydir/ COPY hom?.txt /mydir/ COPY --chown=55:mygroup files* /somedir/ COPY --chown=bin files* /somedir/ COPY --chown=1 files* /somedir/ COPY --chown=10:11 files* /somedir/ ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:9","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"RUN 格式: RUN \u003ccommand\u003e 或者 RUN [\"executable\", \"param1\", \"param2\"] 每条 RUN 指令将在当前镜像的基础上执行指定的命令，并提交为新镜像。当命令过长时可以使用 \\ 换行。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:10","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"EXPOSE 格式: EXPOSE \u003cport\u003e [\u003cport\u003e ...] 示例: EXPOSE 22 80 443 EXPOSE 指令用于暴露容器端口。在启动容器时需要通过 -P, Docker 服务会随机分配一个端口转发到指定的端口； 使用 -p, 可以手动指定具体本地的端口与容器端口映射。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:11","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"VOLUME 格式: VOLUME [\"/data\"] 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放需要持久化的数据等。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:12","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"CMD 格式: CMD [\"executable\", \"param1\", \"param2\"] 使用 exec 执行，推荐方式 CMD command param1 param2 ,在 /bin/sh 中执行 CMD [\"param1\", \"param2\"] 提供给 ENTRYPOINT 的默认参数 指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 指令。如果指定多条，只有最后一条生效被执行。 如果在启动容器时指定了运行的命令，会覆盖掉 CMD 指定的命令。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:13","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ENTRYPOINT 格式: ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖. 每个 Dockerfile 中只能有一个 ENTRYPOINT，指定多个 ENTRYPOINT 时，只有最一个有效。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:14","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"完整的配置项参考: https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file docker 默认配置文件路径为: /etc/docker/daemon.json ","date":"2021-03-06","objectID":"/posts/docker-config/:0:0","tags":["docker"],"title":"Docker 基本配置项","uri":"/posts/docker-config/"},{"categories":["docker"],"content":"docker 常用的配置项 [root@localhost ~]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"], # 镜像加速器 \"insecure-registries\":[\"harbor.host.com\"], # 第三方仓库或自建仓库地址，可以配置为 http \"data-root\": \"/data/docker\", # docker 数据存储目录 \"exec-opts\": [\"native.cgroupdriver=systemd\"], # 额外参数,部署 k8s 时需要指定此选项 \"bip\": \"10.10.0.1/16\", # 配置 docker 网桥 ip \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\"}, \"live-restore\": true } ","date":"2021-03-06","objectID":"/posts/docker-config/:1:0","tags":["docker"],"title":"Docker 基本配置项","uri":"/posts/docker-config/"},{"categories":["docker"],"content":"一键安装 docker Docker 官方提供了一键安装 docker 脚本工具: https://github.com/docker/docker-install ","date":"2021-03-06","objectID":"/posts/docker-install/:1:0","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"安装 docker curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh 默认下载源是 docker 官方境外的源，在国内下载很慢 ","date":"2021-03-06","objectID":"/posts/docker-install/:1:1","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"使用阿里云安装 docker curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh --mirror=Aliyun ","date":"2021-03-06","objectID":"/posts/docker-install/:1:2","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"配置阿里源安装 docker # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs)stable\" # Step 4: 更新并安装Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce ","date":"2021-03-06","objectID":"/posts/docker-install/:2:0","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"安装指定版本的Docker-CE 1: 查找 Docker-CE的版本: root@ops:~# apt-cache madison docker-ce docker-ce | 5:20.10.5~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.4~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.3~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.2~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.1~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.0~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.15~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.14~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.13~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.12~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.11~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.10~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.9~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.8~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.7~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages ... 2: 安装指定版本的 Docker-CE: (VERSION 例如上面的 5:19.03.15~3-0~ubuntu-bionic) root@ops:~# apt-get -y install docker-ce=5:19.03.15~3-0~ubuntu-bionic 注意: 如果部署 kubernetes 集群，就是需要安装经过 kubernetes 验证过的 docker 版本 ","date":"2021-03-06","objectID":"/posts/docker-install/:2:1","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["ubuntu"],"content":"如果你没有在 Linux 下安装和运行 Systemd-Resolved、DNSMasq、Nscd 缓存服务，那就没有操作系统级的 DNS 缓存，不同的 Linux 发行版在刷新 DNS 缓存上方法是不同的。 以下操作在 Ubuntu 18.04 操作系统下进行 ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:0:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 Systemd Resolved 缓存 Ubuntu 18.04 系统是使用 Systemd Resolved 服务来缓存 DNS 的，所以可以运行以下命令确定该服务是否运行： sudo systemctl is-active systemd-resolved.service 如果服务运行，则会看到返回的活动状态信息，否则只会看到非活动状态。 删除 Systemd Resolved DNS 缓存的方法，运行以下命令： sudo systemd-resolve --flush-caches ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:1:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 DNSMasq 缓存 如果你在 Ubuntu 18.04 下使用 DNSMasq 作为缓存服务器，要删除 DNS 缓存，请运行以下命令： sudo systemctl restart dnsmasq.service ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:2:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 Nscd 缓存 如果使用了 Nscd，删除 DNS 缓存只需要运行以下命令： sudo systemctl restart nscd.service 或者运行： sudo service nscd restart ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:3:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"域名服务（DNS）是一种Internet服务，可将IP地址和标准域名（FQDN）相互映射。这样，DNS减轻了记住IP地址的需要。运行DNS的计算机称为名称服务器。Ubuntu附带了BIND (Berkley Internet Naming Daemon)，BIND是用于在Linux上维护名称服务器的最常用程序。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:0:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"安装 在终端提示符下，输入以下命令安装 dns: sudo apt install bind9 dnsutils 软件包是测试和解决 DNS 问题非常有用的。 这些工具通常已经安装，但是要检查或安装 dnsutils，请输入以下内容： sudo apt install dnsutils ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:1:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"配置角色 有许多方法可以配置BIND9。一些最常见的配置是缓存名称服务器，主服务器和辅助服务器。 当配置为缓存名称服务器时，BIND9将找到名称查询的答案，并在再次查询域时记住答案。 作为主要服务器，BIND9从其主机上的文件中读取区域的数据，并且对该区域具有权威性。 作为辅助服务器，BIND9从另一个对该区域具有权威性的名称服务器获取区域数据。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:2:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"配置文件概览 DNS配置文件存储在 /etc/bind 目录中。主要配置文件是 /etc/bind/named.conf ，在软件包提供的布局中仅包括这些文件。 /etc/bind/named.conf.options：DNS 全局选项配置文件 /etc/bind/named.conf.local：自定义区域配置文件 /etc/bind/named.conf.default-zones：默认区域，例如localhost，其反向和根提示 根名称服务器曾经在文件中描述过 /etc/bind/db.root 。 现在由软件包 /usr/share/dns/root.hints 附带的文件提供了此功能 dns-root-data，并且在 named.conf.default-zones 上面的配置文件中对此进行了引用。 可以将同一服务器配置为缓存名称服务器，主要和辅助名称服务器：这都取决于它所服务的区域。服务器可以是一个区域的授权开始（SOA），同时为另一区域提供辅助服务。同时为本地LAN上的主机提供缓存服务。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:3:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"缓存名称服务器 默认配置充当缓存服务器。只需取消注释并编辑 /etc/bind/named.conf.options 即可设置ISP的DNS服务器的IP地址： forwarders { 1.2.3.4; 5.6.7.8; }; 注意: 用实际 DNS 服务器的IP地址替换 1.2.3.4 和 5.6.7.8。 要启用新配置，请重新启动DNS服务器。在终端提示下： sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:4:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"主服务器 在本节中，将BIND9配置为域的主服务器 example.com。只需 example.com 用您的FQDN（完全合格的域名）替换即可。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"转发区域文件 要将DNS区域添加到BIND9，将BIND9变成主服务器，请首先编辑 /etc/bind/named.conf.local： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; }; 注意 如果bind将像使用DDNS一样接收文件的自动更新，请在此处以及下面的复制命令中使用 /var/lib/bind/db.example.com 而不是 /etc/bind/db.example.com。 现在，使用现有的区域文件作为模板来创建 /etc/bind/db.example.com 文件： sudo cp /etc/bind/db.local /etc/bind/db.example.com 编辑新的区域文件，/etc/bind/db.example.com 然后更改 localhost.为服务器的FQDN，.在末尾保留其他文件。更改 127.0.0.1 为名称服务器的IP地址和 root.localhost 有效的电子邮件地址，但用.代替通常的@符号，并再次.在末尾保留。更改注释以指示此文件所针对的域。 为基本域创建A记录example.com。此外，创建一个A记录的ns.example.com，在这个例子中，域名服务器： ; ; BIND data file for example.com ; $TTL 604800 @ IN SOA example.com. root.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL @ IN NS ns.example.com. @ IN A 192.168.1.10 @ IN AAAA ::1 ns IN A 192.168.1.10 每次更改区域文件时，都必须增加序列号(Serial)。如果在重新启动BIND9之前进行了多次更改，只需增加一次串行。 现在，您可以将DNS记录添加到区域文件的底部。有关详细信息，请参阅公共记录类型。 注意，许多管理员喜欢使用最后编辑的日期作为区域的序列号(Serial)，例如2020012100，它是yyyymmddss(其中ss是序列号) 对区域文件进行了更改之后，需要重新启动BIND9以使更改生效 sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:1","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"反向区域文件 现在已经设置了区域并将名称解析为IP地址，现在需要添加反向区域以允许DNS将地址解析为名称。 编辑 /etc/bind/named.conf.local 并添加以下内容： zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; }; 注意: 将 1.168.192 替换为所用网络的前三个八位位组。 另外，适当命名区域文件 /etc/bind/db.192。 它应与网络的第一个八位位组匹配。 现在创建 /etc/bind/db.192 文件: sudo cp /etc/bind/db.127 /etc/bind/db.192 接下来编辑 /etc/bind/db.192，更改与/etc/bind/db.example.com相同的选项： ; ; BIND reverse data file for local 192.168.1.XXX net ; $TTL 604800 @ IN SOA ns.example.com. root.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS ns. 10 IN PTR ns.example.com. 每次更改时，“反向”区域中的序列号也需要增加。 对于您在/etc/bind/db.example.com中配置的每个A记录（即针对另一个地址），您需要在/etc/bind/db.192中创建一个PTR记录。 创建反向区域文件后，重新启动BIND9 sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:2","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"辅助服务器 一旦配置了主服务器，强烈建议使用辅助服务器，以在主服务器不可用时维持域的可用性。 首先，在主服务器上，需要允许区域传输。将 allow-transfer 选项添加到示例正向和反向区域定义中 /etc/bind/named.conf.local： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; allow-transfer { 192.168.1.11; }; }; zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; allow-transfer { 192.168.1.11; }; }; 注意 替换192.168.1.11为辅助名称服务器的IP地址。 在主服务器上重新启动BIND9： sudo systemctl restart bind9.service 接下来，在辅助服务器上，以与主服务器相同的方式安装bind9软件包。然后编辑，/etc/bind/named.conf.local 并为正向和反向区域添加以下声明： zone \"example.com\" { type slave; file \"db.example.com\"; masters { 192.168.1.10; }; }; zone \"1.168.192.in-addr.arpa\" { type slave; file \"db.192\"; masters { 192.168.1.10; }; }; 注意 替换192.168.1.10为您的主要名称服务器的IP地址。 在辅助服务器上重新启动BIND9： sudo systemctl restart bind9.service 在其中，/var/log/syslog 您应该看到类似以下内容的内容（为了适应本文档的格式，对某些行进行了拆分）： client 192.168.1.10#39448: received notify for zone '1.168.192.in-addr.arpa' zone 1.168.192.in-addr.arpa/IN: Transfer started. transfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53: connected using 192.168.1.11#37531 zone 1.168.192.in-addr.arpa/IN: transferred serial 5 transfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53: Transfer completed: 1 messages, 6 records, 212 bytes, 0.002 secs (106000 bytes/sec) zone 1.168.192.in-addr.arpa/IN: sending notifies (serial 5) client 192.168.1.10#20329: received notify for zone 'example.com' zone example.com/IN: Transfer started. transfer of 'example.com/IN' from 192.168.1.10#53: connected using 192.168.1.11#38577 zone example.com/IN: transferred serial 5 transfer of 'example.com/IN' from 192.168.1.10#53: Transfer completed: 1 messages, 8 records, 225 bytes, 0.002 secs (112500 bytes/sec) 注意：仅当主服务器上的序列号大于辅助服务器上的序列号时，才会传输区域。如果要让您的主DNS通知其他辅助DNS服务器区域更改，则可以将其添加also-notify { ipaddress; };到/etc/bind/named.conf.local以下示例中： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; allow-transfer { 192.168.1.11; }; also-notify { 192.168.1.11; }; }; zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; allow-transfer { 192.168.1.11; }; also-notify { 192.168.1.11; }; }; 注意 非权威区域文件的默认目录为/var/cache/bind/。该目录还在AppArmor中配置为允许命名守护程序向其写入。有关AppArmor的更多信息，请参见Security-AppArmor。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:6:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"测试 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"resolv.conf 测试BIND9的第一步是将名称服务器的IP地址添加到主机解析器。应该配置主要名称服务器以及另一个主机，以仔细检查。有关将名称服务器地址添加到网络客户端的详细信息，请参阅DNS客户端配置。最后，您的nameserver一行/etc/resolv.conf应指向，127.0.0.53并且您应该search为您的域指定一个参数。像这样： nameserver 127.0.0.53 search example.com 要检查您的本地解析器正在使用哪个DNS服务器，请运行： systemd-resolve --status 注意 如果主要服务器不可用，您还应该将辅助名称服务器的IP地址添加到客户端配置中。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:1","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"dig 如果安装了dnsutils软件包，则可以使用DNS查找实用程序dig测试设置： 安装完BIND9之后，请对环回接口使用dig来确保它正在侦听端口53。从终端提示符下： dig -x 127.0.0.1 您应该在命令输出中看到类似于以下内容的行： ;; Query time: 1 msec ;; SERVER: 192.168.1.10#53(192.168.1.10) 如果您已将BIND9配置为缓存名称服务器，则“挖掘”外部域以检查查询时间： dig ubuntu.com 注意查询时间接近命令输出的末尾： ;; Query time: 49 msec 经过第二次挖掘后，应该有所改进： ;; Query time: 1 msec ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:2","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"ping 现在演示应用程序如何使用DNS解析主机名，使用ping实用程序发送ICMP回显请求： ping example.com 这测试名称服务器是否可以将名称解析为ns.example.com IP 地址。 命令输出应类似于： PING ns.example.com (192.168.1.10) 56(84) bytes of data. 64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.800 ms 64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.813 ms ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:3","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"named-checkzone 测试区域文件的一种好方法是使用 named-checkzone 与bind9软件包一起安装的实用程序。使用此实用程序，可以在重新启动BIND9并使更改生效之前确保配置正确。 要测试我们的示例正向区域文件，请从命令提示符处输入以下内容： named-checkzone example.com /etc/bind/db.example.com 如果一切配置正确，您应该会看到类似以下的输出： zone example.com/IN: loaded serial 6 OK 同样，要测试反向区域文件，请输入以下内容： named-checkzone 1.168.192.in-addr.arpa /etc/bind/db.192 输出应类似于： zone 1.168.192.in-addr.arpa/IN: loaded serial 3 OK ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:4","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"日志 BIND9有多种可用的日志记录配置选项，但是两个主要的选项是channel和category，它们分别配置日志的去向和要记录的信息。 如果未配置任何日志记录选项，则默认配置为： logging { category default { default_syslog; default_debug; }; category unmatched { null; }; }; 让我们将BIND9配置为将与DNS查询相关的调试消息发送到单独的文件。 我们需要配置一个通道以指定要将消息发送到的文件，以及一个category。在此示例中，类别将记录所有查询。编辑/etc/bind/named.conf.local并添加以下内容： logging { channel query.log { file \"/var/log/named/query.log\"; severity debug 3; }; category queries { query.log; }; }; 注意 该调试选项可以从1设置为3。如果没有指定级别，1级是默认的。 由于命名守护程序以绑定用户身份运行，因此/var/log/named必须创建目录并更改所有权： sudo mkdir /var/log/named sudo chown bind:bind /var/log/named 现在重新启动BIND9，以使更改生效： sudo systemctl restart bind9.service 您应该看到文件中/var/log/named/query.log填充了查询信息。这是BIND9日志记录选项的简单示例。 注意 您的区域文件的序列号可能会有所不同。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:8:0","tags":["ubuntu","bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"临时IP地址分配 对于临时网络配置，可以使用在大多数其他GNU / Linux操作系统上也可以找到的ip命令。ip命令允许您配置立即生效的设置，但是这些设置不是永久性的，并且在重新启动后会丢失。 要临时配置IP地址，可以按以下方式使用ip命令。修改IP地址和子网掩码以符合您的网络要求。 sudo ip addr add 10.102.66.200/24 dev enp0s25 然后可以使用ip来设置链接的打开或关闭。 ip link set dev enp0s25 up ip link set dev enp0s25 down 要验证enp0s25的IP地址配置，可以按以下方式使用ip命令。 ip address show dev enp0s25 10: enp0s25: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:16:3e:e2:52:42 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.102.66.200/24 brd 10.102.66.255 scope global dynamic eth0 valid_lft 2857sec preferred_lft 2857sec inet6 fe80::216:3eff:fee2:5242/64 scope link valid_lft forever preferred_lft forever6 要配置默认网关，可以按以下方式使用ip命令。修改默认网关地址以符合您的网络要求。 sudo ip route add default via 10.102.66.1 要验证默认网关配置，可以按以下方式使用ip命令。 ip route show default via 10.102.66.1 dev eth0 proto dhcp src 10.102.66.200 metric 100 10.102.66.0/24 dev eth0 proto kernel scope link src 10.102.66.200 10.102.66.1 dev eth0 proto dhcp scope link src 10.102.66.200 metric 100 如果您需要DNS进行临时网络配置，则可以在文件中添加DNS服务器IP地址 /etc/resolv.conf。通常，/etc/resolv.conf不建议直接进行编辑，但这是一个临时且非持久的配置。下面的示例显示如何在中输入两个DNS服务器/etc/resolv.conf，应将其更改为适合您的网络的服务器。下一节将更详细地说明进行DNS客户端配置的正确的持久方法。 nameserver 8.8.8.8 nameserver 8.8.4.4 如果您不再需要此配置，并且希望从接口清除所有IP配置，则可以将ip命令与flush选项一起使用，如下所示。 ip addr flush eth0 使用ip命令刷新IP配置不会清除的内容 /etc/resolv.conf。您必须手动删除或修改这些条目，或者重新引导，这也将导致重新写入/etc/resolv.conf，这是到的符号链接/run/systemd/resolve/stub-resolv.conf ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:1:0","tags":["ubuntu"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["ubuntu"],"content":"静态IP地址分配 要将系统配置为使用静态地址分配，请在文件中创建一个netplan配置 /etc/netplan/99_config.yaml。下面的示例假定您正在配置标识为eth0的第一个以太网接口。更改地址，gateway4和名称服务器值，以满足您的网络要求。 network:version:2renderer:networkdethernets:eth0:addresses:- 10.10.10.2/24gateway4:10.10.10.1nameservers:search:- mydomain- otherdomainaddresses:- 10.10.10.1- 1.1.1.1 然后可以使用netplan命令应用该配置。 sudo netplan apply ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:2:0","tags":["ubuntu"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["ubuntu"],"content":"名称解析 与IP网络相关的名称解析是将IP地址映射到主机名的过程，从而更容易识别网络上的资源。下一节将说明如何使用DNS和静态主机名记录正确配置系统以进行名称解析。 DNS客户端配置 传统上，该文件/etc/resolv.conf是静态配置文件，很少需要通过DCHP客户端挂接进行更改或自动更改。Systemd 解析处理名称服务器配置，并且应该通过systemd-resolve命令与之交互。Netplan配置systemd-resolved以生成要放入的名称服务器和域的列表/etc/resolv.conf，这是一个符号链接： /etc/resolv.conf -\u003e ../run/systemd/resolve/stub-resolv.conf 要配置解析器，请将适合您的网络的名称服务器的IP地址添加到netplan配置文件中。您还可以添加可选的DNS后缀搜索列表以匹配您的网络域名。生成的文件可能如下所示： network:version:2renderer:networkdethernets:enp0s25:addresses:- 192.168.0.100/24gateway4:192.168.0.1nameservers:search:[mydomain, otherdomain]addresses:[1.1.1.1,8.8.8.8,4.4.4.4] 该搜索选项也可以用多个域名使用，使得DNS查询将按照它们的输入顺序追加。例如，您的网络可能有多个子域可供搜索；的父域example.com和两个子域，sales.example.com以及dev.example.com。 如果您要搜索多个域，则配置可能如下所示： network:version:2renderer:networkdethernets:enp0s25:addresses:- 192.168.0.100/24gateway4:192.168.0.1nameservers:search:[example.com, sales.example.com, dev.example.com]addresses:[1.1.1.1,8.8.8.8,4.4.4.4] 如果您尝试对名称为server1的主机执行ping操作，系统将按以下顺序自动查询DNS的完全合格域名（FQDN）： server1.example.com server1.sales.example.com server1.dev.example.com 如果找不到匹配项，则DNS服务器将提供notfound的结果，并且DNS查询将失败。 ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:3:0","tags":["ubuntu"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["command"],"content":"awk 文本处理 awk 是一种很棒的语言，它适合文本处理和报表生成，其语法较为常见，借鉴了某些语言的一些精华，如 C 语言等。在 linux 系统日常处理工作中，发挥很重要的作用，掌握了 awk 将会使你的工作变的高大上。awk 是三剑客的老大，利剑出鞘，必会不同凡响。 ","date":"2021-03-03","objectID":"/posts/awk/:1:0","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["command"],"content":"awk 内置变量 $0 匹配当前记录整行数据 $1-$n 匹配当前记录的第n个字段（列） FS 输入字段分隔符，默认是空格或制表符(tab) RS 输入记录分隔符，默认为换行符 NF 当前记录的字段个数，就是有多个列 NR 记录所有行数，就是行号，从1开始 OFS 输出字段分隔符，默认也是空格 ORS 输入的记录分隔符，默认为换行符 内置变量很多，更多参数自行查询 ","date":"2021-03-03","objectID":"/posts/awk/:1:1","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["command"],"content":"awk 使用示例 1. 统计 linux 系统下 tcp 协议所有网络状态条数 root@ops:~# ss -ant | awk '{++d[$1]} END {for (k in d){print k \": \" d[k]}}' | grep -v State LISTEN: 10 ESTAB: 1 语法解析 { # 定义一个数组 d, 键为 $1 (状态名), ++ 表示数组中键名相同时值自动加 1 ++d[$1] } END { # awk 在处理了输入文件中的所有行之后执行这个块 # 输入数组中所有数据，k 是键名，通过键名取数据 for (k in d){ print k \": \" d[k] } } 2. 求和并排序 文本有两列字段，用户名，充值金额。 在数据中用户可能会重复出现，现在需要统计出所有用户的金额总值并按降序排列 root@ops:~# cat b.txt 1234 100 1344 13 1242 783 1234 234 4563 21 4562 145 root@ops:~# awk '{d[$1]+=$2} END { for (n in d){print n \": \" d[n]}}' b.txt | sort -nr -k 2 1242: 783 1234: 334 4562: 145 4563: 21 1344: 13 awk 语法解析 { # 定义一个数据，key 为用户名，value 为 充值金额， # 由于用户名会重复出现金额需要累加，所以使用 += 表达式进行赋值 d[$1]+=$2 } END { # 输入数据中所有值 for (n in d){ print n \": \" d[n] } } 3. 列出占用 80 端口进程的 pid 并结束它 root@ops:~# lsof -i:80 | awk '{ if ($2 ~ /[0-9]/) {print $2}}' | xargs kill 30213 30232 30233 也可以使用: ss -antp | grep ':80 ' | egrep -o 'pid=[0-9]{,5}' | tr -d 'pid=' | xargs kill ","date":"2021-03-03","objectID":"/posts/awk/:1:2","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["haproxy"],"content":"HAPrxoy 介绍 HAProxy 是一个使用 C 语言编写的自由及开放源代码软件，其提供高可用性、负载均衡，以及基于 tcp 和 http 的应用程序代理。 mode http：七层反向代理，受端口数量限制 mode tcp：四层反向代理，不受套接字文件数量限制 HAProxy 特别适用于那些负载特大的 web 站点，这些站点通常又需要会话保持或七层处理。HAProxy 运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中，同时可以保护你的 web 服务器不被暴露到网络上。 更多内容请查看 官方网站 – 官方文档 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:1:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"安装 HAProxy 以 Ubuntu Server 18.04 操作系统为例, 官方教程 您需要使用以下命令启用专用的PPA： root@lb-01:~# apt-get install --no-install-recommends software-properties-common root@lb-01:~# add-apt-repository ppa:vbernat/haproxy-2.2 然后，使用以下命令： root@lb-01:~# apt-get install haproxy=2.2.\\* 您将获得最新版本的 HAProxy 2.2（并坚持使用此分支）。 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:2:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"HAProxy 服务配置 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"程序环境 主程序：/usr/sbin/haproxy 主配置文件：/etc/haproxy/haproxy.cfg systemd 服务配置文件：/lib/systemd/system/haproxy.service ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:1","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"配置文件说明 root@lb-01:/etc/haproxy# cat haproxy.cfg # 全局配置段 global log /dev/log local0 log /dev/log local1 notice # 指定最大连接数 maxconn 200000 # 限制 haproxy 根路径 chroot /var/lib/haproxy # 指定 haproxy 管理 socket 文件并与指定的进程绑定(process) stats socket /run/haproxy/admin1.sock mode 660 level admin expose-fd listeners process 1 stats socket /run/haproxy/admin2.sock mode 660 level admin expose-fd listeners process 2 stats timeout 30s # 指定 haproxy 运行的用户和组 user haproxy group haproxy # 开启守护进程 daemon # 避免对于后端检测同时并发造成的问题，设置错开时间比，范围0到50，一般设置2-5较好 spread-checks 5 # 开启多进程 nbproc 2 # 设定进程与 CPU 绑定 cpu-map 1 0 cpu-map 2 1 # 默认参数配置段， 为 frontend, listen, backend 提供默认配置 defaults log global # 使用 http 模式 mode http # 日志类别 http 日志格式 option httplog # 对应的服务器挂掉后,强制定向到其他健康的服务器 option redispatch # 不记录健康检查的日志信息 option dontlognull # 开启 ip 透传，向后端服务发送真实客户端地址 option forwardfor # 开启 http 长连接 option http-keep-alive # 长连接超时时间设置 timeout http-keep-alive 120s # 连接超时时间设置 timeout connect 60s # 与后端服务器连接超时时间设置 timeout server 600s # 与客户端连接超时时间设置 timeout client 600s # check 检测超时时间设置 timeout check 5s # 配置默认 http 错误响应码对应的页面文件 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # 开启状态页 listen stauts mode http bind :9999 stats enable # 指定状态面的 uri 路径 stats uri /haproxy-status # 配置状态页的用户名和密码 stats auth haproxy:liwg # 配置 http 代理 listen http # 代理监控端口 bind *:80 # 使用的负载均衡调度算法 balance roundrobin # 在后端启用基于 cookie 的持久性 cookie SRVID insert indirect nocache # 后端服务器配置列表, cookie: 指定 cookie值， check: 启用健康检测 # inter 检测间隔， fall 检测失败次数， rise 重试次数 server server1 192.168.31.31:80 cookie 31 check inter 3s fall 3 rise 5 server server2 192.168.31.32:80 cookie 32 check inter 3s fall 3 rise 5 当使用 haproxy 负载均衡集群时，监听的地址为 vip 时，服务会无法启动， 这是因为 Linux 绑定了一个网卡上没有配置的 ip 地址导致的， 此时可以通过修改 Linux 内核 net.ipv4.ip_nonlocal_bind = 1 参数解决 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:2","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"启动 HAProxy 服务并测试 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"启动 HAProxy 服务 root@lb-01:/etc/haproxy# systemctl start haproxy.service root@lb-01:/etc/haproxy# ss -anptl | grep haproxy LISTEN 0 128 0.0.0.0:9999 0.0.0.0:* users:((\"haproxy\",pid=26699,fd=12),(\"haproxy\",pid=26697,fd=12)) LISTEN 0 128 0.0.0.0:80 0.0.0.0:* users:((\"haproxy\",pid=26699,fd=14),(\"haproxy\",pid=26697,fd=14)) root@lb-01:/etc/haproxy# ps -ef | grep haproxy root 26690 1 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock haproxy 26697 26690 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock haproxy 26699 26690 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock 端口 9999 是 haproxy 状态页，端口 80 用于后端服务器负载均衡， 通过 ps 命令， 可以看到 haproxy 开启了多进程 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:1","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"动态下线/上线后端服务器 动态下线后端服务器 root@lb-01:/etc/haproxy# echo 'disable server http/server2' | socat stdio /run/haproxy/admin1.sock root@lb-01:/etc/haproxy# echo 'disable server http/server2' | socat stdio /run/haproxy/admin2.sock 注意： 当输入软下线的命令时 haproxy 依旧可以将用户的请求调度到后端已经下线的服务器上，这是因为 haproxy 的 socket 文件的关系，一个 socket 文件对应一个进程，当 haproxy 处于多进程的模式下时，就需要有多个 socket 文件，并将其和进程进行绑定，对后端服务器进行软下线时需要对所有的 socket 文件下达软下线的指令。 可以通过 http://haproxy_ipaddress:9999/haproxy-status 查看后端服务器状态 动态上线后端服务器 root@lb-01:/etc/haproxy# echo 'enable server http/server2' | socat stdio /run/haproxy/admin1.sock root@lb-01:/etc/haproxy# echo 'enable server http/server2' | socat stdio /run/haproxy/admin2.sock ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:2","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["tomcat"],"content":"配置优化 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:0","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"修改 Server 节点 shutdown 属性值为长随机数 \u003cServer port=\"8005\" shutdown=\"d41d8cd98f00b204e9800998ecf8427e\"\u003e ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:1","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"启用 tomcat 线程池 使用线程池，用较少的线程处理较多的访问，可以提高tomcat处理请求的能力。使用方式： 打开 conf/server.xml，增加 \u003cExecutorname=\"tomcatThreadPool\" namePrefix=\"catalina-exec-\" maxThreads=\"500\" minSpareThreads=\"20\" maxIdleTime=\"60000\" prestartminSpareThreads=\"true\" maxQueueSize=\"100\"/\u003e 属性说明 name: 线程名称 namePrefix: 线程前缀 maxThreads : 最大并发连接数，不配置时默认200，一般建议设置500~ 800 ，要根据自己的硬件设施条件和实际业务需求而定。 minSpareThreads：Tomcat 启动初始化的线程数，默认值25 prestartminSpareThreads：在 Tomcat 初始化的时候就初始化 minSpareThreads 的值 maxQueueSize: 最大的等待队列数，超过则拒绝请求 maxIdleTime：线程最大空闲时间60秒 然后，修改 Connector 节点，增加 executor 属性，如: \u003cConnector port=\"8080\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" executor=\"tomcatThreadPool\" connectionTimeout=\"20000\" enableLookups=\"false\" redirectPort=\"8443\" maxPostSize=\"20971520\" acceptCount=\"2000\" acceptorThreadCount=\"2\" disableUploadTimeout=\"true\" URIEncoding=\"utf-8\"/\u003e 属性说明 port ：连接端口。 protocol：连接器使用的传输方式。 executor： 连接器使用的线程池名称 enableLookups：禁用DNS 查询 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理，默认设置 100 。 maxPostSize：限制以 FORM URL 参数方式的POST请求的内容大小，单位字节，默认是 2097152(2M)，10485760 为 10M。如果要禁用限制，则可以设置为 -1。 acceptorThreadCount： 用于接收连接的线程的数量，默认值是1。一般这个指需要改动的时候是因为该服务器是一个多核CPU，如果是多核 CPU 一般配置为 2。 disableUploadTimeout：上传时是否使用超时机制，以是 servlet 有较长时间来完成它的执行，默认值为 false； URIEncoding: 指定 Url 字符编码，防止出现乱码 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:2","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"运行权限优化 默认情况下 Tomcat 服务是以 root 用户运行的，为了减少安全隐患需要更改为普通用户运行 Tomcat 服务 [root@10-7-171-239 apache-tomcat-9.0.43]# cd bin/ [root@10-7-171-239 bin]# tar xzf commons-daemon-native.tar.gz [root@10-7-171-239 bin]# cd commons-daemon-1.2.4-native-src/unix/ [root@10-7-171-239 unix]# ./configure [root@10-7-171-239 unix]# make [root@10-7-171-239 unix]# mv jsvc /usr/local/apache-tomcat-9.0.43/bin [root@10-7-171-239 unix]# cd /usr/local/apache-tomcat-9.0.43/bin [root@10-7-171-239 bin]# cat \u003e setenv.sh \u003c\u003cEOF #!/bin/bash JAVA_HOME=/usr/local/jdk TOMCAT_USER=tomcat JSVC_OPTS='-jvm server' JAVA_OPTS=\"-server -Xms3072m -Xmx3072m -Djava.security.egd=file:/dev/./urandom\" # 开启监控配置 #CATALINA_OPTS=\"${CATALINA_OPTS} -Dcom.sun.management.jmxremote.port=\u003c监听端口\u003e -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=\u003c本机ip地址\u003e -Dcom.sun.management.jmxremote\" EOF [root@10-7-171-239 bin]# useradd -r tomcat [root@10-7-171-239 apache-tomcat-9.0.43]# chown -R tomcat.tomcat /usr/local/apache-tomcat-9.0.43/ 注意 setenv.sh 配置文件中的 jvm 内存大小，请根据实际情况进行配置。 此时就可以使用 daemon.sh 脚本对 Tomcat 服务进行启停操作了 [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/daemon.sh start [root@10-7-171-239 apache-tomcat-9.0.43]# ps -ef | grep tomcat root 31610 1 0 11:54 ? 00:00:00 jsvc.exec -jvm server -java-home /usr/local/jdk -user tomcat -pidfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.pid -wait 10 -umask 0027 -outfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.out -errfile \u00261 -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/commons-daemon.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -server -Xms512m -Xmx512m -Djava.security.egd=file:/dev/./urandom -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Dignore.endorsed.dirs= -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap tomcat 31611 31610 30 11:54 ? 00:00:03 jsvc.exec -jvm server -java-home /usr/local/jdk -user tomcat -pidfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.pid -wait 10 -umask 0027 -outfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.out -errfile \u00261 -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/commons-daemon.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -server -Xms512m -Xmx512m -Djava.security.egd=file:/dev/./urandom -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Dignore.endorsed.dirs= -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap root 31645 29718 0 11:54 pts/0 00:00:00 grep --color=auto tomcat 从上面的进程信息可以看到 Tomcat 服务的进程，现在已经是使用的 tomcat 用户运行了。 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:2:0","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"Tomcat 介绍 Tomcat 是 Apache 软件基金会（Apache Software Foundation）项目中的一个核心项目，由 Apache、Sun 和其他一些公司及个人共同开发而成。 Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试 JSP 程序的首选。 实际生产环境中建议和 nginx 配合一起使用，nginx 处理静态，tomcat 处理动态程序 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:1:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"开始安装 安装 tomcat 前需先安装 JDK 工具包。 JDK 是 java 语言的软件开发工具包，它包含了 java 的运行环境（jvm + java 系统类库）和 java 工具。 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"安装 JDK JDK 下载地址: https://www.oracle.com/java/technologies/javase-downloads.html 这里我们选择安装 JDK 8 [root@10-7-171-239 src]# wget https://download.oracle.com/otn/java/jdk/8u281-b09/89d678f2be164786b292527658ca1605/jdk-8u281-linux-x64.tar.gz?AuthParam=1614219040_36465185941d2c06fb1457b5fc724aee -O jdk-8u281-linux-x64.tar.gz [root@10-7-171-239 src]# tar xzf jdk-8u281-linux-x64.tar.gz -C /usr/local [root@10-7-171-239 src]# cd /usr/local/ [root@10-7-171-239 local]# ln -s jdk1.8.0_281/ jdk 配置 JDK 环境变量 [root@10-7-171-239 local]# cat \u003e /etc/profile.d/jdk.sh \u003c\u003c EOF \u003e export JAVA_HOME=/usr/local/jdk \u003e export PATH=\\$JAVA_HOME/bin:\\$PATH \u003e EOF [root@10-7-171-239 local]# source /etc/profile [root@10-7-171-239 local]# java -version java version \"1.8.0_281\" Java(TM) SE Runtime Environment (build 1.8.0_281-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode) ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"安装 Tomcat Tomcat 下载地址: https://tomcat.apache.org/download-90.cgi [root@10-7-171-239 src]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gz [root@10-7-171-239 src]# tar xzf apache-tomcat-9.0.43.tar.gz -C /usr/local/ Tomcat 的安装很简单，只要下载解压即可开始使用 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:2","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat 服务启停介绍 默认 Tomcat 启动可以直接执行 bin 目录的 startup.sh 脚本，停止使用 shutdown.sh 脚本，如果你查看这两个脚本文件中的内容会发现它们都是通过调用 catalina.sh 脚本并传递相应的参数进行启动的。 所以我们可以直接使用 .catalina.sh 脚本进行 Tomcat 服务的启动与停止. catalina.sh 脚本帮助信息 [root@10-7-171-239 bin]# ./catalina.sh Using CATALINA_BASE: /usr/local/apache-tomcat-9.0.43 Using CATALINA_HOME: /usr/local/apache-tomcat-9.0.43 Using CATALINA_TMPDIR: /usr/local/apache-tomcat-9.0.43/temp Using JRE_HOME: /usr/local/jdk Using CLASSPATH: /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar Using CATALINA_OPTS: Usage: catalina.sh ( commands ... ) commands: debug Start Catalina in a debugger debug -security Debug Catalina with a security manager jpda start Start Catalina under JPDA debugger run Start Catalina in the current window run -security Start in the current window with security manager start Start Catalina in a separate window start -security Start in a separate window with security manager stop Stop Catalina, waiting up to 5 seconds for the process to end stop n Stop Catalina, waiting up to n seconds for the process to end stop -force Stop Catalina, wait up to 5 seconds and then use kill -KILL if still running stop n -force Stop Catalina, wait up to n seconds and then use kill -KILL if still running configtest Run a basic syntax check on server.xml - check exit code for result version What version of tomcat are you running? Note: Waiting for the process to end and use of the -force option require that $CATALINA_PID is defined ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:3","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"启动 Tomcat 服务 [root@10-7-171-239 apache-tomcat-9.0.43]# /usr/local/apache-tomcat-9.0.43/bin/startup.sh Using CATALINA_BASE: /usr/local/apache-tomcat-9.0.43 Using CATALINA_HOME: /usr/local/apache-tomcat-9.0.43 Using CATALINA_TMPDIR: /usr/local/apache-tomcat-9.0.43/temp Using JRE_HOME: /usr/local/jdk Using CLASSPATH: /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar Using CATALINA_OPTS: Tomcat started. [root@10-7-171-239 apache-tomcat-9.0.43]# ss -anptl | grep java LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:((\"java\",pid=30004,fd=68)) LISTEN 0 100 :::8080 :::* users:((\"java\",pid=30004,fd=57)) Tomcat 默认监听于 8080 端口，直接在浏览器上访问 http://\u003cyour_server_ipaddress\u003e:8080 [root@10-7-171-239 apache-tomcat-9.0.43]# ps -ef | grep tomcat root 30004 1 8 10:23 pts/0 00:00:02 /usr/local/jdk/bin/java -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap start root 30038 29718 0 10:24 pts/0 00:00:00 grep --color=auto tomcat 通过查看 tomcat 进程可以看到 tomcat 默认使用 root 用户启动，这会存在安全风险，那该如何使用普通用户启动呢？ ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:4","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat 目录结构 [root@10-7-171-239 apache-tomcat-9.0.43]# tree -L 1 -d . ├── bin # 管理脚本存放路径 ├── conf # 配置文件目录 ├── lib # 公共程序库目录 ├── logs # 日志目录 ├── temp ├── webapps # 默认 web 应用程序目录 └── work ","date":"2021-02-22","objectID":"/posts/tomcat-install/:3:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"webapps 目录 webapps 目录存放都是 web 应用，每个目录都是单独的应用。其中 ROOT 比较特殊，ROOT 目录中的应用是打开网页可以直接访问到的，例如 http://localhost:8080 访问的是 ROOT 目录中的应用，如果需要访问 docs 应用需要在 url 上加上 http://localhost:8080/docs/ 路径。 [root@10-7-171-239 apache-tomcat-9.0.43]# tree webapps -L 1 -d webapps ├── docs ├── examples ├── host-manager ├── manager └── ROOT 默认 webapps 中有 Tomcat 自带管理应用，不用可以移除。 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:3:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"配置 Tomcat ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"配置介绍 Tomcat 的配置文件存放在 conf 目录中，其中 server.xml 为主配置文件。默认配置信息如下 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!-- port: tomcat 服务管理端口， shutdown: 服务停止字符，如果从服务管理端口接收到此字符 tomcat 服务将会停止， 有安全风险，建议更改为更复杂的字符串。 可以使用 cat /dev/urandom | head -n 1 | md5sum 生成 --\u003e \u003cServer port=\"8005\" shutdown=\"SHUTDOWN\"\u003e \u003cListener className=\"org.apache.catalina.startup.VersionLoggerListener\" /\u003e \u003cListener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /\u003e \u003cListener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /\u003e \u003cListener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /\u003e \u003cListener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /\u003e \u003cGlobalNamingResources\u003e \u003cResource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /\u003e \u003c/GlobalNamingResources\u003e \u003cService name=\"Catalina\"\u003e \u003c!-- HTTP 服务监听的端口 --\u003e \u003cConnector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /\u003e \u003c!-- defaultHost: 定义缺省处理请求的虚拟主机域名 --\u003e \u003cEngine name=\"Catalina\" defaultHost=\"localhost\"\u003e \u003cRealm className=\"org.apache.catalina.realm.LockOutRealm\"\u003e \u003cRealm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/\u003e \u003c/Realm\u003e \u003c!-- 虚拟主机定义, name: 域名， appBase: WEB 应用程序路径 --\u003e \u003cHost name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e \u003c/Host\u003e \u003c/Engine\u003e \u003c/Service\u003e \u003c/Server\u003e ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat url 路径配置 如果想让 Tomcat 的根指向为 webapps 中的 test 应用该如何配置？？ 准备测试数据 [root@10-7-171-239 webapps]# mkdir test [root@10-7-171-239 webapps]# echo 'hello test file' \u003e test/index.html 修改 server.xml 配置文件, 在 Host 节点加入 Context 配置项，具体内容如下 \u003cContext path=\"/\" docBase=\"test/\" reloadable=\"false\" debug=\"0\" /\u003e path: 指定 url 路径，如果是 / 可以忽略不写 docBase: 用于指定 WEB 应用路径，可以是相当路径（相对于 Host 的 appBase 属性的值），也可以是绝对路径 reloadable: 是否自动重载，建议\u0008值设置为 false，此属性会影响服务性能 重启服务，测试 [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/shutdown.sh [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/startup.sh [root@10-7-171-239 apache-tomcat-9.0.43]# curl localhost:8080 hello test file ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:2","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["keepalived","lvs"],"content":"环境准备 两台调度服务器: 安装 keepalived + ipvsadm 两台真实服务器: 安装 nginx 提供 Web 服务 web-01: 192.168.31.31/24 web-02: 192.168.31.32/24 lb-01: 192.168.31.33/24 lb-02: 192.168.31.34/24 vip: 192.168.31.30 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:1:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 Real Server 服务器 安装 nginx 来提供一个简单 Web 页面用来测试 web-01 root@web-01:~# apt install nginx root@web-01:~# echo '\u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-01:~# systemctl start nginx root@web-01:~# curl localhost \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e web-02 root@web-02:~# apt install nginx root@web-02:~# echo '\u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-02:~# systemctl start nginx root@web-02:~# curl localhost \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:2:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 lvs 调用可用集群 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"安装 keepalived 与 ipvs 管理工具 root@lb-01:~# apt install keepalived ipvsadm ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:1","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 keepalived 配置主节点 /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface ens32 virtual_router_id 50 priority 100 advert_int 1 nopreempt authentication { auth_type PASS auth_pass 11111 } virtual_ipaddress { 192.168.31.30 } } virtual_server 192.168.31.31 80 { delay_loop 6 ! 使用 rr 调度算法 lb_algo rr ! 使用 DR 直接路由工作方式 lb_kind DR ! 会话保持时间，单位是秒。这个选项对动态网页是非常有用的，为集群系统中的 SEEION 共享 ! 提供了一个很好的解决方案。有了这个会话保持功能，用户的请求会一直分发到同一个服务节点， ! 直到超过这个会话的保持时间。 注意: 这个会话保持时间是最大无响应超时时间。如果用户一直 ! 在操作动态页面，是不受这个时间限制的。 persistence_timeout 50 ! 使用转发的协议类型， 也可以是 UDP protocol TCP ! 真实提供服务器的机器 real_server 192.168.31.31 80 { ! 定义权重 weight 3 ! real_server 状态检测，单位为秒 TCP_CHECK { ! 表示3秒无响应超时 connect_timeout 3 ! 表示重试次数 nb_get_retry 3 ! 重试间隔 delay_before_retry 3 } } real_server 192.168.31.32 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } } 配置备用节点 /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state BACKUP interface ens32 virtual_router_id 50 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 11111 } virtual_ipaddress { 192.168.31.30 } } virtual_server 192.168.31.31 80 { delay_loop 6 lb_algo rr lb_kind DR persistence_timeout 50 protocol TCP real_server 192.168.31.31 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } real_server 192.168.31.32 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } } ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:2","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"启动 keepalivd 服务 启动 keepalived 服务并查看 ipvs 规则， vip 配置情况 root@lb-01:/etc/keepalived# systemctl start keepalived.service root@lb-01:/etc/keepalived# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.31.30:80 rr persistent 50 -\u003e 192.168.31.31:80 Route 3 0 0 -\u003e 192.168.31.32:80 Route 3 0 0 root@lb-01:/etc/keepalived# ip addr show ens32 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 备用节点的 keepalived 服务也启动起来 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:3","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 RealServer 由于 lvs 的 DR 方式，需要在两台 Real Server 上也配置 vip 地址，并且还需要抑制 arp 广播。 使用以下脚本完成。 #!/bin/bash # # filename: lvs-vip.sh # VIP=192.168.31.30 INTERFACE=lo case $1 in start) echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The RS Server is Ready!\" ;; stop) ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce echo \"The RS Server is Canceled!\" ;; *) echo $\"Usage: $0{start|stop|restart}\" exit 1 ;; esac 启用 vip root@web-01:~# mv lvs-vip.sh /usr/local/bin/ root@web-01:~# chmod +x /usr/local/bin/lvs-vip.sh root@web-01:~# lvs-vip.sh start The RS Server is Ready! ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:4","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"测试 lvs 高可用集群 测试过程略，请自行测试… 可以将主 lvs 服务器宕机，然后查看服务是否会中断， 备用 lvs 服务器是否能接手，并正确配置 vip 提供调度服务。 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:4:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived"],"content":"Keepalived 介绍 Keepalived 是一个基于 VRRP 协议来实现的 WEB 服务高可用方案，可以利用其来避免单点故障。使用多台节点安装keepalived。其他的节点用来提供真实的服务（RealServer），同样的，他们对外表现一个虚拟的IP（vip）。主服务器宕机的时候，备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。 ","date":"2021-02-21","objectID":"/posts/keepalived/:1:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"Keepalived 简单应用 拓扑图 ","date":"2021-02-21","objectID":"/posts/keepalived/:2:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"环境准备 本次实验需要准备2台后端 Web 服务器，用于提供真实服务，最前端挂2台 nginx 反向代理并安装 keepalived 实现高可用。 总共4台机器。 实现的目的 当用户请求 web 服务器时，请求先到 keeplived 主服务器，然后 keepalived 主服务器根据 nginx 的负载均衡配置规则 选择 1台后端的 web 服务器将用户的请求转发给它，keepalived 主服务器拿到相应的数据后在响应给用户。如果keepalived 主服务器的 nginx 服务挂了，将由备份 keepalived 服务器接手为用户提供服务。 nginx：反向代理、负载均衡配置参考 nginx 文档 ","date":"2021-02-21","objectID":"/posts/keepalived/:2:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装配置 keepalived ","date":"2021-02-21","objectID":"/posts/keepalived/:3:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装 keepalived root@lb-01:~# apt install keepalived # 复制一份示例配置文件 root@lb-01:~# cp /usr/share/doc/keepalived/samples/keepalived.conf.sample /etc/keepalived/keepalived.conf ","date":"2021-02-21","objectID":"/posts/keepalived/:3:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"keepalived 高可用配置 keepalived 的配置文件默认位于：/etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { ! 运行Keepalived服务器的一个标识，发邮件时显示在邮件主题中。 router_id LVS_DEVEL } ! 定义 nginx 服务检测脚本 vrrp_script check_nginx { script \"/bin/bash /etc/keepalived/chk_ngx.sh\" ! 检查时间间隔，2秒 interval 2 } ! 定义高可用实例 vrrp_instance VI_1 { ! 指明当前服务的角色，备用为 BACKUP state MASTER ! 指定监测的网络接口 interface ens32 ! 虚拟路由标识，同一个实例使用唯一的标识，master与backup必须是一致的 virtual_router_id 50 ! 定义节点的优先级，数字越大，优先级越高。 priority 100 ! 主从主机之间同步检查的时间间隔 advert_int 1 ! 不抢占，恢复后不进行切换，主挂了再起来不会抢回身份。防止 vip 飘来飘去影响稳定性 nopreempt ! 用于设置节点间通信验证类型和密码 authentication { auth_type PASS auth_pass 11111 } ! 设置虚拟 ip 地址 virtual_ipaddress { 192.168.31.30 } ! 检查条件 track_script { check_nginx } } /etc/keepalived/chk_ngx.sh #!/bin/bash /bin/pidof nginx \u0026\u003e /dev/null ## 也可使用 killall -0 nginx 探测服务状态 if [[ $? -ne 0 ]]; then echo 'nginx is stop' exit 1 fi 将 keepalived.conf 和 chk_ngx.sh 复制到备份主机上，修改 keepalived.conf 文件 state BACKUP, priority 90, 删除 nopreempt 即可。 ","date":"2021-02-21","objectID":"/posts/keepalived/:3:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装反代和Web服务 ","date":"2021-02-21","objectID":"/posts/keepalived/:4:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装 Web 服务 这里使用 nginx 来提供一个简单 Web 页面用来测试 web-01 root@web-01:~# apt install nginx root@web-01:~# echo '\u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-01:~# systemctl start nginx root@web-01:~# curl localhost \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e web-02 root@web-02:~# apt install nginx root@web-02:~# echo '\u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-02:~# systemctl start nginx root@web-02:~# curl localhost \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e ","date":"2021-02-21","objectID":"/posts/keepalived/:4:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装配置 nginx 反向代理 在两台 keepalived 机器上安装 nginx 并配置反向代理到后端两台 Web 服务器, 两台机的配置是一样的。 root@lb-01:~# apt install nginx root@lb-01:~# cat /etc/nginx/sites-available/default upstream web { server 192.168.31.31; server 192.168.31.32; } server { listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / { proxy_pass http://web; } } root@lb-01:~# systemctl restart nginx ","date":"2021-02-21","objectID":"/posts/keepalived/:4:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"测试 keepalived 高可用 ","date":"2021-02-21","objectID":"/posts/keepalived/:5:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"启用 keepalived 服务 root@lb-01:~# systemctl start keepalived.service root@lb-01:~# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 日志信息 Feb 27 08:10:46 lb-01 systemd[1]: Starting Keepalive Daemon (LVS and VRRP)... Feb 27 08:10:46 lb-01 Keepalived[8259]: Starting Keepalived v1.3.9 (10/21,2017) Feb 27 08:10:46 lb-01 Keepalived[8259]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 systemd[1]: Started Keepalive Daemon (LVS and VRRP). Feb 27 08:10:46 lb-01 Keepalived[8275]: Starting Healthcheck child process, pid=8276 Feb 27 08:10:46 lb-01 Keepalived[8275]: Starting VRRP child process, pid=8277 Feb 27 08:10:46 lb-01 Keepalived_healthcheckers[8276]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering Kernel netlink reflector Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering Kernel netlink command channel Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering gratuitous ARP shared channel Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: WARNING - default user 'keepalived_script' for script execution does not exist - please create. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: (VI_1): Warning - nopreempt will not work with initial state MASTER Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Using LinkWatch kernel netlink reflector... Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: VRRP_Script(check_nginx) succeeded Feb 27 08:10:47 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Transition to MASTER STATE Feb 27 08:10:48 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Entering MASTER STATE 从上面的信息可以看到， keepalived 自动为 ens32 网卡添加一个 vip，服务角色为 master 开启备份节点服务 root@lb-02:/etc/keepalived# systemctl start keepalived.service ","date":"2021-02-21","objectID":"/posts/keepalived/:5:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"测试 测试使用 vip 访问 Web 服务 root@web-01:~# while :; do curl 192.168.31.30; sleep .5; done \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e 停止 keepalived 主服务的 nginx 服务，查看 vip 是否转移。Web 服务是否可用。 root@lb-01:~# systemctl stop nginx 主节点日志信息 Feb 27 08:17:15 lb-01 systemd[1]: Stopping A high performance web server and a reverse proxy server... Feb 27 08:17:15 lb-01 systemd[1]: Stopped A high performance web server and a reverse proxy server. Feb 27 08:17:17 lb-01 Keepalived_vrrp[8277]: /bin/bash /etc/keepalived/chk_ngx.sh exited with status 1 Feb 27 08:17:17 lb-01 Keepalived_vrrp[8277]: VRRP_Script(check_nginx) failed Feb 27 08:17:18 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Entering FAULT STATE Feb 27 08:17:18 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Now in FAULT state Feb 27 08:17:19 lb-01 Keepalived_vrrp[8277]: /bin/bash /etc/keepalived/chk_ngx.sh exited with status 1 日志信息表明，检测到 nginx 服务停止，检测失败，keepalived 进入 FAULT 状态。 查看备用节点的日志和网卡信息 root@lb-02:/etc/keepalived# systemctl start keepalived.service root@lb-02:/etc/keepalived# tail /var/log/syslog Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Registering gratuitous ARP shared channel Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: WARNING - default user 'keepalived_script' for script execution does not exist - please create. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Using LinkWatch kernel netlink reflector... Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Entering BACKUP STATE Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: VRRP_Script(check_nginx) succeeded Feb 27 08:17:01 ubuntu CRON[10641]: (root) CMD ( cd / \u0026\u0026 run-parts --report /etc/cron.hourly) Feb 27 08:17:18 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Transition to MASTER STATE Feb 27 08:17:19 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Entering MASTER STATE root@lb-02:/etc/keepalived# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:0c:ee:46 brd ff:ff:ff:ff:ff:ff inet 192.168.31.34/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe0c:ee46/64 scope link valid_lft forever preferred_lft forever 从日志和网卡信息可以看出，备用节点接替了主节点的 vip ，并将自己提升为主节点 客户端请求测试信息 \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e 可以看到在 vip 切换时，有一些无法响应的请求，之后又立即恢复了 ","date":"2021-02-21","objectID":"/posts/keepalived/:5:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["lvs"],"content":"环境准备 Director Server: 192.168.31.33/24 Real Server 1: 192.168.31.31/24 Real Server 2: 192.168.31.32/24 VIP: 192.168.31.30/32 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:1:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"安装配置 ipvs ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"安装 ipvs 管理软件 在 Director Server 上安装 ipvsadm root@lb-01:~# apt install ipvsadm ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"配置 ipvs 配置 vip root@lb-01:~# ip addr add 192.168.31.30/32 brd 192.168.31.30 dev ens32 root@lb-01:~# ip addr show ens32 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 brd 192.168.31.30 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 配置 ipvs 规则 # 清空 ipvsadm 配置表 root@lb-01:~# ipvsadm -C # 设置连接超时值 root@lb-01:~# ipvsadm --set 30 5 60 # 添加虚拟服务器 root@lb-01:~# ipvsadm -A -t 192.168.31.30:80 -s rr -p 20 # 向虚拟服务器添加 RealServer root@lb-01:~# ipvsadm -a -t 192.168.31.30:80 -r 192.168.31.31:80 -g -w 1 root@lb-01:~# ipvsadm -a -t 192.168.31.30:80 -r 192.168.31.32:80 -g -w 1 # 查看 lvs 状态 root@lb-01:~# ipvsadm -L -n --stats IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes -\u003e RemoteAddress:Port TCP 192.168.31.30:80 0 0 0 0 0 -\u003e 192.168.31.31:80 0 0 0 0 0 -\u003e 192.168.31.32:80 0 0 0 0 0 参数说明 -C: 清空 ipvs 所有规则 -A: 添加虚拟服务器 -t: 使用 tcp 协议, 后接虚拟服务器的 ip 地址和端口 -s: 使用的调度算法 -p: 定义持久时间，能够实现将来自同一个地址的请求始终发往同一个 RealServer -a: 添加 RealServer -r: 指定 RealServer 地址和端口 -g: 使用的 DR 实现方式 -w: RealServer 服务器的权重 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"RealServer 服务器配置 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"向回环接口（lo）添加 vip root@web-01:~# ip addr add 192.168.31.30/32 brd 192.168.31.30 dev lo ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"抑制 arp 广播 禁止发送 arp 广播，由于 vip 在多台机器上配置了，如果不抑制广播会造成 ip 地址冲突。 root@web-01:~# echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore root@web-01:~# echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore root@web-01:~# echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce root@web-01:~# echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"在客户机测试 LVS 负载群集 curl 192.168.31.30 注意: 至此 lvs 负载均衡集群配置完成，由于 lvs 服务器本身不支持高可用，存在单点故障， 可以配合 keepalived 一起使用 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:4:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"服务脚本 lvs 的配置是终端上配置的，机器重启后会丢失，那么该如何管理配置呢？ 有两种方法: ipvsadm 包自带配置信息管理工具，ipvsadm-save, ipvsadm-restore 开发脚本管理 ipvsadm-save: 用于导出 lvs 配置 ipvsadm-restore: 用于从文件中恢复 lvs 配置 也可以使用 ipvsadm 包中的脚本管理 /etc/init.d/ipvsadm ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"lvs 配置管理 root@lb-01:~# ipvsadm-save \u003e /etc/ipvsadm.rules # 等价于 /etc/init.d/ipvsadm save root@lb-01:~# ipvsadm-restore \u003c /etc/ipvsadm.rules # 等价于 /etc/init.d/ipvsadm load root@lb-01:~# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.31.30:80 rr persistent 20 -\u003e 192.168.31.31:80 Route 1 0 0 -\u003e 192.168.31.32:80 Route 1 0 0 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"lvs 服务器配置脚本 #!/bin/bash INTERFACE=ens32 PORT=80 VIP=192.168.31.30 RIP=( 192.168.31.31 192.168.31.32 ) IPVSADM=/usr/sbin/ipvsadm start() { ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE $IPVSADM -C $IPVSADM --set 30 5 60 $IPVSADM -A -t ${VIP}:${PORT} -s rr -p 20 for (( i = 0; i \u003c echo ${#RIP[*]}; i++ )); do $IPVSADM -a -t ${VIP}:${PORT} -r ${RIP[$i]}:${PORT} -g -w 1 done echo \"The lvs Server is start!\" } stop() { $IPVSADM -C ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The lvs Server is stop!\" } status() { $IPVSADM -L -n --stats } case $1 in start) start ;; stop) stop ;; restart) stop sleep 1 start ;; status) status ;; *) echo $\"Usage: $0{start|stop|status|restart}\" exit 2 ;; esac ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"RealServer 服务器配置脚本 #!/bin/bash VIP=192.168.31.30 INTERFACE=lo case $1 in start) echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The RS Server is Ready!\" ;; stop) ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce echo \"The RS Server is Canceled!\" ;; *) echo $\"Usage: $0{start|stop|restart}\" exit 1 ;; esac ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:3","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"LVS 介绍 Linux 虚拟服务器（Linux Virtual Server, LVS），是一个由章文嵩开发的一款自由软件。利用 LVS 可以实现高可用、可伸缩的 Web、Mail、Cache 等网络服务。 LVS 具有很好的可伸缩性、可靠性和可管性，通过 LVS 要实现的最终目标是：利用 Linux 操作系统和 LVS 集群软件实现一个高可用、高性能、低成本的服务器应用集群。 常用的实现负载均衡集群的开源软件有: LVS、haproxy、Nginx 等 LVS 工作于 OSI模型的传输层, 也可以称为4层负载。Nginx 与 Haproxy 即可以工作在 4 层（传输层）也可以工作于 7 层 （应用层）。 LVM 集群架构由2部分组成：最前端是负载均衡层（Load Balancer），后端是服务器群组 (一般称其为 Real Server) 负载均衡层: 由于一台或多台负载调度器(Director Server)组成。 LVS 核心模板 IPVS 就安装在 Director Server 上，而 Director 的主要作用类似于一个路由器，它含有为完成 LVS 功能所设定的路由表，通过这些路由表把用户的请求分发给服务器群组的应用服务器（Real Server）。 同时，在 Director Server 上还要安装对 Real Server 的监控模块 Ldirectord, 此模块用于监测各个 Real Server 服务健康状况。 在 Real Server 不可用时可以将其从 LVS 路由表中删除，在恢复时加入。 服务器群组层：由一组实际提供服务的机器组成，Real Server 可以在同一个网络中，也可以在不同网络中或者不同物理位置。 Linux 内核原生内转了 LVS 的各个模块，不用任何设置就可以支持 LVS 功能。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:1:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"IPVS 负载均衡实现方式 IPVS 实现负载均衡的方式有3种，分别是 NAT, TUN 和 DR。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/NAT (Virtual Server via Network Address Translation) VS/NAT 方式使用网络地址翻译技术实现虚拟服务器。当用户请求到达调度器时，调度器将请求报文的目标地址(即虚拟IP地址)改写成选定的 RealServer 地址，同时将报文的目标端口也改成选定的 RealServer 的相应端口，最后将报文发送到选定的RealServer。 在服务器得到数据后， RealServer 将数据返回给用户时，需要再次经过负载均衡调度器将报文的源地址和源端口改成虚拟IP地址和相应的端口，然后把数据发送给用户，完成整个负载调度过程。 可以看出，在 NAT 方式下，用户请求和响应的报文都需要经过负载均衡调度器重写，当请求越来越多时，调度器的处理能力将成为瓶颈。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:1","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/TUN (Virtual Server via IP Tunneling) VS/TUN 方式是通过 IP 隧道技术实现虚拟服务器。这种方式的连接调度和管理与 VS/NAT 方式一样，只是报文转发方法不同。 在 VS/TUN 方式中，调度器采用 IP 隧道技术将用户的请求转发到某个 RealServer，而这个 RealServer 将直接响应用户的请求，不再经过前端调度器。 此外，对 RealServer 的地域位置没有要求。 因为在 TUN 方式中，调度器将只处理用户请求的报文，从而使集群系统的吞吐量大大提高。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:2","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/DR (Virtual Server via Direct Routing) VS/DR 就是直接路由技术实现虚拟服务器，这种方式的连接调度和管理与前两种一样，但它的报文转发方法又有所不同，VS/DR 通过改写请求报文的 MAC 地址，将请求发送到 RealServer， 而 RealServer 将响应直接返回给用户，免去了 VS/TUN的 IP 隧道开销。 这种方式是3咱负载调度方式中性能最好的，但是要求 Director Server 与 RealServer 必须由一块网卡连在同一物理网段上。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:3","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"负载调度算法 根据前面的介绍，我们了解了LVS的三种工作模式，但不管实际环境中采用的是哪种模式，调度算法进行调度的策略与算法都是LVS的核心技术，LVS在内核中主要实现了一下十种调度算法。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"1.轮询调度 轮询调度（Round Robin 简称’RR'）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:1","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"2.加权轮询调度 加权轮询（Weight Round Robin 简称’WRR'）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:2","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"3.最小连接调度 最小连接调度（Least Connections 简称’LC'）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:3","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"4.加权最小连接调度 加权最少连接（Weight Least Connections 简称’WLC'）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:4","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"5.基于局部的最少连接 基于局部的最少连接调度（Locality-Based Least Connections 简称’LBLC'）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用’最少连接’的原则选出一个可用的服务器，将请求发送到服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:5","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"6.带复制的基于局部性的最少连接 带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称’LBLCR'）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按’最小连接’原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按’最小连接’原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:6","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"7.目标地址散列调度 目标地址散列调度（Destination Hashing 简称’DH'）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:7","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"8.源地址散列调度 源地址散列调度（Source Hashing 简称’SH'）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:8","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"9.最短的期望的延迟 最短的期望的延迟调度（Shortest Expected Delay 简称’SED'）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2  B：（1+2）/2=3/2  C：（1+3）/3=4/3  就把请求交给得出运算结果最小的服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:9","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"10.最少队列调度 最少队列调度（Never Queue 简称’NQ'）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:10","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["redis"],"content":"使用场景介绍 Memcached：多核的缓存服务，更加适合于多用户并发访问次数较少的应用场景 Redis：单核的缓存服务，单节点情况下，更加适合于少量用户，多次访问的应用场景。 redis 一般是单机多实例架构，配合 redis 集群出现。 ","date":"2021-02-12","objectID":"/posts/redis-install/:1:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"安装 Reis ","date":"2021-02-12","objectID":"/posts/redis-install/:2:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"源码编译安装 Redis cd /usr/local/src wget http://download.redis.io/releases/redis-3.2.12.tar.gz tar xzf redis-3.2.12.tar.gz yum -y install gcc automake autoconf libtool make cd /usr/local/src/redis-3.2.12 make make install ","date":"2021-02-12","objectID":"/posts/redis-install/:2:1","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"安装 Redis 服务 使用源码包中的 install_server.sh 脚本工具，安装 redis 服务 [root@10-7-171-239 redis-3.2.12]# cd utils [root@10-7-171-239 utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] Selecting default: 6379 Please select the redis config file name [/etc/redis/6379.conf] Selected default - /etc/redis/6379.conf Please select the redis log file name [/var/log/redis_6379.log] Selected default - /var/log/redis_6379.log Please select the data directory for this instance [/var/lib/redis/6379] Selected default - /var/lib/redis/6379 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6379 Config file : /etc/redis/6379.conf Log file : /var/log/redis_6379.log Data dir : /var/lib/redis/6379 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6379.conf =\u003e /etc/init.d/redis_6379 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! ","date":"2021-02-12","objectID":"/posts/redis-install/:2:2","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"配置 Redis ","date":"2021-02-12","objectID":"/posts/redis-install/:3:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"连接测试 Redis redis 默认配置文件中监听的地址和端口是 127.0.0.1:6379，无密码验证(requirepass) 可以直接使用 redis-cli 命令连接 [root@localhost ~]# /etc/init.d/redis_6379 status Redis is running (1446) [root@localhost ~]# redis-cli 127.0.0.1:6379\u003e ping PONG 输入 ping 指令，redis 回复 PONG w代表连接成功，可以正常与 redis-server 通信 ","date":"2021-02-12","objectID":"/posts/redis-install/:3:1","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"配置 redis 配置文件基础项说明 $ egrep -v '^#|^$' /etc/redis/6379.conf # 绑定的 ip 地址， 只绑定 127.0.0.1 地址是无法对外提供服务的 # 生产环境中建议配置此项 bind 127.0.0.1 10.7.171.239 # 是否启用保护模式 # 在未指定绑定地址，未向客户端请求认证密码。 在此模式下，仅从环回接口接受连接。 # 如果要从外部计算机连接到 Redis 可以使用以下方法 # 1. 关闭保护模式, 通过在线修改 (CONFIG SET protected-mode no) 或者 修改配置文件 # 2. 启动服务时加入 '--protected-mode no' 参数 # 3. 配置 bind 地址或身份验证密码 # 注意：您只需要执行上述操作之一，服务器就可以开始接受来自外部的连接。 protected-mode yes # 连接时验证的密码 requirepass S4Ea0lFLwJjehB91 # 服务监听端口 port 6379 # 是否后台运行 daemonize yes # pidfile 存放路径 pidfile /var/run/redis_6379.pid # 日志 存放路径 logfile /var/log/redis_6379.log # 日志级别 loglevel notice ## RDB 持久化配置 ## # RDB 持久化数据文件, 存储在 dir 选项配置的目录下 dbfilename dump.rdb # 数据持久化存储路径 dir /var/lib/redis/6379 # 900 秒内如果累积 1 个变更就持久化一次 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes # 最大使用内存大小 maxmemory 512m slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 ## AOF 持久化配置，如果用于缓存用途可以不开启 ## appendonly no appendfilename \"appendonly.aof\" appendfsync everysec ","date":"2021-02-12","objectID":"/posts/redis-install/:3:2","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"在线查看和修改 redis 配置 在线查看配置项 # 查看所有配置项 127.0.0.1:6379\u003e config get * 1) \"dbfilename\" 2) \"dump.rdb\" 3) \"requirepass\" 4) \"123\" 5) \"masterauth\" 6) \"\" 7) \"unixsocket\" 8) \"\" 9) \"logfile\" .... # 查看验证密码 127.0.0.1:6379\u003e config get maxmemory 1) \"maxmemory\" 2) \"0\" # 模糊查看配置项 127.0.0.1:6379\u003e config get maxm* 1) \"maxmemory\" 2) \"128000000\" 3) \"maxmemory-samples\" 4) \"5\" 5) \"maxmemory-policy\" 6) \"noeviction\" 在线调整配置项 # 配置最大使用内存量 127.0.0.1:6379\u003e config set maxmemory 128m OK # 配置连接验证密码 127.0.0.1:6379\u003e config set requirepass 123 OK # 连接验证 127.0.0.1:6379\u003e auth 123 OK # 查看配置 127.0.0.1:6379\u003e config get maxmemory 1) \"maxmemory\" 2) \"128000000\" 127.0.0.1:6379\u003e config get requirepass 1) \"requirepass\" 2) \"123\" # 将修改的配置修改写入配置文件中 127.0.0.1:6379\u003e config rewrite OK ","date":"2021-02-12","objectID":"/posts/redis-install/:3:3","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["mongodb"],"content":"备份工具介绍 MongoDB 自带两种备份工具, 以备份出的文件区分为文本备份工具与二进制备份工具，各有不同的适用场景。 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:0","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"文本备份工具 使用此工具备份出的文件是可读的，备份格式可选为 json 或 csv。 适用场景 异构平台: 当我们需要迁移 mysql 数据至 mongodb 时就可以选用此工具了(相反亦可)。 同平台，跨大版本升级：mongodb2 –\u003e mongodb3 mongoexport: 以 CSV 或 JSON 格式从 MongoDB 导出数据 mongoimport: 将 CSV，TSV 或 JSON 数据导入 MongoDB。 如果未提供文件，则 mongoimport 从 stdin 中读取。 在 test 库中生成测试数据 ues test for (var i=1; i\u003c=10000; i++){ db.rands.insert({id: i, date: new Date()}) } mongoexport json 格式 mongoexport 默认导入数据为 json 格式 $ mongoexport -u root -p root123 --authenticationDatabase admin -d test -c rands -o rands.json 2021-02-24T10:38:15.398+0800 connected to: localhost 2021-02-24T10:38:15.557+0800 exported 10000 records csv 格式 导出 csv 格式需要加上 --type 选项并指定要导出的键名使用 -f 选项 $ mongoexport -u root -p root123 --authenticationDatabase admin -d test -c rands --type=csv -f id,date -o rands.csv 2021-02-24T10:44:01.075+0800 connected to: localhost 2021-02-24T10:44:01.144+0800 exported 10000 records mongoimport 导入 json 数据 $ mongoimport -u root -p root123 --authenticationDatabase admin -d test -c rands --file rands.json 2021-02-24T10:52:55.845+0800 connected to: localhost 2021-02-24T10:52:55.935+0800 imported 10000 documents 导入 csv 数据 如果 csv 文件首行是为列名，需要加入 --headerline 选项，如果不是需要使用 -f 选项指定列名. --headerline: 指明第一行是列名，不需要导入 mongoimport -u root -p root123 --authenticationDatabase admin -d test -c rands --type=csv --headerline --file rands.csv 2021-02-24T10:55:23.714+0800 connected to: localhost 2021-02-24T10:55:23.776+0800 imported 10000 documents 注意: 数据导入是追加导入，所以不重复导入以免数据重复 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:1","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"二进制备份工具 日常备份恢复推荐使用此工具 mongodump 能够在 Mongodb 运行时进行备份，它的工作原理是对运行的 Mongodb 做查询，然后将所有查到的文档写入磁盘。 但是存在的问题时使用 mongodump 产生的备份不一定是数据库的实时快照，如果我们在备份时对数据库进行了写入操作， 则备份出来的文件可能不完全和 Mongodb 实时数据相等。另外在备份时可能会对其它客户端性能产生不利的影响。 mongodump 参数说明 -h: 指明数据库宿主机的 IP -u: 指明数据库的用户名 -p: 指明数据库的密码 --authenticationDatabase: 指明验证库名 -d: 指明数据库的名字 -c: 指明 collection 的名字 -o: 指明到要导出到的路径名 -q: 指明导出数据的过滤条件 -j, --numParallelCollections: 并行转储的集合数（默认为4个） --oplog: 备份的同时备份 oplog 全库备份 不指定 -d 和 -c 选项时备份全库 $ mongodump -u root -p root123 --authenticationDatabase admin -o ./full 2021-02-24T11:13:12.997+0800 writing admin.system.users to 2021-02-24T11:13:12.997+0800 done dumping admin.system.users (1 document) 2021-02-24T11:13:12.997+0800 writing admin.system.version to 2021-02-24T11:13:12.998+0800 done dumping admin.system.version (2 documents) 2021-02-24T11:13:12.998+0800 writing test.rands to 2021-02-24T11:13:13.035+0800 done dumping test.rands (20000 documents) mongodump 备份的是 bson 格式的二进制文件, 备份目录不存在自动创建，目录结构按库名分 备份 test 库 $ mongodump -u root -p root123 --authenticationDatabase admin -d test -o /backup 2021-02-24T11:28:11.957+0800 writing test.rands to 2021-02-24T11:28:12.045+0800 done dumping test.rands (20000 documents) mongorestore 恢复 test 库 $ mongorestore -u root -p root123 --authenticationDatabase admin -d test /backup/test 2021-02-24T11:34:47.061+0800 the --db and --collection args should only be used when restoring from a BSON file. Other uses are deprecated and will not exist in the future; use --nsInclude instead 2021-02-24T11:34:47.061+0800 building a list of collections to restore from test dir 2021-02-24T11:34:47.062+0800 reading metadata for test.rands from test/rands.metadata.json.gz 2021-02-24T11:34:47.067+0800 restoring test.rands from test/rands.bson.gz 2021-02-24T11:34:47.153+0800 no indexes to restore 2021-02-24T11:34:47.153+0800 finished restoring test.rands (20000 documents) 2021-02-24T11:34:47.153+0800 done 当我们恢复数据库出现这样的错误信息 - E11000 duplicate key error collection: test.rands index: _id_ dup key: { : ObjectId('6035c17ea0af461fd150f74c') } 时，是因为数据重复无法写入此可以加入 --drop 选项解决, 但不建议使用 --drop 选项，此操作危险，可能会有数据丢失的风险。 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:2","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"Replication Set 基本原理 MongoDB 复制集的基本构成是一主两从的结构，自带互相监控投标机制，使用 Raft 协议保证数据一致性，（MySQL MGR 用的是 Paxos 变种） 如果发生主库宕机，复制集内部会进行投票选举，选择一个新的主库替代原有主库对外提供服务。同时复制集会自动通知 客户端程序，主库已经发生切换了。应用就会连接到新的主库。 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:1:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"Replication Set 配置过程 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"多实例复制集环境规划 三个以上的 mongodb 节点或多实例, 这里使用多实例。 多实例端口: 28017、28018、28019 多实例配置目录: /data/mongodb/{28017,28018,28019}/etc 多实例配置目录: /data/mongodb/{28017,28018,28019}/logs 多实例数据目录: /data/mongodb/{28017,28018,28019}/data ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"创建多实例环境 使用以下脚本创建 MongoDB 多实例 #!/bin/bash # # filename: mongodb-instances.sh # for port in {28017..28019}; do # 创建多实例目录 mkdir -p /data/mongodb/$port/etc mkdir -p /data/mongodb/$port/logs mkdir -p /data/mongodb/$port/data chown -R mongod.mongod /data/mongodb/$port # 生成配置文件 cat \u003e /data/mongodb/$port/etc/mongod.conf \u003c\u003cEOF systemLog: destination: file path: /data/mongodb/$port/logs/mongodb.log logAppend: true storage: journal: enabled: true dbPath: /data/mongodb/$port/data directoryPerDB: true #engine: wiredTiger wiredTiger: engineConfig: cacheSizeGB: 1 directoryForIndexes: true collectionConfig: blockCompressor: zlib indexConfig: prefixCompression: true processManagement: fork: true net: bindIp: 127.0.0.1,10.7.171.239 port: $port replication: oplogSizeMB: 2048 replSetName: my_repl EOF echo \"start mongodb $portinstance\" echo \"/usr/local/mongodb/bin/mongod -f /data/mongodb/$port/etc/mongod.conf\" done 执行脚本 $ bash mongodb-instances.sh start mongodb 28017 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28017/etc/mongod.conf start mongodb 28018 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28018/etc/mongod.conf start mongodb 28019 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28019/etc/mongod.conf 启用 mongodb 多实例服务 ## 使用 mongod 用户启动 mongodb 多实例服务 # su - mongod $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28017/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15702 child process started successfully, parent exiting $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28018/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15731 child process started successfully, parent exiting $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28019/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15760 child process started successfully, parent exiting ## 查看服务启动状态 $ ss -anptl | grep mongo LISTEN 0 128 10.7.171.239:28017 *:* users:((\"mongod\",pid=15702,fd=12)) LISTEN 0 128 127.0.0.1:28017 *:* users:((\"mongod\",pid=15702,fd=11)) LISTEN 0 128 10.7.171.239:28018 *:* users:((\"mongod\",pid=15731,fd=12)) LISTEN 0 128 127.0.0.1:28018 *:* users:((\"mongod\",pid=15731,fd=11)) LISTEN 0 128 10.7.171.239:28019 *:* users:((\"mongod\",pid=15760,fd=12)) LISTEN 0 128 127.0.0.1:28019 *:* users:((\"mongod\",pid=15760,fd=11)) ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"配置普通复制集 配置一主两从，两普通从库 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"初始化复制集 $ mongo --port 28017 admin ## 定义初始化信息 \u003e config = {_id: 'my_repl', members: [ {_id: 0, host: '10.7.171.239:28017'}, {_id: 1, host: '10.7.171.239:28018'}, {_id: 2, host: '10.7.171.239:28019'}] } ## 初始化复制集 \u003e rs.initiate(config) { \"ok\" : 1, \"operationTime\" : Timestamp(1614066863, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614066863, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查询复制集状态 my_repl:PRIMARY\u003e rs.status() { \"set\" : \"my_repl\", \"date\" : ISODate(\"2021-02-23T07:56:10.466Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) } }, \"members\" : [ ## 这里记录集群中所有实例的信息及其状态 { \"_id\" : 0, \"name\" : \"10.7.171.239:28017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 421, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1614066874, 1), \"electionDate\" : ISODate(\"2021-02-23T07:54:34Z\"), \"configVersion\" : 1, \"self\" : true, \"lastHeartbeatMessage\" : \"\" }, { \"_id\" : 1, \"name\" : \"10.7.171.239:28018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 106, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T07:56:10.130Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T07:56:08.582Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"10.7.171.239:28019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 106, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T07:56:10.130Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T07:56:08.581Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1614066965, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614066965, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"特殊从节点 (arbiter) arbiter 节点，翻译过来就是仲裁节点的意义，arbiter 主要负责选主过程中的投票，但是不存储任何数据，也不提供任何服务。 当我们想搭建一主一从的 MongoDB 复制集时就需要配置 arbiter 节点了。 搭建过程和搭建普通复制集基本是一样的，就是初始化配置多加一个配置。 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"arbiter 复制集 $ mongo --port 28017 admin ## 定义初始化信息 \u003e config = {_id: 'my_repl', members: [ {_id: 0, host: '10.7.171.239:28017'}, {_id: 1, host: '10.7.171.239:28018'}, {_id: 2, host: '10.7.171.239:28019', \"arbiterOnly\": true }] } ## 初始化复制集 \u003e rs.initiate(config) ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"将普通复制庥更改为含有 arbiter 节点复制集 要想将普通节点改为 arbiter 节点，需要先移除在添加为 arbiter 节点。 此例我们将 10.7.171.239:28019 节点更改为 arbiter 节点 ## 移除 10.7.171.239:28019 节点 my_repl:PRIMARY\u003e rs.remove('10.7.171.239:28019') { \"ok\" : 1, \"operationTime\" : Timestamp(1614069229, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069229, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ## 添加为 arbiter 节点 my_repl:PRIMARY\u003e rs.addArb('10.7.171.239:28019') { \"ok\" : 1, \"operationTime\" : Timestamp(1614069241, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069241, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 复制集中的节点被移除时服务会自动停止，需要手动开启服务 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查看 arbiter 复制集 my_repl:PRIMARY\u003e rs.status() { \"set\" : \"my_repl\", \"date\" : ISODate(\"2021-02-23T08:38:56.291Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) } }, \"members\" : [ { \"_id\" : 0, \"name\" : \"10.7.171.239:28017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 2987, \"optime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"electionTime\" : Timestamp(1614066874, 1), \"electionDate\" : ISODate(\"2021-02-23T07:54:34Z\"), \"configVersion\" : 3, \"self\" : true, \"lastHeartbeatMessage\" : \"\" }, { \"_id\" : 1, \"name\" : \"10.7.171.239:28018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 2672, \"optime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T08:38:56.010Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T08:38:56.015Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 3 }, { \"_id\" : 2, \"name\" : \"10.7.171.239:28019\", \"health\" : 1, \"state\" : 7, \"stateStr\" : \"ARBITER\", \"uptime\" : 174, \"lastHeartbeat\" : ISODate(\"2021-02-23T08:38:56.047Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T08:38:55.204Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"configVersion\" : 3 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1614069535, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069535, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 注意 “10.7.171.239:28019” 节点 “stateStr” 字段的值，此为 “ARBITER”， 说明 arbiter 节点配置成功 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:3","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"复制集管理操作 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查看复制集状态信息 \u003e rs.status(); //查看整体复制集状态 \u003e rs.isMaster(); // 查看当前是否是主节点 \u003e rs.conf()； //查看复制集配置信息 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"添加删除节点 \u003e rs.remove(\"ip:port\"); // 删除一个节点 \u003e rs.add(\"ip:port\"); // 新增从节点 \u003e rs.addArb(\"ip:port\"); // 新增仲裁节点 注意: 以下操作需要在主节点上进行 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"什么是验证库？ 验证库是建立用户时 use 到的库，在使用用户时，要加上验证库才能登陆。 对于管理员用户, 必须在 admin 下创建(先 use admin，再创建管理员用户)。 需要注意点 建用户时, use 到的库,就是此用户的验证库 登录时,必须明确指定验证库才能登录 通常,管理员用的验证库是 admin, 普通用户的验证库一般是所管理的库设置为验证库 如果直接登录到数据库,不进行use, 默认的验证库是 test, 不是我们生产建议的. 从 3.6 版本开始，不添加 bindIp 参数，默认不让远程登录，只能本地管理员登录。 ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:1:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"创建用户并赋于权限 创建管理员用户 \u003e use admin \u003e db.createUser( { user: \"root\", pwd: \"root123\", roles: [ { role: \"root\", db: \"admin\" } ] }) 基本语法说明 user: 用户名 pwd: 用户密码 roles: role: 角色名，常用角色名(root, readWrite,read) db: 作用的库对象 ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:2:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"启用 mongodb 用户验证 在 /etc/mongod.conf 配置文件中加入以下配置以启用用户验证功能, 然后重启 MongoDB 服务 security:authorization:enabled 测试连接 mongo -u root -p root123 127.0.0.1/admin 查看用户信息 \u003e use admin # 先 use 到验证库 switched to db admin \u003e db.system.users.find().pretty() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:3:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"删除用户 ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"创建测试用户 \u003e use test switched to db test \u003e db.createUser({user: \"test\",pwd: \"test123\",roles: [ { role: \"readWrite\" , db: \"test\" }]}) Successfully added user: { \"user\" : \"test\", \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:1","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"删除用户 查看所有用户 \u003e use admin switched to db admin \u003e db.system.users.find().pretty() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } { \"_id\" : \"test.test\", \"userId\" : UUID(\"505db884-397e-4eee-a050-2dab9a6dc500\"), \"user\" : \"test\", \"db\" : \"test\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"P0mVJ7NqhnXkzzXyWEfQFw==\", \"storedKey\" : \"JOjf0Xya+cOKTKuGCki7J7f7GNI=\", \"serverKey\" : \"TQLjC7FQabHjrZOtWuxIFv8kfZg=\" } }, \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } 删除用户 删除用户时需要先 use 到此用户的验证库，再执行删除命令 # 切换到 test 用户的验证库 test 库，删除 test 用户 \u003e use test; switched to db test \u003e db.dropUser('test') true # 切换到 admin 库查看所有用户 \u003e use admin; switched to db admin \u003e db.system.users.find() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:2","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"下载 MongoDB 3.6 MongoDB 社区版下载地址 [root@localhost src]# wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.6.20.tgz ","date":"2021-01-21","objectID":"/posts/mongodb-install/:1:0","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"配置 MongoDB ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:0","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"解压]安装 MongoDB [root@localhost src]# tar xzf mongodb-linux-x86_64-rhel62-3.6.20.tgz -C /usr/local/ [root@localhost src]# cd /usr/local/ [root@localhost local]# ln -s /usr/local/mongodb-linux-x86_64-rhel62-3.6.20 /usr/local/mongodb # 加入环境变量 [root@localhost mongodb]# echo 'export PATH=/usr/local/mongodb/bin:$PATH' \u003e /etc/profile.d/mongo.sh [root@localhost mongodb]# source /etc/profile ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:1","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"创建配置文件 [root@localhost mongodb]# cat \u003e /etc/mongod.conf \u003c\u003cEOF # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /data/mongo journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo # network interfaces net: port: 27017 bindIp: 127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. #security: # authorization: enabled # 是否打开用户名和密码验证 #operationProfiling: #replication: #sharding: EOF # 创建数据目录并授权 [root@localhost mongodb]# useradd -r -s /bin/false mongod [root@localhost mongodb]# mkdir -p /data/mongo [root@localhost mongodb]# chown mongod:mongod /data/mongo ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:2","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"使用 systemd 管理 MongoDB 服务 [root@localhost mongodb]# cat \u003e /usr/lib/systemd/system/mongod.service \u003c\u003cEOF [Unit] Description=MongoDB Database Server Documentation=https://docs.mongodb.org/manual After=network.target [Service] User=mongod Group=mongod Environment=\"OPTIONS=-f /etc/mongod.conf\" ExecStart=/usr/local/mongodb/bin/mongod $OPTIONS ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb PermissionsStartOnly=true PIDFile=/var/run/mongodb/mongod.pid Type=forking # file size LimitFSIZE=infinity # cpu time LimitCPU=infinity # virtual memory size LimitAS=infinity # open files LimitNOFILE=64000 # processes/threads LimitNPROC=64000 # locked memory LimitMEMLOCK=infinity # total threads (user+kernel) TasksMax=infinity TasksAccounting=false # Recommended limits for for mongod as specified in # http://docs.mongodb.org/manual/reference/ulimit/#recommended-settings [Install] WantedBy=multi-user.target EOF [root@localhost mongodb]# systemctl enable mongod.service ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:3","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"启动 MongoDB 服务 [root@localhost ~]# systemctl start mongod.service ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:4","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mysql"],"content":" 参考资料: DBAplus 社区 ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:0:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"搭建主从集群 参考 MySQL GTID 主从复制配置 ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:1:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"安装 MaxScale MaxScale Github 地址 MaxScale 下载地址 yum install https://downloads.mariadb.com/MaxScale/2.5.6/centos/7/x86_64/maxscale-2.5.6-1.rhel.7.x86_64.rpm ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:2:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"配置 MaxScale 在主库创建监控用户，路由用户 # 监控账号 create user scalemon@'%' identified by \"123456\"; grant replication slave, replication client on *.* to scalemon@'%'; # 路由用户 create user maxscale@'%' identified by \"123456\"; grant select on mysql.* to maxscale@'%'; grant show databases on *.* to maxscale@'%'; 从库会自动同步账号 开始配置 由于我们只使用 Read-Write-Service，不需要 Read-Only-Service，将其注释即可。 Read-Only-Listener 也需要同时注释 [root@db-proxy ~]# cat /etc/maxscale.cnf # MaxScale documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24/ # Global parameters # # Complete list of configuration options: # https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-maxscale-configuration-guide/ [maxscale] threads=auto log_info=1 logdir=/tmp/ admin_host=0.0.0.0 admin_secure_gui=false # Server definitions # # Set the address of the server to the network # address of a MariaDB server. # [server1] type=server address=10.10.1.11 port=3306 protocol=MariaDBBackend [server2] type=server address=10.10.1.12 port=3306 protocol=MariaDBBackend [server3] type=server address=10.10.1.13 port=3306 protocol=MariaDBBackend # Monitor for the servers # # This will keep MaxScale aware of the state of the servers. # MariaDB Monitor documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-monitor/ [MariaDB-Monitor] type=monitor module=mariadbmon servers=server1,server2,server3 user=scalemon password=123456 monitor_interval=2000 # Service definitions # # Service Definition for a read-only service and # a read/write splitting service. # # ReadConnRoute documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-readconnroute/ #[Read-Only-Service] #type=service #router=readconnroute #servers=server1,server2,server3 #user=maxscale #password=123456 #router_options=slave # ReadWriteSplit documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-readwritesplit/ [Read-Write-Service] type=service router=readwritesplit servers=server1,server2,server3 user=maxscale password=123456 # Listener definitions for the services # # These listeners represent the ports the # services will listen on. # #[Read-Only-Listener] #type=listener #service=Read-Only-Service #protocol=MariaDBClient #port=4008 [Read-Write-Listener] type=listener service=Read-Write-Service protocol=MariaDBClient port=4006 启动检查状态 [root@db-proxy ~]# systemctl start maxscale.service [root@MHA_Maxscale ~]# netstat -anptl | grep maxscale [root@db-proxy ~]# ss -anptl | grep maxscale LISTEN 0 128 *:8989 *:* users:((\"maxscale\",pid=1498,fd=23)) LISTEN 0 128 :::4006 :::* users:((\"maxscale\",pid=1498,fd=28)) 4006: 是 MaxScale 实现 MySQL 读写分离时连接使用的端口 8989: 是 MaxScale web 管理页面端口 使用 maxctrl 命令查看数据库连接状态 [root@db-proxy ~]# maxctrl list services ┌────────────────────┬────────────────┬─────────────┬───────────────────┬───────────────────────────┐ │ Service │ Router │ Connections │ Total Connections │ Servers │ ├────────────────────┼────────────────┼─────────────┼───────────────────┼───────────────────────────┤ │ Read-Write-Service │ readwritesplit │ 0 │ 0 │ server1, server3, server2 │ └────────────────────┴────────────────┴─────────────┴───────────────────┴───────────────────────────┘ [root@db-proxy ~]# maxctrl list servers ┌─────────┬────────────┬──────┬─────────────┬─────────────────┬──────┐ │ Server │ Address │ Port │ Connections │ State │ GTID │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server2 │ 10.10.1.12 │ 3306 │ 0 │ Slave, Running │ │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server1 │ 10.10.1.11 │ 3306 │ 0 │ Master, Running │ │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server3 │ 10.10.1.13 │ 3306 │ 0 │ Slave, Running │ │ └─────────┴────────────┴──────┴─────────────┴─────────────────┴──────┘ 也可以登录 Web 页面查看，地址: http://maxscale_server_ip:8989, 默认的用户名和密码是 admin/mariadb ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:3:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"测试读写分离 使用 mysql 命令连接 maxscale 4006 端口进行测试，应用端也是使用此地址和端口进行连接数据库 [root@db-proxy ~]# mysql -h 10.10.1.10 -P 4006 -u lwg -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: 5.7.28-log MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [(none)]\u003e select @@hostname; # 默认读操作会发送至从库，重复多次执行可以看到两台从库轮询的效果 +------------+ | @@hostname | +------------+ | db2 | +------------+ 1 row in set (0.01 sec) MySQL [(none)]\u003e begin; select @@hostname; rollback; # 使用开启事务方式，模拟写操作，可以看到写操作被发送到主库 Query OK, 0 rows affected (0.01 sec) +------------+ | @@hostname | +------------+ | db1 | +------------+ 1 row in set (0.00 sec) Query OK, 0 rows affected (0.00 sec) ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:4:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"MySQL MHA 架构介绍： 官方文档: https://github.com/yoshinorim/mha4mysql-manager/wiki MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本 DeNA 公司 youshimaton（现就职于Facebook公司）开发，是一套优秀的作为 MySQL 高可用性环境下故障切换和主从提升的高可用软件。 在MySQL故障切换过程中，MHA能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 该软件由两部分组成：MHA Manager（管理节点）和MHA Node（数据节点）。 MHA Manager 可以单独部署在一台独立的机器上管理多个 master-slave 集群，也可以部署在一台 slave 节点上。 MHA Node 运行在每台 MySQL 服务器上，MHA Manager 会定时探测集群中的 master 节点，当 master 出现故障时，它可以自动将最新数据的 slave 提升为新的 master，然后将所有其他的 slave 重新指向新的 master。整个故障转移过程对应用程序完全透明（配合 vip）。 在 MHA 自动故障切换过程中，MHA 试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过 ssh 访问，MHA 没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用 binlog-server 可以最大程度减少日志的缺失，大大降低数据丢失的风险。MHA 可以 binlog-server 结合起来。如果只有一个 slave已经收到了最新的二进制日志，MHA 可以将最新的二进制日志应用于其他所有的 slave 服务器上，因此可以保证所有节点的数据一致性。 目前 MHA 主要支持一主多从的架构，要搭建 MHA,要求一个复制集群中必须最少有三台数据库服务器，一主二从，即一台充当 master，一台充当备用 master，另外一台充当从库，因为至少需要三台服务器，出于机器成本的考虑，淘宝也在该基础上进行了改造，目前淘宝TMHA已经支持一主一从。 注: MHA 是一次性高可用，Failover 后, Manager 会自动退出 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:1:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"MHA 工作原理 可以将 MHA 工作原理总结如下 从宕机崩溃的 master 保存二进制日志事件（binlog events）; 识别含有最新更新的 slave； 应用差异的中继日志（relay log）到其他的 slave； 应用从 master 保存的二进制日志事件（binlog events）； 提升一个 slave 为新的 master； 使其他的 slave 连接新的 master 进行复制； ","date":"2021-01-14","objectID":"/posts/mysql-mha/:2:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"MHA 软件说明 MHA 软件由两部分组成，Manager 工具包和 Node 工具包，具体的说明如下。 Manager 工具包主要包括以下几个工具： masterha_check_ssh: 检查MHA的SSH配置状况 masterha_check_repl: 检查MySQL复制状况 masterha_manger: 启动MHA masterha_check_status: 检测当前MHA运行状态 masterha_master_monitor: 检测master是否宕机 masterha_master_switch: 控制故障转移（自动或者手动） masterha_conf_host: 添加或删除配置的server信息 Node工具包（这些工具通常由MHA Manager的脚本触发，无需人为操作）主要包括以下几个工具： save_binary_logs: 保存和复制master的二进制日志 apply_diff_relay_logs: 识别差异的中继日志事件并将其差异的事件应用于其他的 slave filter_mysqlbinlog: 去除不必要的 ROLLBACK 事件（MHA 已不再使用这个工具） purge_relay_logs: 清除中继日志（不会阻塞 SQL 线程） ","date":"2021-01-14","objectID":"/posts/mysql-mha/:3:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"环境准备 环境说明: 服务器数量: 3台，一主两从（使用 GTID 模式搭建主从环境，搭建过程略） 操作系统: Ubuntu 18.04 server, MySQL 版本: 5.7.28 注意: 如果需要使用 MHA 的 VIP 功能，三台机的网卡名必须一致 Master: ip: 10.10.1.2/24 vip: 10.10.1.10/24 (应用连接主库使用的 ip 地址) server_id: 2 mha_role: node Slave1: ip: 10.10.1.3/24 server_id: 3 mha_role: node Slave2: ip: 10.10.1.4/24 server_id: 4 mha_role: node, manager 创建 mha 管理 mysql 用户， 在主库执行 create user mha@'10.10.1.%' identified by 'YMhHZawmFAFBEf6T'; grant all privileges on *.* to 'mha'@'10.10.1.%'; 配置 mysql, mysqlbinlog 软链接 ln -s /usr/local/mysql/bin/mysql /usr/bin/ ln -s /usr/local/mysql/bin/mysqlbinlog /usr/bin/ ","date":"2021-01-14","objectID":"/posts/mysql-mha/:4:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 SSH 互信 MHA Manager 在内部通过 SSH 连接到 MySQL 服务器。最新从站上的 MHA 节点还通过 SSH（scp）在内部将中继日志文件发送到其他从站。为了使这些过程自动化，通常建议在不使用口令的情况下启用SSH公钥身份验证。您可以使用 MHA Manager 中包含的 masterha_check_ssh 命令来检查SSH连接是否正常工作。 slave2 机器上操作 ssh-keygen -t rsa ...（略） ssh-copy -i /root/.ssh/id_rsa.pub 10.10.1.4 rsync -arvP /root/.ssh/ 10.10.1.2:/root./ssh rsync -arvP /root/.ssh/ 10.10.1.3:/root./ssh ","date":"2021-01-14","objectID":"/posts/mysql-mha/:4:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 MHA MHA 下载地址: mha4mysql-node: https://github.com/yoshinorim/mha4mysql-node/releases mha4mysql-manager: https://github.com/yoshinorim/mha4mysql-manager/releases ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 mha4mysql-node 在三台机器执行安装 dpkg -i mha4mysql-node_0.58-0_all.deb apt install -f 修复 mha4mysql-node bug mha4mysql-node-0.58 版本中 /usr/share/perl5/MHA/NodeUtil.pm 文件在执行 masterha_check_repl 命令时会提示错误，修复方法: 直接从 mha4mysql-node 存储库下载最新的 NodeUtil.pm 覆盖即可 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 mha4mysql-manager 在 slave2 机器上安装 mha4mysql-manager dpkg -i mha4mysql-manager_0.58-0_all.deb ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 MHA ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"生成 MHA 配置文件 # 创建配置目录 mkdir /etc/mha # 配置 mha 配置文件 cat \u003e /etc/mha/app1.conf \u003c\u003cEOF [server default] # mha 的工作目录 manager_workdir=/var/log/masterha # mha-manager 的日志文件 manager_log=/var/log/masterha/app1.log # 主库的 BINLOG 日志存储路径 master_binlog_dir=/data/mysql/3306 # MHA管理器 ping 主库主机的频率 ping_interval=2 # mha 管理 mysql 的用户名和密码 user=mha password=123456 # mysql replication username and password repl_user=repl repl_password=123456 # ssh 连接其他服务器的用户名 ssh_user=root # 主库故障切换时 VIP 漂移脚本文件，需要自定义 master_ip_failover_script=/usr/local/bin/master_ip_failover # 故障切换时发送邮箱提示, 自定义脚本(可以调用通讯工具的 api 发送消息, 例如: 微信) report_script=/usr/local/bin/alarm.sh [server1] hostname=10.10.1.2 port=3306 [server2] hostname=10.10.1.3 port=3306 # 配置为备选主，但是如果日志量落后 master 太多话也可能不会选为新主 # 此时需要配合 check_repl_delay = 0 参数 candidate_master=1 # 不检查日志落后量 check_repl_delay=0 [server3] hostname=10.10.1.4 port=3306 [binlog1] # 不参与选主 no_master=1 hostname=10.10.1.4 # 注意此参数必须与 [server default] 下配置值不同 master_binlog_dir=/data/mysql/binlog EOF binlogserver 配置：找一台额外的机器，必须要有 MySQL 5.6 以上的版本，支持 gtid 并开启 注意: mha 配置文件名是可以自己随意指定，建议和业务有关。mha 可以管理多套主从高可用 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 vip vip 配置项 [server default] master_ip_failover_script=/usr/local/bin/master_ip_failover 注意: 需要先在主库手动配置上 vip 地址，本例是: 10.10.1.10/24 vip 切换脚本 注意: 使用 mha vip 功能需要保证所有机器的网卡名是一致的 脚本内容修改说明: 根据实际情况修改脚本 vip 变量: $vip, $ssh_start_vip, $ssh_stop_vip #!/usr/bin/env perl # Copyright (C) 2011 DeNA Co.,Ltd. # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 2 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., # 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA ## Note: This is a sample script and is not complete. Modify the script based on your environment. use strict; use warnings FATAL =\u003e 'all'; use Getopt::Long; use MHA::DBHelper; my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port, $new_master_user, $new_master_password ); # vip 变量配置处 my $vip = '10.10.1.10/24'; my $ssh_start_vip = \"/sbin/ip addr add $vip dev ens33\"; my $ssh_stop_vip = \"/sbin/ip addr del $vip dev ens33\"; GetOptions( 'command=s' =\u003e \\$command, 'ssh_user=s' =\u003e \\$ssh_user, 'orig_master_host=s' =\u003e \\$orig_master_host, 'orig_master_ip=s' =\u003e \\$orig_master_ip, 'orig_master_port=i' =\u003e \\$orig_master_port, 'new_master_host=s' =\u003e \\$new_master_host, 'new_master_ip=s' =\u003e \\$new_master_ip, 'new_master_port=i' =\u003e \\$new_master_port, 'new_master_user=s' =\u003e \\$new_master_user, 'new_master_password=s' =\u003e \\$new_master_password, ); exit \u0026main(); sub main { if ( $command eq \"stop\" || $command eq \"stopssh\" ) { # $orig_master_host, $orig_master_ip, $orig_master_port are passed. # If you manage master ip address at global catalog database, # invalidate orig_master_ip here. my $exit_code = 1; eval { # updating global catalog, etc $exit_code = 0; }; if ($@) { warn \"Got Error: $@\\n\"; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"start\" ) { # all arguments are passed. # If you manage master ip address at global catalog database, # activate new_master_ip here. # You can also grant write access (create user, set read_only=0, etc) here. my $exit_code = 10; eval { print \"Enabling the VIP - $vip on the new master - $new_master_host \\n\"; \u0026start_vip(); \u0026stop_vip(); $exit_code = 0; }; if ($@) { warn $@; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"status\" ) { print \"Checking the Status of the script.. OK \\n\"; `ssh $ssh_user\\@$orig_master_host \\\" $ssh_start_vip \\\"`; exit 0; } else { \u0026usage(); exit 1; } } sub start_vip() { `ssh $ssh_user\\@$new_master_host \\\" $ssh_start_vip \\\"`; } # A simple system call that disable the VIP on the old_master sub stop_vip() { `ssh $ssh_user\\@$orig_master_host \\\" $ssh_stop_vip \\\"`; } sub usage { print \"Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n\"; } ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置故障切换告警 故障切换告警脚本需自行开发，可以调用通讯应用的 api 接口发送信息，例如: 微信，叮叮等 [server default] report_script=/usr/local/bin/alarm.sh ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:3","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 binlogserver mha 的 binlogserver 配置项 [binlog1] no_master=1 hostname=10.10.1.4 # 注意此参数须与 [server default] 下配置值不同 master_binlog_dir=/data/mysql/binlog 启动 binlogserver 服务 # 必须进入到自己创建好的目录 cd /data/mysql/binlog mysqlbinlog -R --host=10.10.1.10 --user=mha --password=mha --raw --stop-never mysql-bin.000001 \u0026 --host=10.10.1.10: 从主库拉取二进制日志 mysql-bin.000001: 拉取二进制日志的起始日志文件名，可以在从库 show slave status\\G 查看获取 注意： 拉取日志的起点, 需要按照目前从库的已经获取到的二进制日志点为起点 binlogserver 服务可以使用 supervisor 程序管理 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:4","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"环境检测 检查 ssh 连接 root@db3:~# masterha_check_ssh --conf=/etc/mha/app1.conf Tue Dec 29 20:09:18 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:18 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Starting SSH connection tests.. Tue Dec 29 20:09:23 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:22 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:18 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [info] All SSH connection tests passed successfully. 检查 mysql 连接 root@db3:~# masterha_check_ssh --conf=/etc/mha/app1.conf Tue Dec 29 20:09:18 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:18 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Starting SSH connection tests.. Tue Dec 29 20:09:23 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:22 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:18 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [info] All SSH connection tests passed successfully. root@db3:~# masterha_check_repl --conf=/etc/mha/app1.conf Tue Dec 29 20:09:59 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:59 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:59 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:59 2020 - [info] MHA::MasterMonitor version 0.58. Tue Dec 29 20:10:01 2020 - [info] GTID failover mode = 1 Tue Dec 29 20:10:01 2020 - [info] Dead Servers: Tue Dec 29 20:10:01 2020 - [info] Alive Servers: Tue Dec 29 20:10:01 2020 - [info] 10.10.1.2(10.10.1.2:3306) Tue Dec 29 20:10:01 2020 - [info] 10.10.1.3(10.10.1.3:3306) Tue Dec 29 20:10:01 2020 - [info] 10.10.1.4(10.10.1.4:3306) Tue Dec 29 20:10:01 2020 - [info] Alive Slaves: Tue Dec 29 20:1","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:5","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"启动 MHA 由于 masterha_manager 需要手动将程序放入后台运行，这里使用 supervisor 作为进程管理工具 # 安装 supervisor root@db3:~# apt install supervisor # 启动 supervisor root@db3:~# systemctl start supervisor.service # 编写 supervisor mha 配置文件 root@db3:~# cd /etc/supervisor/conf.d/ root@db3:/etc/supervisor/conf.d# cat \u003e mha.conf \u003c\u003cEOF [program:mha] command=/usr/bin/masterha_manager --conf=/etc/mha/app1.conf --remove_dead_master_conf --ignore_last_failover process_name=%(program_name)s numprocs=1 directory=/var/log/masterha umask=022 autostart=true startsecs=1 startretries=3 autorestart=unexpected exitcodes=0,2 stopsignal=QUIT stopwaitsecs=10 stopasgroup=false killasgroup=false user=root redirect_stderr=true stdout_logfile=/var/log/masterha/app1.log stderr_logfile=/var/log/masterha/app1.log EOF # 更新 supervisor 配置 root@db3:/etc/supervisor/conf.d# supervisorctl update # 查看 supervisor 管理进程信息 root@db3:/etc/supervisor/conf.d# supervisorctl status ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:6","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"故障测试 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"测试故障 停止主库进程，查看 masterha_manager 日志信息，检查主从复制状态, 请自行测试！ ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"恢复过程 主库宕机后，binlogserver 自动停掉，masterha_manager 也会自动停止 处理思路: 检查主从复制状态 检查 masterha_manager 配置文件，查看前主库信息是否已经删除 (–remove_dead_master_conf 选项会自动删除故障主库配置信息)，如果存在故障切换可能失败。 查看 masterha_manager 日志文件 清理 binlogserver 二进制日志，重新获取新主库的 binlog 到 binlogserver 中 (使用了 vip 连接不需更改，启动服务即可) 修复故障库，手工加入主从。 重新配置 masterha_manager 配置文件 最后再启动 MHA ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"最后 虽然 mha 高可用解决了主库故障问题，但真实使用的只有一台主库别两台从库处于空闲状态，资源得不到有效的利用。 此时为了更好地利用资源，提升效率，我们可以在 mha 高可用的基础上加入读写分离架构进行优化提升效率。 参考资源: https://www.jianshu.com/p/0f7b5a962ba7 MHA 官方文档: https://github.com/yoshinorim/mha4mysql-manager/wiki ","date":"2021-01-14","objectID":"/posts/mysql-mha/:8:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"优化参数 [mysqld] # 从库配置优化 master_info_repository = TABLE relay_log_info_repository = TABLE relay_log_recovery = 1 relay-log-purge = 1 read_only = 1 super_read_only = 1 master.info: 存储连接主库的信息，已经接收的 binlog 位置点信息 (默认在从库数据目录中) 配置项: master_info_repository = FILE/TABLE master_info_repository 默认值为 FILE 存储文件名为 master.info，值为改为 TABLE 时 master.info 信息将存储在表中，可以提高性能 relay-log.info: 记录从库回放 relay-log 位置点 (默认在从库数据目录中) 配置项: relay_log_info_repository = FILE/TABLE relay_log_info_repository 默认值为 FILE 存储文件名为 relay-log.info，值为改为 TABLE 时 relay-log.info 信息将存储在表中，可以提高性能 relay_log_purge: 自动清理 relay-log 文件 read_only: 禁止写操作，从库配置可以防止误写操作 super_read_only: 禁止管理员写操作，从库配置可以防止误写操作 ","date":"2021-01-13","objectID":"/posts/mysql-replication-optimization/:1:0","tags":["mysql-replication"],"title":"MySQL 主从复制优化","uri":"/posts/mysql-replication-optimization/"},{"categories":["mysql"],"content":"延时从库 应用场景：普通主从正常情况可以应对物理损坏，但无法应用逻辑损坏。例如: drop 和 delete 等操作。 延时从库可以应对这种逻辑损坏场景： 主库做了某项操作后，等多少秒后从库再应用。 注: 延时从库延时的是 sql 线程回放 relay 日志的时间，不是与主库传输二进制日志的时间 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"配置延时从库 主要参数: MASTER_DELAY mysql\u003e stop slave; mysql\u003e CHANGE MASTER TO MASTER_DELAY=10800; mysql\u003e start slave; ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:1","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"恢复思路 1.先停业务，挂维护页 2.停从库 SQL 线程, stop slave sql_thread; 看 relay_log 位置点; stop slave; 这里只是停止 sql 线程，io 线程并没有停，也就是说主库与从库的二进制日志传输是一直存在的。 最后停止从库时注意观察主从复制二进制日志的情况是否一至。 3.追加后续缺失的日志到从库。（相当于手工替代 sql 线程工作） 日志文件: relay-log 日志文件起始位置确认: 查看命令: show slave status\\G 也可以通过查看relay-log.info 文件 cat /data/mysql/3306/relay-log.info 日志文件终点确认: 查看命令: show relaylog events in 'db2-relay-bin.000002' 查看 relaylog 日志事件，只需要看 Pos 列； End_log_pos 列是对应主库的 binlog 位置点。 4.恢复业务，直接将业务指向从库或者将数据导回到主库 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:2","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"过滤复制 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"主库配置 binlog_do_db: 需要记录二进制的库 binlog_ignore_db: 不需要记录二进制的库 [mysqld] binlog_do_db=test 二选一即可 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:1","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"从库配置 如果使用 replicate-ignore-db 参数设置不同步的库，需要注意: 使用 use 语句选库后执行的操作才会被忽略不同步，如果 sql 直接通过 库名.表名 执行的操作还是会被同步的。 如果希望不管选不选库的操作都会被忽略可以使用 replicate-wild-ignore-table 配置项 库级别 replicate_do_db: 需要复制的库名 replicate_ignore_db: 忽略复制的库名 表级别 replicate_do_table: 需要复制的库中的表 replicate_ignore_table: 忽略复制的库中的表 带有模糊匹配的配置项 replicate_wild_do_table replicate_wild_ignore_table ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:2","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"半同步复制 经典主从复制使用的异步复制工作模型，会导致主从数据不一致的情况 MySQL 5.5 版本为了保证主从数据的一致性问题，加入半同步复制的组件(插件) 在主从复制结构中都需要启用半同步复制插件。 半同步复制主要是控制从库io是否将 relay-log 写入磁盘，一旦落盘通过插件返回 ACK 给主库的 ACK_rec， 接收到 ACK 之后，主库的事务才能提交成功。 在默认情况下,如果超过 10s 没有返回 ACK，此次复制行为会切换为异步复制。 半同步复制会影响数据库性能，也并不能完全保证主从复制的数据一致性。并不推荐使用 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:3:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"备份主库 为了节省恢复的时间我们使用 xtrabackup 备份主库，然后拷贝到从库再将数据恢复到从库中 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:1:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"完整备份主库 # 备份 xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf -S /data/mysql/mysql.sock -u root -p --backup --target-dir=/data/backup ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:1:1","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"恢复主从复制 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"恢复从库数据 # 恢复准备，应用日志 xtrabackup --prepare --target-dir=/data/backup/mysql # 恢复备份 xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf --copy-back --target-dir=/data/backup/mysql # 修改数据目录的权限 chown -R mysql:mysql /data/mysql # 启动数据库 systemctl start mysqld ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:1","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"开始恢复主从复制 由于我们使用的是 xtrabackup 工具备份恢复的数据，gtid_purged 值可以从 xtrabackup_binlog_info 文件中获取。 如果是使用是 mysqldump 命令备份加上 --master-data=1 参数就可以在备份文件中看到 SET @@GLOBAL.GTID_PURGED='d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-4'; 语句，此时恢复从库数据时就会自动执行 SET @@GLOBAL.GTID_PURGED='d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-4'; 语句，在恢复主从复制时就不需要在手动执行 set global gtid_purged=... 命令\u0008啦~ stop slave; reset master; reset slave; set global gtid_purged='cdb92087-ac64-11e9-bb08-20040ff98044:1-395071'; change master to master_host='10.10.1.11', master_user='repl', master_password='123456',master_port=3306, master_auto_position=1; start slave; 注意: 设置 gtid_purged 是为了告诉从库与主库进行主从复制时的起始 GITD， 如果不加会默认从 gtid 的1号位置点执行，此时就会出现主从复制错误 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:2","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"检查，测试 请自行检测，略… ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:3:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"环境准备 准备两台服务器安装 MySQL 5.7, 参考 安装 MySQL 5.7 服务器列表 master: 10.10.1.11/24 slave1: 10.10.1.12/24 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:1:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"配置 MySQL 配置基于 GTID 的主从复制需要启动 gtid 和 binlog 功能，具体配置如下 主库: my.cnf [client] port = 3306 socket = /data/mysql/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log # gtid 配置 server_id=11 gtid_mode=on enforce-gtid-consistency = true master-info-repository = TABLE relay-log-info-repository = TABLE relay_log_recovery = on sync-master-info = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 从库: my.cnf [client] port = 3306 socket = /data/mysql/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log # gtid 配置 server_id=12 gtid_mode=on enforce-gtid-consistency = true master-info-repository = TABLE relay-log-info-repository = TABLE relay_log_recovery = on sync-master-info = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 ## 禁止从库数据写入 read_only = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:2:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"配置主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"创建主从同步用户 在主库上操作 create user 'repl'@'10.10.1.%' identified by '123456'; grant replication slave on *.* to 'repl'@'10.10.1.%'; flush privileges; ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:1","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"主从数据同步 由于当前 MySQL 环境是全新搭建的，没有任何数据，此步可以忽略。 如果是在已经运行了很久的数据库或者数据库存在数据的情况下，需要对主库进行全备然后恢复到从库，在配置启动主从复制 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:2","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"启动主从同步 在从库上执行以下命令 change master to master_host='10.10.1.11', master_user='repl', master_password='123456', master_port=3306, master_auto_position=1; start slave; GTID 主从复制只需指定 master_auto_position=1 参数即可，相比经典主从复制更简单 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:3","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"查看主从同步状态 mysql\u003e show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.10.1.11 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mybinlog.000003 Read_Master_Log_Pos: 763 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 974 Relay_Master_Log_File: mybinlog.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 763 Relay_Log_Space: 1179 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 10 Master_UUID: d6f25a03-5d80-11eb-9fe8-000c29738b1d Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-3 Executed_Gtid_Set: d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-3 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:4","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"主从同步测试 请自行测试, 略… ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:4:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"安装 MySQL 5.7 服务器列表 master: 10.10.1.2/24 slave1: 10.10.1.3/24 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"下载 MySQL root@db2:/usr/local/src# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz root@db1:/usr/local/src# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ root@db1:/usr/local# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"环境准备 # 安装依赖 root@db1:/usr/local/mysql# apt-get install libaio1 # 创建程序用户 root@db1:/usr/local/mysql# useradd -r -s /sbin/nologin mysql # 创建数据目录 root@db1:/usr/local/mysql# mkdir -p /data/mysql/3306 # 更改数据目录权限 root@db1:/usr/local/mysql# chown -R mysql.mysql /data/mysql/3306/ # 配置环境变量 root@db1:/usr/local/mysql# echo 'export PATH=/usr/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh root@db1:/usr/local/mysql# source /etc/profile ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"初始化数据库 准备 my.cnf 配置文件 root@db1:/usr/local/mysql# mkdir etc root@db1:/usr/local/mysql# cat \u003e etc/my.cnf \u003c\u003c EOF [client] port = 3306 socket = /data/mysql/3306/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql/3306 socket = /data/mysql/3306/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 log-error = /data/mysql/3306/error.log skip_name_resolve = 1 # 不同实例设置不同数字，不能相同 server-id = 1 # BINGLO 配置，主从同步必须启用 BINLOG 日志 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 初始化数据库 root@db1:/usr/local/mysql# mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3306 另一台数据库同样的方法安装初始化，就是配置文件 server_id 的值需要修改 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:3","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"配置主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"主库操作 启用主库的 binlog root@db1:/usr/local/mysql# cat etc/my.cnf [mysqld] # 不同实例设置不同数字，不能相同 server-id = 1 # BINGLO 配置，主从同步必须启用 BINLOG 日志 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 创建同步账号 mysql\u003e create user 'repl'@'10.10.1.%' identified by '123456'; mysql\u003e grant replication slave on *.* to 'repl'@'10.10.1.%'; mysql\u003e flush privileges; 导出数据用于创建 slave root@db1:~# mysqldump -uroot -p -A -R -E -B -x --master-data=2 | gzip \u003e all.sql.gz -A, –all-databases: 备份所有数据库 -E, –events: 备份事件 -B, –databases: 备份的数据库 -x, –lock-all-tables：锁定所有数据库的所有表 –master-data=2: 等于2时会将 CHANGE MASTER 命令以注释的方式加入备份文件中 将备份文件拷贝到从库还原 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"从库操作 配置 my.cnf 从库(slave)如果用于备份可以启用 binlog, 如果用于读操作可以不启用, 只配置 server-id 即可. [root@slave1 ~]# cat /usr/local/mysql/etc/my.cnf [mysqld] server-id = 2 # binlog 配置 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 从 master 恢复数据 root@db2:~# gzip -d all.sql.gz root@db2:~# mysql -uroot \u003c all.sql 设置主从同步 # 从备份文件找到 CHANGE MASTER 命令 root@db2:~# more all.sql -- CHANGE MASTER TO MASTER_LOG_FILE='mybinlog.000002', MASTER_LOG_POS=763; # 配置 slave mysql\u003e CHANGE MASTER TO -\u003e MASTER_HOST='10.10.1.2', -\u003e MASTER_PORT=3306, -\u003e MASTER_USER='repl', -\u003e MASTER_PASSWORD='123456', -\u003e MASTER_LOG_FILE='mybinlog.000002', -\u003e MASTER_LOG_POS=763; mysql\u003e start slave; mysql\u003e show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.10.1.2 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mybinlog.000002 Read_Master_Log_Pos: 763 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 319 Relay_Master_Log_File: mybinlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes ... 查看从库 Slave_IO_Running 和 Slave_SQL_Running 两IO线程状态是否为 YES，为 YES 表示主从复制成功 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"测试主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"在主库查看从库 mysql\u003e SHOW SLAVE HOSTS; +-----------+------+------+-----------+--------------------------------------+ | Server_id | Host | Port | Master_id | Slave_UUID | +-----------+------+------+-----------+--------------------------------------+ | 2 | | 3306 | 1 | 2cc18b4b-4658-11eb-adf4-000c2955408a | +-----------+------+------+-----------+--------------------------------------+ 1 row in set (0.00 sec) mysql\u003e show processlist; +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ | 5 | root | localhost | NULL | Query | 0 | starting | show processlist | | 6 | repl | 10.10.1.3:57800 | NULL | Binlog Dump | 215 | Master has sent all binlog to slave; waiting for more updates | NULL | +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ 2 rows in set (0.00 sec) SHOW SLAVE HOSTS: 查看所有从库信息 show processlist: 查看当前所有线程信息， Binlog Dump 是主库和从库主从复制专用线程，如果有多个从库会有多个 Binlog Dump 线程 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"测试主从复制 在主从库创建 test 库和 test 表，插入一些数据，然后到从库查看数据是否存在 主库 mysql\u003e create database test charset utf8mb4; Query OK, 1 row affected (0.01 sec) mysql\u003e use test; Database changed mysql\u003e create table test (id int, username varchar(60)); Query OK, 0 rows affected (0.05 sec) mysql\u003e insert into test values(1,'lisi'), (2, 'zhangshan'); Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 从库 mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql\u003e use test; mysql\u003e select * from test; +------+-----------+ | id | username | +------+-----------+ | 1 | lisi | | 2 | zhangshan | +------+-----------+ 2 rows in set (0.00 sec) ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"GTID 的概述 是对一个已提交事务的编号，并且是全局唯一的编号 全局事物标识：global transaction identifieds。 GTID事物是全局唯一性的，且一个事务对应一个GTID。 一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。 GTID 用来代替 经典 (classic) 的复制方法，不在使用 binlog + pos 开启复制。而是使用master_auto_postion=1 的方式自动匹配 GTID 断点进行复制。 MySQL-5.6.5 开始支持的，MySQL-5.6.10 后开始完善。 在传统的 slave 端，binlog 是不用开启的，但是在 GTID 中，slave 端的 binlog 是必须开启的，目的是记录执行过的GTID（强制）. ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:1:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 的组成部分 GTID 由 server_uuid 和 sequence number 组成，通过 : 连接 例如：7800a22c-95ae-11e4-983d-080027de205a:10 server_uuid: 每个mysql实例的唯一ID，由于会传递到 slave，所以也可以理解为源 ID。 sequence number：在每台MySQL服务器上都是从1开始自增长的序列，一个数值对应一个事务。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:2:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 比传统复制的优势 更简单的实现 failover，不用以前那样在需要找 log_file 和 log_Pos。 更简单的搭建主从复制。 比传统复制更加安全。 GTID是连续没有空洞的，因此主从库出现数据冲突时，可以用添加空事物的方式进行跳过。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:3:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 的工作原理 master 更新数据时，会在事务前产生 GTID，一同记录到 binlog 日志中。 slave 端的 i/o 线程将变更的 binlog，写入到本地的 relay-log 中。 sql 线程从 relay-log 中获取 GTID，然后对比 slave 端的 binlog 是否有记录。 如果有记录，说明该 GTID 的事务已经执行，slave 会忽略(幂等性)。 如果没有记录，slave 就会从 relay-log 中执行该 GTID 的事务，并记录到 binlog。 在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。 要点： slave 在接受 master 的 binlog 时，会校验 master 的 GTID 是否已经执行过（一个服务器只能执行一次）。 为了保证主从数据的一致性，多线程只能同时执行一个 GTID。 由于幂等性特点，开启 GTID 后，MySQL 恢复 binlog 时，重复的 GTID 事务不会执行. 所以在导出 binlog 日志时需要加上 --skip-gtid 参数，从而让导出 binlog 语句中不保留全局事务标识符； 而是让服务器像执行新事务一样执行事务。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:4:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"配置 GITD 启用 GTID 主要配置参数 root@db1:~# cat /usr/local/mysql/etc/my.cnf [mysqld] gtid-mode = on enforce-gtid-consistency = true ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:5:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"slowlog 慢日志 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:0","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"作用 记录 MySQL 运行过程运行过慢的语句，通过一个文本的文件记录下来。 帮助我们进行语句优化工作。 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:1","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"配置慢日志 root@db1:~# cat /usr/local/mysql/etc/my.cnf [mysqld] # 慢语句开关 slow_query_log = 1 # 慢日志存储路径 slow_query_log_file = /data/mysql/3306/slow.log # 定义慢语句时间阈值单位为秒 long_query_time = 1 # 记录不走索引的语句 log_queries_not_using_indexes = 1 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:2","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"查看慢日志 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:0","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"mysqldumpslow 参数说明 root@db1:~# mysqldumpslow -h -s ORDER 日志排序方式, 默认排序为 'at' al: 平均锁定时间 ar: 发送的平均行数 at: 平均查询时间 c: 执行次数 l: 锁定时间 r: 发送行数 t: 查询时间 -r 反向排序 -t NUM 只显示前n个查询 查看执行次数最多的前5条慢语句 root@db1:~# mysqldumpslow -s c -t 5 /data/mysql/3306/slow.log ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:1","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"可视化展示慢日志 slow-log 工具: pt-query-digest + Amemometer ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:2","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"mysqlbinlog 参数说明 -d, --database 指定截取日志的库名 --start-position 截取日志起始 position 号 --stop-position 截取日志最终 position 号 --start-datetime 截取日志开始时间 --stop-datetime 截取日志结束时间 --skip-gtids 不保留全局事务标识符； 而是让服务器像执行新事务一样执行事务。 --include-gtids=name 截取日志起始和结束 GTID 号，例如: 09833d3f-4656-11eb-9892-000c2913c78e:1-4 --exclude-gtids=name 截取日志排除的 GTID 号 注: –skip-gtids 在启用 GTID 时需要加上，否则数据无法通过导出的 sql 还原, 这是由于 GTID 的特性（幂等性）导致的。 截取标志可以混用，也可以只指定起始位置点，不指定结束位置点(默认到最新的点) ","date":"2021-01-11","objectID":"/posts/mysql-mysqlbinlog/:1:0","tags":["mysqlbinlog"],"title":"MySQL mysqlbinlog 命令使用说明","uri":"/posts/mysql-mysqlbinlog/"},{"categories":["mysql"],"content":"通过 BINLOG 恢复数据 BINLOG 一般配合备份一起使用，单独使用 BINLOG 恢复数据在数据量大的情况比较困难。 ","date":"2021-01-11","objectID":"/posts/mysql-mysqlbinlog/:2:0","tags":["mysqlbinlog"],"title":"MySQL mysqlbinlog 命令使用说明","uri":"/posts/mysql-mysqlbinlog/"},{"categories":["mysql"],"content":"binlog 作用 主要记录数据库变化(DDL,DML,DCL)性质的日志 用于数据恢复：如果你的数据库出问题了，而你之前有过备份，那么可以看日志文件，找出是哪个命令导致你的数据库出问题了，想办法挽回损失。 主从服务器之间同步数据：主服务器上所有的操作都在记录日志中，从服务器可以根据该日志来进行，以确保两个同步。 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:1:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"配置 MySQL 8.0 版本以前默认都没有开启，生产环境建议开启 配置方法 [mysqld] # 主机编号，主从中使用，5.7 版本中 开启 binlog 必须设置 server_id server_id = 1 # binlog 日志名前缀，『/data/mysql/binlog』 binlog 存储目录， 『mysql-bin』 binlog 文件名前缀，便如 mysql-bin.000001 mysql-bin.000002 log_bin = /data/mysql/binlog/mysql-bin # binlog 日志写入磁盘策略，双1参数中的其中一个 sync_binlog = 1 # binlog 日志记录格式为 row binlog_format = row # binlog 保存天数 expire_logs_days = 7 注意: binlog 日志最好和数据分开存储 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:2:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"记录内容 binlog 是 SQL 层的功能。记录的是变更 SQL 语句，不记录查询语句。 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"记录 SQL 语句种类 DDL：原封不动记录当前 DDL（statement 语句方式） DML：原封不动记录当前 DDL（statement 语句方式） DCL：只记录已提交事务的 DML (insert, update, delete) ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"DML 三种语句记录方式 statement (5.6 默认) SBR (statement based replication): 原封不动的记录当前DML ROW (5.7 默认) RBR (ROW based replication): 记录数据行的变化，用户看不懂（需要工具分析） mixed (混合) MBR (mixed based replication): 以上两模式的混合 STATEMENT 模式 可读性较高，日志量小，不够严谨 ROW 模式 可读性很低，日志量大，足够严谨 建议使用 ROW 模式 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:2","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"事件的简介 二进制日志的最小单元，对于 DML,DCL，一条语句就是一个事件(event) 对于 DML 语句来讲，只记录已提交的事务 例如: 以下列子，分为4个事件 begin; 101-320 DML; 320-630 DML; 630-740 commit; 740-810 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:4:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"event 的组成 三部分构成 事件的开始标识 事件的内容 事件的结束标识 Position 开始标记: at 1262 结束标记: end_log_pos 1312 作用: 为了方便截取日志 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:4:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"查看二进制日志 查看一共有几个日志文件 mysql\u003e show binary logs; +-----------------+-----------+ | Log_name | File_size | +-----------------+-----------+ | mybinlog.000001 | 177 | | mybinlog.000002 | 1403 | +-----------------+-----------+ 查看当前在用的日志文件 mysql\u003e show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | mybinlog.000002 | 1403 | | | | +-----------------+----------+--------------+------------------+-------------------+ 查看二进日志事件 mysql\u003e show binlog events in 'mybinlog.000002'; +-----------------+------+----------------+-----------+-------------+-----------------------------------------------------------------------------------------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +-----------------+------+----------------+-----------+-------------+-----------------------------------------------------------------------------------------------------------------------+ | mybinlog.000002 | 4 | Format_desc | 1 | 123 | Server ver: 5.7.28-log, Binlog ver: 4 | | mybinlog.000002 | 123 | Previous_gtids | 1 | 154 | | | mybinlog.000002 | 154 | Anonymous_Gtid | 1 | 219 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' | | mybinlog.000002 | 219 | Query | 1 | 407 | CREATE USER 'repl'@'10.10.1.%' IDENTIFIED WITH 'mysql_native_password' AS '*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9' | | mybinlog.000002 | 407 | Anonymous_Gtid | 1 | 472 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' 导出查看二进制日志 root@db1:~# mysqlbinlog /data/mysql/3306/mybinlog.000002 \u003e /tmp/2.sql # 或者， --base64-outpu=decode-rows 解码日志信息 root@db1:~# mysqlbinlog --base64-outpu=decode-rows /data/mysql/3306/mybinlog.000002 \u003e /tmp/2.sql 通过 position 号截取日志 root@db1:~# mysqlbinlog --start-position=219 --stop-position=1403 /data/mysql/3306/mybinlog.000002 \u003e /tmp/r.sql ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:5:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志管理 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:6:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志滚动 每次重启 MySQL 时 BINLOG 日志会自动滚动生成并使用新的日志文件 bin-log 文件大小达到参数 max_binlog_size 限制； 手动滚动更新 mysql\u003e flush logs; root@db1:~# mysqladmin flush-logs mysqldump 的 -F 参数也会触发自动滚动更新 BINLOG 日志文件，不建议使用 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:6:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志清理 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"自动清理方法1：（修改配置文件和在mysql内设置参数可无需重启服务） root@db1:~# vim /etc/my.cnf [mysqld] expire_logs_days = 7 // 表示日志保留7天，超过7天则设置为过期的, 默认为 0 永不过期 root@db1:~# mysql -u root -p mysql\u003e show binary logs; mysql\u003e show variables like '%log%'; mysql\u003e set global expire_logs_days = 7; expire_logs_days 一般设置为一个全备周期 + 1，如果全备周期为7天，就设置为 7+1=8 一般生产中至少保留2个全备周期 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"手动清理方法2： 如果没有主从复制，可以通过下面的命令重置数据库日志，清除之前所有的日志文件： mysql\u003e reset master 注: 此操作危险，请谨慎操作！！！ 但是如果存在复制关系，应当通过 PURGE 的名来清理 bin-log 日志，语法如下： # mysql -u root -p mysql\u003e purge master logs to 'mysql-bin.010’; //清除 mysql-bin.010 之前所有的日志 mysql\u003e purge master logs before '2016-02-28 13:00:00'; //清除2016-02-28 13:00:00前的日志 mysql\u003e purge master logs before date_sub(now(), interval 3 day); //清除3天前的bin日志 注意，不要轻易手动去删除 binlog，会导致 binlog.index 和真实存在的 binlog 不匹配，而导致 expire_logs_day 失效 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:2","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"information_schema 库 ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:0","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"统计单表占用物理空间大小 查询表: information_schema.tables 计算公式: 方法一: 单表占用空间大小 = AVG_ROW_LENGTH * TABLE_ROWS + INDEX_LENGTH 方法二: 单表占用空间大小 = DATA_LENGTH 示例: 查看 employees 库中 salaries 表的占用空间大小 mysql\u003e select table_schema,table_name, -\u003e (avg_row_length * table_rows + index_length) / 1024 / 1024 as data_mb -\u003e from tables where table_schema='employees' and table_name = 'salaries'; +--------------+------------+-------------+ | table_schema | table_name | data_mb | +--------------+------------+-------------+ | employees | salaries | 94.74268913 | +--------------+------------+-------------+ ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:1","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"查看数据库碎片占用最大的表, 前 10 名 mysql\u003e select table_schema,table_name, data_free / 1024 / 1024 as data_free_mb from tables order by data_free_mb limit 10; ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:2","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"MyISAM 引擎默认是支持通过拷贝文件方式迁移数据，InnoDB 引擎不支持。 如果需要迁移 InnoDB 引擎数据可以先将数据表的引擎由 InnoDB 更改为 MyISAM。 也可以通过管理 MySQL 独立表空间文件实现数据库的迁移。操作步骤如下: ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:0:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"准备测试数据 可以使用 MySQL 官方提供的测试数据进行实验演示: https://github.com/datacharmer/test_db git clone https://github.com/datacharmer/test_db.git cd test_db mysql -t \u003c employees.sql ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:1:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"导出库中所有表结构 [root@10-13-90-34 ~]# mysqldump -d -B employees \u003e employees_schema.sql ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:2:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"在目标数据库中创建与源库一样的表文件 [root@10-13-90-34 ~]# mysql -S /data/mysql/3308/mysql.sock mysql\u003e source employees_schema.sql; mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | performance_schema | | sys | +--------------------+ ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:3:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"管理表空间文件，恢复数据 删除表空间文件 删除表空间文件时可能会由于外键约束导致失败，可以先暂时关闭外键约束 SET foreign_key_checks = 0;, 操作完成后在开启 SET foreign_key_checks = 1; alter table employees.departments discard tablespace; alter table employees.dept_emp discard tablespace; alter table employees.dept_manager discard tablespace; alter table employees.employees discard tablespace; alter table employees.salaries discard tablespace; alter table employees.titles discard tablespace; 导入表空间文件 将源库中所有表的 idb 文件拷贝到目标库中并修改权限 [root@10-13-90-34 employees]# cp -p /data/mysql/3306/employees/*.ibd /data/mysql/3308/employees 导入表空间文件 alter table employees.departments import tablespace; alter table employees.dept_emp import tablespace; alter table employees.dept_manager import tablespace; alter table employees.employees import tablespace; alter table employees.salaries import tablespace; alter table employees.titles import tablespace; 开启外键约束 SET foreign_key_checks = 1; 验证数据 注意: 此方法操作有风险，不到万不得已不建议使用 ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:4:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"mysqldump 参数说明 -A, –all-databases: 备份所有库 -B, –databases: 使用此参数可以同时备份多个库 单库备份可以加上 -B 参数，这样备份文件中加会加入 create database ... 及 use DATABASE 语句. –master-data=2: 加入此参数可以记录 binlog 日志文件位置和文件名 (生产中建议加上此参数) 备份时会自动记录 binlog 日志信息在备份文件中，值为2时以注释的形式记录，值为1时以语句形式记录。 会自动锁全表及解锁 加 –single-transaction 参数，可以减少锁表时间 --master-data 在生产中建议使用的值为 2 –single-transaction 对于 Innodb引擎表备份时，开启一个独立事务，获取一个一致性快照进行备份。 -R: 备份存储过程，函数 -E: 备份事件 -triggers: 备份触发器 -d, –no-data: 不备份数据，只备份数据结构 -n, –no-create-db: 不生成创建数据库语句 -t, –no-create-info: 不生成创建表语句 mysqldump 可选参数 --max_allowed_packet=64M 建议使用 mysqldump 命令参数: mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:1:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"数据库备份 ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份所有库 [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E -A \u003e all.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:1","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"只备份所有库的结构 [root@localhost ~]# mysqldump -uroot -p -A -d \u003e all.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:2","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份单个数据库 [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E -B DATABASENAME \u003e DATABASENAME.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:3","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"一次备份多个数据库 (-B, –databases) [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E --databases db1 db2 \u003e dbs.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:4","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份数据库中指定的表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME TABLENAME \u003e DATABASENAME_TABLENAME.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:5","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"一次备份数据库中指定的多张表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME t1 t2 \u003e DATABASENAME_ts.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:6","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"导出函数或者存储过程 mysqldump -h HOSTNAME -u USERNAME -p PASSWORD -ntd --triggers -R -E DATABASENAME \u003e DATABASENAME.sql -ntd 表示不导出数据及创建库和表的语句； ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:7","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"数据库恢复 当我们需要还原数据时可以通过以下命令进行还原 方法1 直接还原数据库，如果备份语句中没有禁用记录 binlog 会产生大量无用的 binlog 信息增加还原时长 [root@localhost ~]# mysql -uroot -p \u003c all.sql 方法2 先连接上数据库，然后临时禁止记录 binlog 日志，再还原数据库文件，最后在开启 binlog 日志记录功能(断开重连也会恢复) [root@localhost ~]# mysql -uroot -p mysql\u003e set sql_log_bin=0; mysql\u003e source /data/bak/backup.sql; mysql\u003e set sql_log_bin=0; ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:3:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql","xtrabackup"],"content":"percona-xtrabackup 是物理备份工具，拷贝数据文件。 原生态支持全备和增量备份。 会记录二进制日志文件及位置。 InnoDB 表: 热备份，业务正常发生时，影响较小的备份方式 非 InnoDB 表: 温备份，会锁表 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:0:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"安装 percona-xtrabackup 下载地址: https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/ ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:1:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"xtrabackup 使用 使用 xtrabackup 命令前提条件 数据库必须启动 能连接上数据库，指定用户名，密码，socket 配置文件 my.cnf 中必须配置 datadir 参数 配置 my.cnf [client] # 配置客户端工具连接 socket 文件路径，有此参数 xtrabackup 可以省略 -S 参数 socket = /data/mysql/3306/mysql.sock [mysqld] # 配置数据目录 datadir = /data/mysql/3306 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"全量备份 root@db1:/data/bak# xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf -u root -p --backup --target-dir=/data/bak/full-$(date +%F) –defaults-file: 指定 my.cnf 配置文件，此参数必须放在第一位 -u: 数据库用户名 -p: 用户密码 –target-dir: 指定备份存储目录 如果 my.cnf 配置文件的 [client] 配置项中没有指定 socket 参数，需要指定 -S 指定 mysql.sock 文件路径 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:1","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"还原数据 准备数据 此步操作主要应用那些还没有应用的事务，该回滚的事务回滚，该提交的事务提交。 该步骤可以使文件在单个时间点上完全一致。 root@db1:/data/bak# xtrabackup --prepare --target-dir=/data/bak/full-2020-12-25 注: 准备数据操作过程不能被中断，否则备份将不可用。 还原数据 可以直接修改 my.cnf 的 datadir 值，将路径修改为备份路径，然后修改备份目录的权限即可 直接拷贝文件到 MySQL 数据目录中，然后修改数据目录下所有文件的权限即可 可以直接使用 cp or rsync 命令，也可以使用 xtrabackup 命令。 root@db1:/data/bak# xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf --copy-back --target-dir=/data/bak/full-2020-12-25 root@db1:/data/bak# chown -R mysql.mysql /data/mysql/3306/ 也可以使用如下命令 cp -a /data/bak/full-2020-12-25 /data/mysql/3306 rsync -avrP /data/bak/full-2020-12-25/ /data/mysql/3306/ ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:2","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql"],"content":"适用于 MySQL5.6 及之前的版本 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:0","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"1. 停止MySQL服务 执行： /etc/init.d/mysql stop，你的机器上不一定是 /etc/init.d/mysql 也可能是 /etc/init.d/mysqld ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:1","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"2. 跳过验证启动MySQL /usr/local/mysql/bin/mysqld_safe --skip-grant-tables \u003e/dev/null 2\u003e\u00261 \u0026 注：如果 mysqld_safe 命令所在的路径和上面不一样需要修改成你的，如果不清楚可以用find命令查找。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:2","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"3. 重置密码 等一会儿，然后执行： /usr/local/mysql/bin/mysql -u root 出现mysql提示符后输入：update mysql.user set password=password('要设置的密码') where user='root'; 回车后执行：flush privileges; 刷新 MySQL 系统权限相关的表。再执行：exit; 退出。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:3","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"4. 重启MySQL 杀死 MySQL 进程： killall mysqld 重启 MySQL： /etc/init.d/mysql start ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:4","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"MySQL5.7 重置 root 密码 编辑 my.cnf 文件加入以下配置 [mysqld] skip-grant-tables 重启 mysql，正常连接 mysql 使用如下命令修改 root 密码 update mysql.user set authentication_string=PASSWORD('123456') where user='root'; flush privilegs; 最后在去除 my.cnf 配置文件中的 skip-grant-tables 配置项,重启 mysql 即可。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:2:0","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"一、用户权限管理 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 查看帮助信息 使用 mysql 命令连接上 MySQL 服务后可以使用 help 命令查看帮助信息，例如: mysql\u003e help For information about MySQL products and services, visit: http://www.mysql.com/ For developer information, including the MySQL Reference Manual, visit: http://dev.mysql.com/ To buy MySQL Enterprise support, training, or other products, visit: https://shop.mysql.com/ List of all MySQL commands: Note that all text commands must be first on line and end with ';' ? (\\?) Synonym for `help'. clear (\\c) Clear the current input statement. connect (\\r) Reconnect to the server. Optional arguments are db and host. delimiter (\\d) Set statement delimiter. edit (\\e) Edit command with $EDITOR. ego (\\G) Send command to mysql server, display result vertically. exit (\\q) Exit mysql. Same as quit. go (\\g) Send command to mysql server. help (\\h) Display this help. nopager (\\n) Disable pager, print to stdout. notee (\\t) Don't write into outfile. pager (\\P) Set PAGER [to_pager]. Print the query results via PAGER. print (\\p) Print current command. prompt (\\R) Change your mysql prompt. quit (\\q) Quit mysql. rehash (\\#) Rebuild completion hash. source (\\.) Execute an SQL script file. Takes a file name as an argument. status (\\s) Get status information from the server. system (\\!) Execute a system shell command. tee (\\T) Set outfile [to_outfile]. Append everything into given outfile. use (\\u) Use another database. Takes database name as argument. charset (\\C) Switch to another charset. Might be needed for processing binlog with multi-byte charsets. warnings (\\W) Show warnings after every statement. nowarning (\\w) Don't show warnings after every statement. resetconnection(\\x) Clean session context. For server side help, type 'help contents' 例如查看 select 语句的用法 可以使用 help select； 命令查看帮助信息 mysql\u003e help select; Name: 'SELECT' Description: Syntax: SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE 'file_name' [CHARACTER SET charset_name] export_options | INTO DUMPFILE 'file_name' | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] SELECT is used to retrieve rows selected from one or more tables, and can include UNION statements and subqueries. See [HELP UNION], and https://dev.mysql.com/doc/refman/5.7/en/subqueries.html. The most commonly used clauses of SELECT statements are these: o Each select_expr indicates a column that you want to retrieve. There must be at least one select_expr. o table_references indicates the table or tables from which to retrieve rows. Its syntax is described in [HELP JOIN]. o SELECT supports explicit partition selection using the PARTITION with a list of partitions or subpartitions (or both) following the name of the table in a table_reference (see [HELP JOIN]). In this case, rows are selected only from the partitions listed, and any other partitions of the table are ignored. For more information and examples, see https://dev.mysql.com/doc/refman/5.7/en/partitioning-selection.html. SELECT ... PARTITION from tables using storage engines such as MyISAM that perform table-level locks (and thus partition locks) lock only the partitions or subpartitions named by the PARTITION option. For more information, see https://dev.mysql.com/doc/refman/5.7/en/partitioning-limitations-lock ing.html. o The WHERE clause, if given, indicates the condition or conditions that rows must satisfy to be selected. where_condition is an expression that evaluates to true for each row to be selected. The statement selects all rows if there is no WHERE clause. In the WHERE expression, you can us","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 用户创建 # 创建用户（默认密码为空） mysql\u003e create user 'username'@'host'; # 创建用户并设置密码 mysql\u003e create user 'username'@'host' identified by 'password'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"3. 删除用户 mysql\u003e drop user 'username'@'host'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:3","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"4. 更改密码 # 更改密码 （只对当前登录账号有效） mysql\u003e set password=password('123456'); # 2. 更改指定用户的密码 mysql\u003e set password for 'username'@'host'=password('123456'); ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:4","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"5. 查询用户权限 # 查询当前账号的权限 mysql\u003e show grants; # 查询指定账号的权限 mysql\u003e show grants for 'user'@'host'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:5","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"6. 用户授权 # 对用户授权（如果用户存在就增加权限，不存在就创建用户不过密码为空） mysql\u003e grant privileges on databasename.tablename to 'username'@'host'; # 对用户授权并设置密码（如果用户存在就增加权限，不存在就创建用户） # mysql 8.0 版本以后需要先创建用户在授权 mysql\u003e grant privileges on databasename.tablename -\u003e to 'username'@'host' identified by 'password'; privileges: 权限列表以逗号隔开，例如： select, insert, update 注意: 进行数据库基本信息相关更改后请使用 flush privileges; 刷新数据库信息 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:6","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"7. 用户权限回收 mysql\u003e revoke privilege on databasename.tablename from 'user'@'host'; 注：数据库名要用反撇号引起，或者不用 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:7","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"二、数据库 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 数据库的基本操作 # 显示数据库 mysql\u003e show databases; # 创建数据库 mysql\u003e create database DATABASENAME charset utf8mb4;; # 查看数据库创建语句 mysql\u003e show create database DATABASENAME; # 删除数据库 mysql\u003e drop database DATABASENAME; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 备份数据库数据及表结构 # 备份整个数据库 [root@localhost ~]# mysqldump -uroot -p -A \u003e all.sql # 备份整个数据库的结构 [root@localhost ~]# mysqldump -uroot -p -A -d \u003e all.sql # 备份单个数据库 [root@localhost ~]# mysqldump -uroot -p DATABASENAME \u003e DATABASENAME.sql # 一次备份多个数据库, 同时备份 db1, db2 二个库的数据 (-B, --databases) [root@localhost ~]# mysqldump -uroot -p --databases db1 db2 \u003e dbs.sql # 备份数据库中指定的表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME TABLENAME \u003e DATABASENAME_TABLENAME.sql # 一次备份数据库中指定的多张表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME t1 t2 \u003e DATABASENAME_ts.sql -B, --databases: 单库备份可以加上 -B 参数，这样备份文件中加会加入 create database ... 及 use DATABASE 语句. -A, --all-databases : 备份所有数据库 -d, --no-data ：只导出表结构 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"3. 导出函数或者存储过程 mysqldump -hHOSTNAME -uUSERNAME -pPASSWORD -ntd -R DATABASENAME \u003e DATABASENAME.sql -ntd 是表示导出存储过程； -R 是表示导出函数 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:3","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"4. 恢复数据库数据 ** 使用系统命令** [root@localhost ~]# mysql -uroot DATABASENAME \u003c DATABASENAME.sql 使用 source 命令 # 禁止记录 binlog 日志，恢复数据就没必要记录 binlog 了 mysql\u003e set sql_log_bin=0 mysql\u003e use lwg; mysql\u003e source /root/lwg.sql; 注意: 恢复数据时，如果数据库不存在需要先创建 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:4","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"三、数据表 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 表的基本操作 # 查看数据库下所有的表 mysql\u003e show tables; # 创建表 mysql\u003e CREATE TABLE `TABLENAME` ( `id` int(10) NOT NULL PRIMARY KEY AUTO_INCREMENT, `user` varchar(30) NOT NULL, `password` varchar(30) NOT NULL ) ENGINE=MyISAM DEFAULT CHARSET=utf8; # 显示表结构 mysql\u003e desc TABLENAME; # 显示表创建语句 mysql\u003e show create table TABLENAME; # 清空表数据 mysql\u003e truncate table TABLENAME; mysql\u003e delete from TABLENAME; 不带 where 参数的 delete 语句可以删除 mysql 表中所有内容 使用 truncate table 也可以清空 mysql 表中所有内容。 效率上 truncate 比 delete 快，但 truncate 删除后不记录 mysql 日志，不可以恢复数据。 delete 的效果有点像将 mysql 表中所有记录一条一条删除到删完， 而 truncate 相当于保留 mysql 表的结构，重新创建了这个表，所有的状态都相当于新表。 所以 delete 不会重置 ID 列，而 truncat 会重置。 delete 删除是逻辑上的删除，并不会真正的释放硬盘空间，而 truncat 是物理上的删除操作会真正的释放硬盘空间 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 表 alter 的相关操作 # 增加一个字段(一列),并放到第一列的位置 (first) mysql\u003e desc users; +------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+----------+------+-----+---------+-------+ | username | char(30) | NO | PRI | NULL | | | userpasswd | char(20) | NO | | 123456 | | +------------+----------+------+-----+---------+-------+ 2 rows in set (0.00 sec) mysql\u003e alter table users add column id int not null first; Query OK, 0 rows affected (0.08 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+----------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | char(30) | NO | PRI | NULL | | | userpasswd | char(20) | NO | | 123456 | | +------------+----------+------+-----+---------+-------+ 3 rows in set (0.00 sec) # 删除一个字段 mysql\u003e alter table users drop userpasswd; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +----------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+----------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | char(30) | NO | PRI | NULL | | +----------+----------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 更改列的字段类型 mysql\u003e alter table users modify username varchar(100); Query OK, 2 rows affected (0.14 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +----------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+--------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | varchar(100) | NO | PRI | | | +----------+--------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 更改列名及字段类型 mysql\u003e alter table users change username user varchar(20); Query OK, 2 rows affected (0.03 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | user | varchar(20) | NO | PRI | | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 修改表的存储引擎 mysql\u003e show create table users; +-------+---------------------------------------+ | Table | Create Table | +-------+---------------------------------------+ | users | CREATE TABLE `users` ( `id` int(11) NOT NULL, `user` varchar(20) NOT NULL DEFAULT '', PRIMARY KEY (`user`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +-------+---------------------------------------+ 1 row in set (0.00 sec) mysql\u003e alter table users ENGINE=myisam; Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 # 这里我们使用另一种方法查询表的默认引擎 mysql\u003e show table status from lwg where name='users'\\G *************************** 1. row *************************** Name: users Engine: MyISAM Version: 10 Row_format: Dynamic Rows: 2 Avg_row_length: 20 Data_length: 40 Max_data_length: 281474976710655 Index_length: 2048 Data_free: 0 Auto_increment: NULL Create_time: 2017-08-25 04:15:46 Update_time: 2017-08-25 04:15:46 Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec) ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"当数据库服务器资源有剩余时，为了充分利用剩余资源可以通过部署 MySQL 多实例提升资源利用率， 下面演示如何在一台机上安装 MySQL 多实例 ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:0:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"下载 MySQL 5.7 二进制包 [root@10-13-90-34 src]# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:1:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"解压并建立软链接(/usr/local/mysql) [root@10-13-90-34 src]# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ [root@10-13-90-34 local]# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:2:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"配置环境变量 echo 'export PATH=/urs/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh source /etc/profile ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:3:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"准备多实例环境 创建用户 [root@10-13-90-34 mysql]# useradd -r -s /sbin/nologin mysql 创建数据目录 [root@10-13-90-34 mysql]# mkdir -p /data/mysql/{3306,3307} [root@10-13-90-34 mysql]# chown -R mysql.mysql /data/mysql 准备多实例配置文件 实例1：3307 cat \u003e /usr/local/mysql/etc/my-3307.cnf \u003c\u003cEOF [client] port = 3307 socket = /data/mysql/3307/mysql.sock [mysqld] user = mysql port = 3307 basedir = /usr/local/mysql datadir = /data/mysql/3307 socket = /data/mysql/3307/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/3307/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/3307/mybinlog #sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 EOF 实例2：3308 cat \u003e /usr/local/mysql/etc/my-3308.cnf \u003c\u003cEOF [client] port = 3308 socket = /data/mysql/3307/mysql.sock [mysqld] user = mysql port = 3308 basedir = /usr/local/mysql datadir = /data/mysql/3308 socket = /data/mysql/3308/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/3308/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/3308/mybinlog #sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 EOF ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:4:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"多实例初始化 [root@10-13-90-34 3307]# mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3307 [root@10-13-90-34 3307]# mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3308 --defaults-file= 参数必须放在最前面或者初始化不会成功 ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:5:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"使用 systemd 管理多实例服务 3307 cat \u003e /usr/lib/systemd/system/mysqld-3307.service \u003c\u003c EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf LimitNOFILE = 5000 EOF 3308 cat \u003e /usr/lib/systemd/system/mysqld-3308.service \u003c\u003c EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf LimitNOFILE = 5000 EOF ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:6:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"启动 MySQL 多实例 [root@10-13-90-34 ~]# systemctl start mysqld-3307 [root@10-13-90-34 ~]# systemctl start mysqld-3308 [root@10-13-90-34 ~]# systemctl status mysqld-3307.service ● mysqld-3307.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld-3307.service; disabled; vendor preset: disabled) Active: active (running) since 四 2020-12-24 13:44:18 CST; 14s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 40145 (mysqld) CGroup: /system.slice/mysqld-3307.service └─40145 /usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf 12月 24 13:44:18 10-13-90-34 systemd[1]: Started MySQL Server. [root@10-13-90-34 ~]# systemctl status mysqld-3308.service ● mysqld-3308.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld-3308.service; disabled; vendor preset: disabled) Active: active (running) since 四 2020-12-24 13:44:20 CST; 16s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 40179 (mysqld) CGroup: /system.slice/mysqld-3308.service └─40179 /usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf 12月 24 13:44:20 10-13-90-34 systemd[1]: Started MySQL Server. ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:7:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"连接 MySQL 多实例 [root@10-13-90-34 ~]# mysql -S /data/mysql/3307/mysql.sock ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:8:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"下载 MySQL 5.7 二进制包 [root@10-13-90-34 src]# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz ","date":"2020-12-20","objectID":"/posts/mysql-install/:1:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"解压并建立软链接(/usr/local/mysql) [root@10-13-90-34 src]# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ [root@10-13-90-34 local]# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2020-12-20","objectID":"/posts/mysql-install/:2:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"配置环境变量 echo 'export PATH=/usr/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh source /etc/profile ","date":"2020-12-20","objectID":"/posts/mysql-install/:3:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"初始化前准备工作 # 安装依赖 [root@10-13-90-34 mysql]# yum install libaio # 创建 mysql 用户 [root@10-13-90-34 mysql]# useradd -r -s /sbin/nologin mysql # 创建数据存储目录 [root@10-13-90-34 mysql]# mkdir -p /data/mysql [root@10-13-90-34 mysql]# chown -R mysql.mysql /data/mysql/ # 生成配置文件 my.cnf [root@10-13-90-34 mysql]# mkdir etc [root@10-13-90-34 mysql]# vim etc/my.cnf [client] port = 3306 socket = /data/mysql/3306/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 ","date":"2020-12-20","objectID":"/posts/mysql-install/:4:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"初始化数据库 初始化参数 --initialize # 初始化时会提供12位的 root 临时密码，使用mysql前必须重置此密码，密码管理使用严格模式。 --initialize-insecure # 不会为 root 用户生成临时密码 [root@10-13-90-34 mysql]# mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql ","date":"2020-12-20","objectID":"/posts/mysql-install/:5:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"管理 mysql 服务 使用自带脚本 MySQL 默认提供服务管理脚本 support-files/mysql.server 使用方法 [root@10-13-90-34 mysql]# cp support-files/mysql.server /etc/init.d/mysqld [root@10-13-90-34 mysql]# /etc/init.d/mysqld start 使用 systemd 管理 MySQL 服务 [root@10-13-90-34 mysql]# vim /usr/lib/systemd/system/mysqld.service [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my.cnf LimitNOFILE = 5000 以上方法二选一即可 ","date":"2020-12-20","objectID":"/posts/mysql-install/:6:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"扩展部署多实例 MySQL 方法1: 多份 MySQL 程序，不同的配置文件，不同的数据存储目录 方法2: 一份 MySQL 程序，不同的配置文件，不同的数据存储目录 （推荐） 实现方法 在 MySQL 服务启动命令 mysqld 使用参数（–defaults-file）指定默认使用的配置文件(my.cnf)即可实现，数据存储目录在配置文件中配置. 查看 mysqld 参数方法: mysqld --verbose --help ","date":"2020-12-20","objectID":"/posts/mysql-install/:7:0","tags":["mysql"],"title":"安装 MySQL 5.7","uri":"/posts/mysql-install/"},{"categories":null,"content":" 冬夜读书示子聿 -宋.陆游 古人学问无遗力，少壮工夫老始成。 纸上得来终觉浅，绝知此事要躬行。 有勇气去改变可以改变的事情；有胸怀去接纳不可以改变的事情；用智慧去区分两者的不同。 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"}]