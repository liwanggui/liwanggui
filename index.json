[{"categories":["linux"],"content":"fpm 简介 fpm 的目标是使得构建二进制包 (deb, rpm, osx 等) 变得简单快速 fpm 项目地址: https://github.com/jordansissel/fpm fpm 文档地址: https://fpm.readthedocs.io/en/latest/ ","date":"2021-07-03","objectID":"/posts/fpm/:1:0","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"fpm 依赖 fpm 使用 Ruby 开发, 所以你得先安装 Ruby. 有些系统中默认已经安装了 Ruby, 例如: OSX, 有些系统可能没有安装 Ruby, 此时你需要执行下命令进行安装: OSX/macOS: brew install gnu-tar brew install rpm Red Hat systems (Fedora 22 or older, CentOS, etc): yum install ruby-devel gcc make rpm-build rubygems 注意: CentOS 源中的 ruby 版本过低，需要手动源码编译安装较新的 Ruby 版本 编译安装 Ruby yum install gcc openssl-devel make wget https://cache.ruby-lang.org/pub/ruby/2.7/ruby-2.7.3.tar.gz tar xzf ruby-2.7.3.tar.gz cd ruby-2.7.3 ./configure --prefix=/usr/local/ruby make make install 配置环境变量 echo 'export PATH=/usr/local/ruby/bin:$PATH' \u003e /etc/profile.d/ruby.sh source /etc/profile 配置 Ruby 源 gem sources -l # 查看当前源 gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/ Fedora 23 or newer: dnf install ruby-devel gcc make rpm-build libffi-devel Oracle Linux 7.x systems: yum-config-manager --enable ol7_optional_latest yum install ruby-devel gcc make rpm-build rubygems Debian-derived systems (Debian, Ubuntu, etc): apt-get install ruby ruby-dev rubygems build-essential ","date":"2021-07-03","objectID":"/posts/fpm/:2:0","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"安装 fpm 可以使用 gem 工具安装 fpm gem install --no-document fpm 检查是否安装 fpm --version 常用参数说明 -s 指定源类型 -t 指定目标类型，即想要制作为什么包 -n 指定包的名字 -v 指定包的版本号 -C 在搜索文件之前将目录更改为此处 -d 指定依赖于哪些包 -a 架构名称，通常匹配 'uname -m', 可以使用 '-a all' 或者 '-a native' -f 第二次打包时目录下如果有同名安装包存在，则覆盖它 -p 输出的安装包的目录，不想放在当前目录下就需要指定 --iteration 指定包的发布次数，例 RPM 的 release 字段 --post-install 软件包安装完成之后所要运行的脚本；同 --after-install --pre-install 软件包安装完成之前所要运行的脚本；同 --before-install --post-uninstall 软件包卸载完成之后所要运行的脚本；同 --after-remove --pre-uninstall 软件包卸载完成之前所要运行的脚本；同 --before-remove ","date":"2021-07-03","objectID":"/posts/fpm/:3:0","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"使用示例 以 nodejs 为例， 将 nodejs 构建成3个包: nodejs, nodejs-dev, nodejs-doc 在示例中需要我们在 make install 时设置 DESTDIR 将编译好的文件安装到特定的目录中 ","date":"2021-07-03","objectID":"/posts/fpm/:4:0","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"制作 nodejs 包 正常编译步骤 % wget http://nodejs.org/dist/v0.6.0/node-v0.6.0.tar.gz % tar -zxf node-v0.6.0.tar.gz % cd node-v0.6.0 % ./configure --prefix=/usr % make 将 nodejs 安装至临时目录 % mkdir /tmp/installdir % make install DESTDIR=/tmp/installdir 制作 nodejs 包 # Create a nodejs deb with only bin and lib directories: # The 'VERSION' and 'ARCH' strings are automatically filled in for you # based on the other arguments given. % fpm -s dir -t deb -n nodejs -v 0.6.0 -C /tmp/installdir \\ -p nodejs_VERSION_ARCH.deb \\ -d \"libssl0.9.8 \u003e 0\" \\ -d \"libstdc++6 \u003e= 4.4.3\" \\ usr/bin usr/lib nodejs 包中只包含 usr/bin usr/lib 中的文件，此为 nodejs 基础运行包 安装 nodejs 包，测试一下 # 'fpm' just produced us a nodejs deb: % file nodejs_0.6.0-1_amd64.deb nodejs_0.6.0-1_amd64.deb: Debian binary package (format 2.0) % sudo dpkg -i nodejs_0.6.0-1_amd64.deb % /usr/bin/node --version v0.6.0 ","date":"2021-07-03","objectID":"/posts/fpm/:4:1","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"制作 nodejs-doc 包 创建 nodejs 文档手册包 # Create a package of the node manpage % fpm -s dir -t deb -p nodejs-doc_VERSION_ARCH.deb -n nodejs-doc -v 0.6.0 -C /tmp/installdir usr/share/man 查看 nodejs-doc 包 % dpkg -c nodejs-doc_0.6.0-1_amd64.deb | grep node.1 -rw-r--r-- root/root 945 2011-01-02 18:35 usr/share/man/man1/node.1 ","date":"2021-07-03","objectID":"/posts/fpm/:4:2","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"制作 nodejs-dev 包 最后，打包用于开发的 headers 文件: % fpm -s dir -t deb -p nodejs-dev_VERSION_ARCH.deb -n nodejs-dev -v 0.6.0 -C /tmp/installdir usr/include % dpkg -c nodejs-dev_0.6.0-1_amd64.deb | grep -F .h -rw-r--r-- root/root 14359 2011-01-02 18:33 usr/include/node/eio.h -rw-r--r-- root/root 1118 2011-01-02 18:33 usr/include/node/node_version.h -rw-r--r-- root/root 25318 2011-01-02 18:33 usr/include/node/ev.h ... ","date":"2021-07-03","objectID":"/posts/fpm/:4:3","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["linux"],"content":"注意事项 当我们需要将某个目录制作成二进制包时，需要注意 “相对路径” 与 “绝对路径” 问题，以 nginx 为例 相对路径 % cd /usr/local/nginx % fpm -s dir -t rpm -n nginx -v 1.16.1 --iteration 1.el7 . no value for epoch is set, defaulting to nil {:level=\u003e:warn} no value for epoch is set, defaulting to nil {:level=\u003e:warn} Created package {:path=\u003e\"nginx-1.16.1-1.el7.x86_64.rpm\"} # 查看 rpm 包文件列表 $ rpm -qpl nginx-1.16.1-1.el7.x86_64.rpm /client_body_temp /conf/extra/dynamic_pools /conf/extra/static_pools ... 绝对路径 $ fpm -s dir -t rpm -n nginx -v 1.16.1 --iteration 2.el7 /usr/local/nginx no value for epoch is set, defaulting to nil {:level=\u003e:warn} no value for epoch is set, defaulting to nil {:level=\u003e:warn} Created package {:path=\u003e\"nginx-1.16.1-2.el7.x86_64.rpm\"} # 查看 rpm 包文件列表 $ rpm -qpl nginx-1.16.1-2.el7.x86_64.rpm /usr/local/nginx/client_body_temp /usr/local/nginx/conf/extra/dynamic_pools /usr/local/nginx/conf/extra/static_pools /usr/local/nginx/conf/fastcgi.conf /usr/local/nginx/conf/fastcgi.conf.default ... 更多使用帮助请查看 fpm 官方文档 https://fpm.readthedocs.io/en/latest/ ","date":"2021-07-03","objectID":"/posts/fpm/:5:0","tags":["fpm"],"title":"fpm - 简单的包制作工具","uri":"/posts/fpm/"},{"categories":["nginx"],"content":"使用 logrotate 管理 nginx 日志 随着时间的推移 nginx 的日志会越来越大，为了减少 nginx 日志的体积大小，使用 logrotate 工具每天对 nginx 日志进行切割处理 nginx logrotate 配置文件: /etc/logrotate.d/nginx /usr/local/nginx/logs/*.log { daily missingok rotate 30 compress dateext delaycompress notifempty sharedscripts postrotate if [ -f /usr/local/nginx/logs/nginx.pid ]; then kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` fi endscript } 更多 logrotate 配置参数请参考 man 手册 man logrotate ","date":"2021-06-27","objectID":"/posts/nginx-logrotate/:1:0","tags":["nginx"],"title":"Nginx 日志自动切割","uri":"/posts/nginx-logrotate/"},{"categories":["cli"],"content":"简介 find 是实时查找工具，通过遍历指定路径完成文件查找 工作特点： 查找速度略慢 精确查找 实时查找 查找条件丰富 只搜索用户具备读取和执行权限的目录 语法格式: find [Option]... [查找路径 [查找条件] [处理动作] 查找路径: 指定具体查找目标路径，不指定时默认为当前目录 查找条件：指定的查找标准，可以是文件名，大小，类型，权限等；默认为找出指定路径下的所有文件 处理动作：对符合条件的文件做操作，默认输出至屏幕 ","date":"2021-06-27","objectID":"/posts/find/:1:0","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"常用参数 ","date":"2021-06-27","objectID":"/posts/find/:2:0","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"指定搜索目录层级 -maxdepth: 最大搜索目录深度，指定的目录下的文件为第1级 -mindepth: 最小搜索目录深度 -depth: 先处理文件再处理目录，默认为是先处理目录后处理文件 ","date":"2021-06-27","objectID":"/posts/find/:2:1","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据文件名和 inode 查找 -name: 指定搜索的文件名，支持使用通配符，如: *, ?, [], [^] 等，使用通配符需使用引号引起来 -iname: 指定搜索的文件名，不区分大小写 -inum: 指定 inode 号，通过 inode 号查找文件 -samefile: 指定文件名，查找 inode 号相同的文件 -links: 链接数，查找文件链接数为指定链接数的文件 -regex: 正则表达式，使用正则表达式匹配整个文件路径，而非文件名称 ","date":"2021-06-27","objectID":"/posts/find/:2:2","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据属主、属组查找 -user：用户名，根据文件属主查找文件 -group: 组名, 根据文件属组查找文件 -uid: 用户id, 根据用户ID(UID)查找文件 -gid: 组id, 根据组ID(GID)查找文件 -nouser: 查找没属主的文件 ","date":"2021-06-27","objectID":"/posts/find/:2:3","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据文件类型查找 -type TYPE f: 普通文件 d: 目录文件 l: 符号链接文件 s: 套接字文件 b: 块文件 c: 字符设备文件 ","date":"2021-06-27","objectID":"/posts/find/:2:4","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"查找空文件或空目录 查找空文件与空目录 find --empty 查找空文件 find -empty -type f 提示: 空文件是指大小 0 的文件 ","date":"2021-06-27","objectID":"/posts/find/:2:5","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"组合条件 与: -a 或: -o 非: -not ! 查找以 .log 或 .txt 结尾的文件 find -name \"*.log\" -o -name \"*.txt\" 查找不是符号链接的文件 find ! -type l ","date":"2021-06-27","objectID":"/posts/find/:2:6","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"排除目录 -prune: 排除查找的结果 搜索 /etc 目录下所有 .conf 结尾的文件，排除 /etc/fonts 和 /etc/systemd 目录 find /etc \\( -path /etc/fonts -o -path /etc/systemd \\) -a -prune -o -name '*.conf' ","date":"2021-06-27","objectID":"/posts/find/:2:7","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据文件大小查找文件 -size: [+/-], 常用单位: k M G c(byte) 查找2k大小的文件 find -size 2k 注意: 查找的大小并不精确，查找目标是 2k 大小，实际查找是大于 1k 小于 2k 的文件 如果想精确查找 2k 大小的文件可以写成， find -size 2048c 查找大于 100M 的文件 find -size +100M 注意： 大于 100M 并不包括 100M 查找小于 10k 的文件 find -size -10k 注意: 查找的文件大小是 0-9k ","date":"2021-06-27","objectID":"/posts/find/:2:8","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据时间查找文件 **以天为单位 ** -actim: 文件访问时间（天） -mtime: 文件修改时间（天） -ctime: 状态更新时间（天） 时间范围说明 +10 表示 11 天之前的 -10 表示 10 天以内，不包括第10天 以分钟为单位 -amin: 文件访问时间（分钟） -mmin: 文件修改时间（分钟） -cmin: 文件状态更新时间（分钟） 查找 3 天前的文件 find -type f -mtime +3 注意: 不包含第3天 查看 3 天内的文件 find -type f -mtime 3 ","date":"2021-06-27","objectID":"/posts/find/:2:9","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"根据权限查找文件 查找权限为 777 的文件 find -type f -perm 777 查找有读取权限的文件 find -type f -perm /444 属主、属组、其他，只要其中有任一有读取权限就可以 注意: 匹配权限时，不加 /- 号表示精确查找，/ 号表示权限的或，- 号表示与 示例 /444 : 表示查找有读取权限的文件（不管是属主、属组或者其他权限的读取权限） -444 : 表示查找 属主、属组及其他 权限位都有读取权限的文件 ","date":"2021-06-27","objectID":"/posts/find/:2:10","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["cli"],"content":"查找文件后的动作 -print: 默认的处理动作，输出至屏幕 -ls： 类似于对查找的文件执行 ls -l 命令 -delete: 删除查找到的文件 -fls: 将查找到的所有文件的长格式信息保存到指定的文件 -ok：对查找到的每个文件执行指定的命令，对于每个文件执行命令之前，都会交互式要求用户确认 -exec: 对查找到的文件执行指定的命令 {}: 用于引用查找到的文件名称自身 由于分号有 shell 中有特殊的含义所以得转义 \\; 查找 /home 中所有目录 find /home -type d -ls 去除 /home 目录下所有文件的执行权限 find /home -type f -exec chmod -x {} \\; 查找删除 3天以前的文件 find -type f -ctime +3 -ok rm {} \\; ","date":"2021-06-27","objectID":"/posts/find/:2:11","tags":["find"],"title":"利用 find 查找文件","uri":"/posts/find/"},{"categories":["jenkins"],"content":"概述 共享库这并不是一个全新的概念，其实具有编程能力的同学应该清楚一些。例如在编程语言 Python 中，我们可以将 Python 代码写到一个文件中，当代码数量增加，我们可以将代码打包成模块然后再以 import 的方式使用此模块中的方法。 在 Jenkins 中使用 Groovy 语法，共享库中存储的每个文件都是一个 Groovy 的类，每个文件（类）中包含一个或多个方法。每个方法包含 Groovy 语句块。 Jenkins 共享参考库: https://github.com/liwanggui/jenkins-share-lib.git ","date":"2021-06-17","objectID":"/posts/jenkins-sharelib/:1:0","tags":["jenkins","groovy"],"title":"Jenkins 共享库应用","uri":"/posts/jenkins-sharelib/"},{"categories":["jenkins"],"content":"共享库内容 共享参考库文件结构如下 ── vars │ └── getIP.groovy │ └── hello.groovy ├── src │ └── org │ └── devops │ └── HTTP.groovy ├── Jenkinsfile └── README.md src 目录主要存放我们要编写的 Groovy 类，执行流水线时，此目录将添加到 class_path 中。 vars目录主要存放脚本文件，这些脚本文件在流水线中作为变量公开。 resources 目录允许从外部库中使用步骤来加载相关联的非 Groovy 文件。 ","date":"2021-06-17","objectID":"/posts/jenkins-sharelib/:2:0","tags":["jenkins","groovy"],"title":"Jenkins 共享库应用","uri":"/posts/jenkins-sharelib/"},{"categories":["jenkins"],"content":"创建共享库 文件 src/org/devops/HTTP.groovy, 在此我将这个文件定义为 HTTP 请求类，主要放一些 HTTP 请求方法。 package org.devops import groovy.json.JsonOutput /** * 发送 HTTP GET 请求 * @param url 请求的网址 * @return String */ def get(url){ return new URL(url).text } /** * 发送 HTTP POST 请求 * @param url 请求的网址 * @param data 请求所需的参数，可选 * @param is_json 请求参数类型是否为 json 格式 * @return String */ def post(url, data = null, is_json = false) { def conn = new URL(url).openConnection() conn.setRequestMethod(\"POST\") if (data) { if (is_json) { conn.setRequestProperty(\"Content-Type\", \"application/json\") data = JsonOutput.toJson(data) } // 输出请求参数 println(data) conn.doOutput = true def writer = new OutputStreamWriter(conn.outputStream) writer.write(data) writer.flush() writer.close() } def result = conn.content.text // 输出请求结果 // result.each({ println it }) return result } ","date":"2021-06-17","objectID":"/posts/jenkins-sharelib/:3:0","tags":["jenkins","groovy"],"title":"Jenkins 共享库应用","uri":"/posts/jenkins-sharelib/"},{"categories":["jenkins"],"content":"使用共享库 我们打开 Jenkins 管理页面，依次点击 Manage Jenkins -\u003e System Configuration -\u003e Global Pipeline Libraries 首先，我们为共享库设置一个名称 jenkinslib，注意这个名称后续在 Jenkinsfile 中引用。 再设置一个默认的版本，这里的版本是分支的名称。我默认配置的是 main (github 将 master 改为了 main) 版本。 好，到此共享库在 Jenkins 的配置就完成了，接下来测试在 Jenkinsfile 中引用。 在 Jenkinsfile 中使用 @Library('jenkinslib') _ 来加载共享库，注意后面符号 _ 用于加载。 类的实例化 def http = new org.devops.HTTP(), 使用类中的方法 http.get(\"https://httpbin.org/ip\")。 @Library('jenkinslib') _ import org.devops.HTTP // 创建 HTTP 类实例 def http = new HTTP() pipeline { agent any stages { stage(\"发送 POST 请求\") { steps { println http.post(\"https://httpbin.org/post\") } } stage(\"获取主机公网 IP\") { steps { println getIP() } } } } 接下来在你的 Jenkins 上面运行一下吧 ","date":"2021-06-17","objectID":"/posts/jenkins-sharelib/:4:0","tags":["jenkins","groovy"],"title":"Jenkins 共享库应用","uri":"/posts/jenkins-sharelib/"},{"categories":["groovy"],"content":"GET 请求 使用 Groovy 发送 GET 请求非常简单，一行代码搞定 def res1 = new URL('https://httpbin.org/ip').text // or def res2 = 'https://httpbin.org/ip'.toURL().text ","date":"2021-06-17","objectID":"/posts/groovy-http/:1:0","tags":["groovy"],"title":"Groovy 发送 HTTP 请求","uri":"/posts/groovy-http/"},{"categories":["groovy"],"content":"POST 请求 使用标准库 URL 类，发送 POST 请求 import groovy.json.JsonOutput import groovy.json.JsonSlurper /** * 发送 HTTP POST 请求 * @param url 请求的网址 * @param data 请求所需的参数，可选 * @param is_json 请求参数类型是否为 json 格式 * @return Map */ def http_post(url, data = null, is_json = false) { def conn = new URL(url).openConnection() conn.setRequestMethod(\"POST\") if (data) { if (is_json) { conn.setRequestProperty(\"Content-Type\", \"application/json\") data = JsonOutput.toJson(data) } // 输出请求参数 println(data) conn.doOutput = true def writer = new OutputStreamWriter(conn.outputStream) writer.write(data) writer.flush() writer.close() } def json = new JsonSlurper() json.parseText(conn.content.text) } http_post('https://httpbin.org/post', '{\"name\": \"John\", \"age\": 34}', true) ","date":"2021-06-17","objectID":"/posts/groovy-http/:2:0","tags":["groovy"],"title":"Groovy 发送 HTTP 请求","uri":"/posts/groovy-http/"},{"categories":["python"],"content":"企业微信 API 通过 python 调用企业微信的 api 接口来发送消息，可用于监控告警。使用 requests 模块。 #!/usr/bin/python # -*- coding: utf-8 -*- # # pip install requests # import os import time import redis import requests class WXWork(object): def __init__(self, corpid, secret, agentid): self.token_file = '/tmp/temp_wechat' self.url = \"https://qyapi.weixin.qq.com/cgi-bin\" self.corpid = corpid self.corpsecret = secret self.agentid = agentid def _get_token(self): # 获取 token 并缓存 response = requests.get(url=self.url + '/gettoken', params=dict(corpid=self.corpid, corpsecret=self.corpsecret)) return response.json() def get_token(self): if os.path.isfile(self.token_file): with open(self.token_file) as f: token_info = f.read() if len(token_info.split()) == 2: expire, token = token_info.split() if float(expire) \u003e time.time(): return token d = self._get_token() try: if d['errcode'] == 0: with open(self.token_file, 'w') as f: f.write(\"%s%s\" % (time.time() + d['expires_in'], d['access_token'])) return d['access_token'] except Exception as e: return False def send(self, msg): token = self.get_token() if token: url = self.url + '/message/send?access_token=%s' % token data = dict( toparty=\"1\", msgtype=\"text\", agentid=self.agentid, text=dict(content=msg), safe=0 ) response = requests.post(url=url, json=data) d = response.json() if d[\"errcode\"] != 0: return 'Send message failed.' else: return 'Get token failed.' if __name__ == '__main__': # 企业ID corpid = \"xxxxxx\" # 应用的凭证密钥 secret = \"xxxxxxxxxxxxxxxxxx\" # 企业应用的id，整型。可在应用的设置页面查看 agentid = 1000002 # 发送的消息 msg = \"这是只个无聊的消息。\" wechat = WXWork(corpid, secret, agentid) wechat.send(msg) ","date":"2021-06-14","objectID":"/posts/python-wechat/:1:0","tags":["requests","wechat"],"title":"Python 调用企业微信发送消息","uri":"/posts/python-wechat/"},{"categories":["python"],"content":" Paramiko Github 仓库: https://github.com/paramiko/paramiko Paramiko 扩展模块 scp.py Github 仓库: https://github.com/jbardin/scp.py ","date":"2021-06-14","objectID":"/posts/python-paramiko/:0:0","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"安装 paramiko pip install paramiko ","date":"2021-06-14","objectID":"/posts/python-paramiko/:1:0","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"SSH 连接 ","date":"2021-06-14","objectID":"/posts/python-paramiko/:2:0","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"用户名密码 import paramiko client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.AutoAddPolicy) client.connect(hostname='192.168.31.100', port=22, username='root', password='123456') stdin, stdout, stderr = client.exec_command('ls') for line in stdout: print('... ' + line.strip('\\n')) client.close() ","date":"2021-06-14","objectID":"/posts/python-paramiko/:2:1","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"使用私钥 import paramiko client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.AutoAddPolicy) client.connect(hostname='192.168.31.100', port=22, username='root', key_filename=\"\u003c你的私钥路径\u003e\", passphrase=\"\u003c私钥密码\u003e\") # pkey = paramiko.RSAKey(data=None, filename='\u003c你的私钥路径\u003e', password='\u003c私钥密码\u003e') # client.connect('118.193.40.147', username='root', pkey=pkey) stdin, stdout, stderr = client.exec_command('ls') for line in stdout: print('... ' + line.strip('\\n')) client.close() ","date":"2021-06-14","objectID":"/posts/python-paramiko/:2:2","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"使用代理 paramiko 实现网关代理连接功能 由于 paramiko 在 windows 下不能使用 ssh 代理连接远程主机，经过苦苦寻找，终于在 fabirc 源代码找到解决方案（fabric 命令有一个选项 gateway 允许用户指定一台网关机，然后所有主机连接都会经过这台网关主机中转连接），代码如下： import paramiko gateway = paramiko.SSHClient() gateway.set_missing_host_key_policy(paramiko.MissingHostKeyPolicy()) gateway.connect(hostname=\"192.168.92.131\", port=22, username=\"root\", password='liwanggui', timeout=5) # 关键就这一步了 sock = gateway.get_transport().open_channel('direct-tcpip', ('192.168.22.2', int(22)), ('', 0)) ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.MissingHostKeyPolicy()) ssh.connect(hostname=\"192.168.22.2\", port=22, username=\"root\", password='liwanggui', sock=sock, timeout=5) session = ssh.get_transport().open_session() session.exec_command('uptime') exit_status = session.recv_exit_status() stdout = session.makefile('r').read() stderr = session.makefile_stderr('r').read() print(exit_status, stdout, stderr) ssh.close() gateway.close() 通过 ProxyCommand 实现 此方法仅适用于类 unix 系统, 代码如下： import paramiko import time private_key_file = \"/root/.ssh/id_rsa\" proxy_command = r\"ssh -i /root/.ssh/id_rsa -p 22 -o StrictHostKeyChecking=no root@192.168.92.131 -W 192.168.22.2:22\" sock = paramiko.proxy.ProxyCommand(proxy_command) ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.MissingHostKeyPolicy()) ssh.connect(hostname=\"192.168.22.2\", port=22, username=\"root\", key_filename=private_key_file, sock=sock, timeout=5) chan = ssh.get_transport().open_session() chan.exec_command('uptime') # 命令执行退出状态 0 成功， 其他数字 失败 exit_status = chan.recv_exit_status() # 标准输出结果 stdout = chan.makefile('r').read() # 标准错误输出结果 stderr = chan.makefile_stderr('r').read() print(exit_status, stdout, stderr) 通过 socks 代理实现 通过 PySocks 模块实现 socks5 代理功能，在 linux 平台使用没有问题，在 windows 中出错，代码如下： import socks import socket import paramiko socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, '127.0.0.1', 1080) socket.socket = socks.socksocket ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) try: ssh.connect('192.168.22.2', 22, 'root', 'liwanggui', timeout=5) except paramiko.AuthenticationException: print('password is error.') chan = ssh.get_transport().open_session() chan.exec_command('uptime') exit_status = chan.recv_exit_status() stdout = chan.makefile('r').read() stderr = chan.makefile_stderr('r').read() print(exit_status, stdout, stderr) ","date":"2021-06-14","objectID":"/posts/python-paramiko/:2:3","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"文件操作 这我们使用 paramiko 第三方扩展库 scp.py 进行文件上传下载操作，当然你也可以使用 paramiko 库进行文件上传下载 安装 scp.py pip instal scp ","date":"2021-06-14","objectID":"/posts/python-paramiko/:3:0","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"简单的文件上传 import paramiko from scp import SCPClient ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy) ssh.connect(hostname='192.168.31.100', port=22, username='root', key_filename=\"\u003c你的私钥路径\u003e\", passphrase=\"\u003c私钥密码\u003e\") scp = SCPClient(ssh.get_transport()) # 上传下载文件，不指定完整路径默认就是用户的家目录 scp.put('/etc/hosts', 'mhost') # 从用户的家目录下载 mhost 文件到当前目录下 scp.get('mhost') # 上传目录至远程主机，并改名为 mail scp.put('postfix', remote_path='/data/mail', recursive=True) # 下载目录到当前路径下 scp.get('/data/mail', recursive=True) scp.close() ","date":"2021-06-14","objectID":"/posts/python-paramiko/:3:1","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"使用 with 语法 from paramiko import SSHClient from scp import SCPClient with SSHClient() as ssh: ssh.load_system_host_keys() ssh.connect('example.com') with SCPClient(ssh.get_transport()) as scp: scp.put('test.txt', 'test2.txt') scp.get('test2.txt') ","date":"2021-06-14","objectID":"/posts/python-paramiko/:3:2","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"上传文件类对象 使用 putfo 方法可用于上传文件类对象 import io from paramiko import SSHClient from scp import SCPClient ssh = SSHClient() ssh.load_system_host_keys() ssh.connect('example.com') # SCPCLient takes a paramiko transport as an argument scp = SCPClient(ssh.get_transport()) # generate in-memory file-like object fl = io.BytesIO() fl.write(b'test') fl.seek(0) # upload it directly from memory scp.putfo(fl, '/tmp/test.txt') # close connection scp.close() # close file handler fl.close() ","date":"2021-06-14","objectID":"/posts/python-paramiko/:3:3","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":"显示文件上传下载进度 跟踪文件上载/下载的进度 progress 函数可以作为对 SCPClient 的回调，以处理当前 SCP 操作如何处理传输的进度。在下面的示例中，我们打印文件传输的完成百分比。 from paramiko import SSHClient from scp import SCPClient import sys ssh = SSHClient() ssh.load_system_host_keys() ssh.connect('example.com') # Define progress callback that prints the current percentage completed for the file def progress(filename, size, sent): sys.stdout.write(\"%s's progress: %.2f%%\\r\" % (filename, float(sent)/float(size)*100) ) # SCPCLient takes a paramiko transport and progress callback as its arguments. scp = SCPClient(ssh.get_transport(), progress=progress) # you can also use progress4, which adds a 4th parameter to track IP and port # useful with multiple threads to track source def progress4(filename, size, sent, peername): sys.stdout.write(\"(%s:%s) %s's progress: %.2f%%\\r\" % (peername[0], peername[1], filename, float(sent)/float(size)*100) ) scp = SCPClient(ssh.get_transport(), progress4=progress4) scp.put('test.txt', '~/test.txt') # Should now be printing the current progress of your put function. scp.close() ","date":"2021-06-14","objectID":"/posts/python-paramiko/:3:4","tags":["paramiko"],"title":"Paramiko SSH 远程连接 Linux 主机","uri":"/posts/python-paramiko/"},{"categories":["python"],"content":" Github 官方仓库: https://github.com/pyauth/pyotp ","date":"2021-06-14","objectID":"/posts/python-pyopt/:0:0","tags":["pyotp"],"title":"PyOTP 实现双重或多因素身份验证","uri":"/posts/python-pyopt/"},{"categories":["python"],"content":"生成密钥 PyOTP 提供了一个帮助函数来生成一个16个字符的 base32 密钥，与Google Authenticator和其他OTP应用程序兼容： import pyotp secret = pyotp.random_base32() 某些应用程序希望将密钥格式化为十六进制编码字符串： pyotp.random_hex() # returns a 32-character hex-encoded secret ","date":"2021-06-14","objectID":"/posts/python-pyopt/:1:0","tags":["pyotp"],"title":"PyOTP 实现双重或多因素身份验证","uri":"/posts/python-pyopt/"},{"categories":["python"],"content":"基于时间的 OTP import pyotp totp = pyotp.TOTP('base32secret3232') totp.now() # =\u003e '492039' # OTP verified for current time totp.verify('492039') # =\u003e True time.sleep(30) totp.verify('492039') # =\u003e False 客户端可以使用 google 验证器，也可以使用洋葱身份验证器 ","date":"2021-06-14","objectID":"/posts/python-pyopt/:2:0","tags":["pyotp"],"title":"PyOTP 实现双重或多因素身份验证","uri":"/posts/python-pyopt/"},{"categories":["python"],"content":"创建虚拟环境 python3 -m venv pyenv source pyenv/bin/activate ","date":"2021-06-13","objectID":"/posts/python-qrcode/:1:0","tags":["qrcode"],"title":"Python 批量生成二维码","uri":"/posts/python-qrcode/"},{"categories":["python"],"content":"安装依赖库 pip install Image pip install qrcode ","date":"2021-06-13","objectID":"/posts/python-qrcode/:2:0","tags":["qrcode"],"title":"Python 批量生成二维码","uri":"/posts/python-qrcode/"},{"categories":["python"],"content":"编写代码 import qrcode def createQR(name, url): img = qrcode.make(url) name = name + '.png' with open(name, 'wb') as f: img.save(f) print(\"create QR code: \", name) def main(filename): with open(filename) as f: for line in f: name, url = line.split(',') createQR(name, url) if __name__ == '__main__': main('test.txt') ","date":"2021-06-13","objectID":"/posts/python-qrcode/:3:0","tags":["qrcode"],"title":"Python 批量生成二维码","uri":"/posts/python-qrcode/"},{"categories":["python"],"content":"准备数据文件 程序从文本文件中读取数据，以行为单位，根据每行数据内容生成二维码 格式: 生成二维码文件名,网址 使用逗号为分隔符 示例 test.txt baidu,https://www.baidu.com tencent,https://www.qq.com ","date":"2021-06-13","objectID":"/posts/python-qrcode/:4:0","tags":["qrcode"],"title":"Python 批量生成二维码","uri":"/posts/python-qrcode/"},{"categories":["python"],"content":"执行 $ python qr.py create QR code: baidu.png create QR code: tencent.png ","date":"2021-06-13","objectID":"/posts/python-qrcode/:5:0","tags":["qrcode"],"title":"Python 批量生成二维码","uri":"/posts/python-qrcode/"},{"categories":["jenkins"],"content":"安装 官方安装文档: https://pkg.jenkins.io/redhat-stable/ wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum install jenkins ","date":"2021-06-13","objectID":"/posts/jenkins-install/:1:0","tags":["jenkins"],"title":"Jenkins 安装配置","uri":"/posts/jenkins-install/"},{"categories":["jenkins"],"content":"配置 ","date":"2021-06-13","objectID":"/posts/jenkins-install/:2:0","tags":["jenkins"],"title":"Jenkins 安装配置","uri":"/posts/jenkins-install/"},{"categories":["jenkins"],"content":"配置方法1 配置前先启动 jenkins 服务, 在浏览器打开 http://\u003cyour_server_ip_address\u003e:8080 systemctl start jenkins 执行以下命令 mkdir -p /var/lib/jenkins/update-center-rootCAs wget https://cdn.jsdelivr.net/gh/lework/jenkins-update-center/rootCA/update-center.crt -O /var/lib/jenkins/update-center-rootCAs/update-center.crt chown jenkins.jenkins -R /var/lib/jenkins/update-center-rootCAs sed -i 's#https://updates.jenkins.io/update-center.json#https://cdn.jsdelivr.net/gh/lework/jenkins-update-center/updates/huawei/update-center.json#' /var/lib/jenkins/hudson.model.UpdateCenter.xml 在浏览器进行下一步时如果提示 “安装过程中出现一个错误： No such plugin: cloudbees-folder” 错误信息，这时我们只需要在 url 后面加 /restart 跳过安装插件的界面，重启 jenkins 即可 ","date":"2021-06-13","objectID":"/posts/jenkins-install/:2:1","tags":["jenkins"],"title":"Jenkins 安装配置","uri":"/posts/jenkins-install/"},{"categories":["jenkins"],"content":"配置方法2 启动 jenkins 服务，打开浏览器完成初始化 进入 Manage Jenkins -\u003e Manage Plugin -\u003e Advanced 最下面有 Update Site 设置为：https://mirrors.huaweicloud.com/jenkins/updates/update-center.json， 点击 Submit 提交，然后在点击 check now 修改 jenkins 配置，进入 /var/lib/jenkins 目录 ， 将 updates/default.json 其中的 https://updates.jenkins.io/download 替换为 https://mirrors.huaweicloud.com/jenkins ，然后把 www.google.com 修改为 www.baidu.com 重启 Jenkins 服务 享受加速下载插件的快感吧！ 替换为华为源 sed -i 's@https://updates.jenkins.io/download@https://mirrors.huaweicloud.com/jenkins@g' /var/lib/jenkins/updates/default.json 替换为清华大学源 重复以上步骤，地址进行相应的替换 源地址: https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json sed -i 's@https://updates.jenkins.io/download@https://mirrors.tuna.tsinghua.edu.cn/jenkins@g' /var/lib/jenkins/updates/default.json ","date":"2021-06-13","objectID":"/posts/jenkins-install/:2:2","tags":["jenkins"],"title":"Jenkins 安装配置","uri":"/posts/jenkins-install/"},{"categories":["linux"],"content":"场景: eth0: 192.168.1.10/24 网关: 192.168.1.1 eth1: 10.10.0.10/24 网关: 10.10.0.1 要求: 10.10.0.10 这个 IP 的流量从 10.10.0.1 网关出，保证 10.10.0.10 这个地址可以正常连接, 其他所有流量均从 192.168.1.1 网关出 配置: 将 192.168.1.1 配置为默认网关，写在网卡配置文件中 添加新的路由表规则 echo \"200 test\" \u003e\u003e /etc/iproute2/rt_tables ip route add default via 10.10.0.1 dev eth1 table test ip rule add from 10.10.0.10 table test 为了重启后也有效，将配置命令写入 /etc/rc.d/rc.local 文件中，并为此文件赋于运行权限 ","date":"2021-06-13","objectID":"/posts/linux-multi-ip/:0:0","tags":["ip"],"title":"Linux 多 ip 源进源出","uri":"/posts/linux-multi-ip/"},{"categories":["bash"],"content":"变量 Bash 变量为弱类型事先不用指定值的类型，Bash 变量默认为全局变量，可以使用 local 关键字定义局部变量 # 默认为全局变量 s='this is a test' # 函数中定义局部变量 a() { local s='s in function a' echo $a } # 打印变量长度 echo ${#s} local 关键字只能在函数中使用 ","date":"2021-06-13","objectID":"/posts/bash-basice/:1:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"逻辑运算符 ! : 逻辑非; 表达式为 true 则返回 false，否则返回 true; 例: [ ! false ] 返回 true \u0026\u0026: 逻辑与; 两个表达式都为 true 才返回 true; 等价于 (-a) 表示; 例: [ $a -lt 20 -a $b -gt 100 ] 返回 true ||: 逻辑或; 有一个表达式为 true 则返回 true; 等价于 (-o) 表示; 例: [ $a -lt 20 -o $b -gt 100 ] 返回 false 示例 which iftop \u0026\u0026 echo \"iftop exist\" which iftop || echo \"iftop does not exist\" ","date":"2021-06-13","objectID":"/posts/bash-basice/:2:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"比较运算符 ","date":"2021-06-13","objectID":"/posts/bash-basice/:3:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"1.数字比较 -gt: 大于 -lt: 小于 -ge: 大于或等于 -le: 小于或等于 -ne: 不等于 -eq: 等于 ","date":"2021-06-13","objectID":"/posts/bash-basice/:3:1","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"2.字符比较 [[ $str1 = $str2 ]]: 判断两个字符串是否相等 （等号前后有空格） [[ $str1 == $str2 ]]: 判断两个字符串是否相等，也可以是数字 [[ $str1 != $str2 ]]: 判断两个字符串是否不相等 检查字符串的字母排序情况，具体如下： [[ $str1 \u003e $str2 ]]: 如果 str1 的字母排序比 str2 大，则返回 true [[ $str1 \u003c $str2 ]]: 如果 str1 的字母排序比 str2 小，则返回 true ","date":"2021-06-13","objectID":"/posts/bash-basice/:3:2","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"3.变量检测 [[ -z $str1 ]]: 检测字符串长度是否不为 0，不为 0 返回 true。 [[ -n $str1 ]]: 检测字符串长度是否不为 0，不为 0 返回 true。 [[ $str1 ]]: 检测字符串是否为空，不为空返回 true。 ","date":"2021-06-13","objectID":"/posts/bash-basice/:3:3","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"4.文件系统选项 [ -f $var ]: 判断是否为文件 [ -x $var ]: 判断文件是否有可执行权限 [ -d $var ]: 判断是否为一个目录 [ -e $var ]: 判断文件是否存在 [ -c $var ]: 判断文件是字符设备 （不知有啥用途） [ -b $var ]: 判断是否为设备文件 [ -w $var ]: 判断文件是否有可写权限”w“ [ -r $var ]: 判断文件是否有可读权限”r\" [ -L $var ]: 判断是否为符号链接 ","date":"2021-06-13","objectID":"/posts/bash-basice/:3:4","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"数组 Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。 与大部分编程语言类似，数组元素的下标由 0 开始。 Shell 数组用括号来表示，元素用\"空格\"符号分割开 arr=(\"first\" \"second\" \"third\") # 修改数组值 arr[0]='No.1' # 增加值 arr[3]='fourth' # 获取数组所有值 echo ${arr[@]} echo ${arr[*]} # 获取数组长度 echo ${#arr[@]} # 打印所有下标(索引)，从 0 开始 echo ${!arr[@]} # for 遍历数组，这个方法有时达不到理想效果, 参考 for 循环的坑 for a in ${arr[@]}; do echo $a done # for 通过下标遍历数组, 推荐使用这个方式 for i in ${!arr[@]}; do echo ${arr[$i]} done for 循环遍历数组的坑 #!/bin/bash string=( \"this is first line\" \"this is second line\" \"this is third line\" ) for line in ${string[@]}; do echo $line sleep 0.5 done 运行以上脚本你就会知道坑在哪里了 ","date":"2021-06-13","objectID":"/posts/bash-basice/:4:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"条件判断语句 ","date":"2021-06-13","objectID":"/posts/bash-basice/:5:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"1. 使用 test 和 [ ] 进行条件判断 test 与 [ ] 是对等的功能一致。 test [root@localhost ~]# test -f /etc/passwd \u0026\u0026 echo yes yes [root@localhost ~]# test -d /etc/ \u0026\u0026 echo yes yes 将test 替换为 [ ] [root@localhost ~]# [ -f /etc/passwd ] \u0026\u0026 echo yes yes [root@localhost ~]# [ -d /etc ] \u0026\u0026 echo yes yes Tips: [] 与其中的命令两边必须得空格隔开才行，否则会报错 [root@localhost ~]# [-d /etc ] \u0026\u0026 echo yes -bash: [-d: command not found 以上就是没有用空格隔开导致的错误 ","date":"2021-06-13","objectID":"/posts/bash-basice/:5:1","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"2. if 语句，条件判断 在其他编程语言中，if语句后面的对象是一个值为TRUE或FALSE的等式。bash shell脚本中的if语句不是这样的。 bash shell中的if语句运行在if行定义的命令。如果命令的退出状态是0（成功执行命令），将执行then后面的所有命令。 如果命令的退出状态是0以外的其他值，那么then后面的命令将不会执行，bash shell会移动到脚本的下一条命令。 单分支 if [[ $? -eq 0 ]]; then echo \"successfully\" else echo \"failed\" fi 多分支 read -p \"please input a number:\" num if [[ $num -le 10 ]]; then echo \"num 小于等于 10\" elif [[ $num -le 20 ]]; then # elif 可以有多个 echo \"num 小于等于 20 fi 扩展：可以直接通过命令执行状态进行判断 示例：服务监控脚本（pptpd 服务器监控） #!/bin/bash TIME=5 while :; do if netstat -anptl | grep -q \"pptpd\" # -q 不显示任何内容，此处只是判断不需要显示 then echo \"$(date +%Y-%m-%d' '%H:%M:%S)pptpd is runing...\" \u003e /tmp/pptpd.log else /etc/init.d/pptpd restart-kill /etc/init.d/pptpd start fi sleep $TIME done ","date":"2021-06-13","objectID":"/posts/bash-basice/:5:2","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"3. case 语句 上面提到的『 if …. then …. fi 』对于变量的判断是以『比对』的方式来分辨的， 如果符合状态就进行某些行为，并且透过较多层次 (就是 elif …) 的方式来进行多个变量的程序码编写。 好，那么万一我有多个既定的变量内容，我所需要的变量就是 “hello” 及空字串两个， 那么我只要针对这两个变量来配置状况就好了，对吧？那么可以使用什么方式来设计呢？呵呵～就用 case … in …. esac 吧～，他的语法如下： case $变量名 in \"第一个变量值\") # 每个变量内容建议用双引号括起来，关键字则为小括号 echo 'first' ;; # 每项最后用两个分号表示结束。 \"第二个变量值\") echo 'second\" ;; *) # 最一个变量用星号来表示所有其他值 echo \"default\" ;; esac 示例 #!/bin/bash clear cat \u003c\u003c EOF Sys Admin Menu 1. Display disk space 2. Display memory usage 0. Exit menu EOF read -n 1 -p \"Enter option:\" option case $option in 1) df -hT ;; 2) free -m ;; 0) exit 0 ;; *) echo \"Warning: wrong choice, please re-select.\" esac ","date":"2021-06-13","objectID":"/posts/bash-basice/:5:3","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"循环语句 ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"1. for 循环 # 1. 循环一个列表 for shname in $(ls *.sh) do name=$(echo \"$shname\" | awk -F. '{print $1}') echo $name done # 2. 指定循环次数 （有点像C语法，但记得双括号） for (( i=0;i\u003c10;i++)) { echo $i } # 3. 等价于2 for i in $(seq 1 10) do echo $i done ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:1","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"2. while 循环 while 循环，只有结果为真时（在linux退出状态为0）才会执行，直接结果为假时才终止循环。为真时执行，用true作为循环条件可以无限循环。 简单语法演示 while : do date +%F sleep 2 done Tips: 在linux中:表示真，可以替代true,以上示例是个死循环，需要添加终止条件。 示例 1 min=1 max=100 while [ $min -le $max ] do echo $min min=`expr $min + 1` done 示例 2 双括号形式，内部结构有点像C的语法，注意赋值：i=$(($i+1)) i=1 while(($i\u003c100)) do if(($i%4==0)) then echo $i fi i=$(($i+1)) done ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:2","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"3. until 循环 直到给定的结果为真时，停止循环。刚好与while循环相反。（貌似不常用） x=0 until [ $x -eq 9 ]; do let x++; echo $x; done ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:3","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"4. 生成连续的数字，字母列表（序列） # 打印1到100的数字 echo {1..100} seq 100 # 打印小写所有字母 echo {a..z} # 打印大写所有字母 echo {A..Z} ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:4","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"5. 循环控制语句 break 命令不执行当前循环体内break下面的语句从当前循环退出. continue 命令是程序在本循体内忽略下面的语句,从循环头开始执行 ","date":"2021-06-13","objectID":"/posts/bash-basice/:6:5","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["bash"],"content":"函数 定义函数的两种方法 function f1 { echo 'this is f1' } f2() { echo 'this is f2' } 函数的调用与传值 hello() { echo \"hello ${1}!\" } # 调用函数并传传递参数 hello $1 ","date":"2021-06-13","objectID":"/posts/bash-basice/:7:0","tags":["bash"],"title":"Bash 基础","uri":"/posts/bash-basice/"},{"categories":["linux"],"content":"Linux 默认使用密码登录，很不安全容易被暴力破解入侵。使用密钥登录可以增加安全性。下面将介绍如何配置密钥登录验证. ","date":"2021-06-13","objectID":"/posts/ssh-key/:0:0","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"生成 ssh 密钥对 首先我们需要在自己的电脑上生成密钥对(公私钥) ","date":"2021-06-13","objectID":"/posts/ssh-key/:1:0","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"Linux 由于 linux 和 macOS 自带 ssh 软件和终端，直接打开终端输入以下输入命令生成密钥对 [root@singhead ~]# ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: 15:a5:d1:4c:bc:dd:fa:0f:78:18:f6:78:4d:04:77:95 root@singhead The key's randomart image is: +--[ RSA 2048]----+ | o*o . *| | ++ E.| | o o ..| | . . ...| | S o ..| | . *.o | | + =..| | o ..| | o| +-----------------+ [root@singhead .ssh]# ls -l total 12 -rw------- 1 root root 1671 Oct 2 00:04 id_rsa -rw-r--r-- 1 root root 395 Oct 2 00:04 id_rsa.pub ","date":"2021-06-13","objectID":"/posts/ssh-key/:1:1","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"Windows 由于 Windows 环境问题，我们需要借助于 GitBash 这个工具来生成密钥对，如果你有使用终端管理工具（例如 Xshell, SecureCRT 等）也可以使用终端管理工具生成。 这里只介绍如何使用 GitBash 生成密钥的操作过程 下载安装 Git: https://gitforwindows.org/ 安装完成后，会有一个 GitBash 的终端可用，我们就用这个来操作 打开 GitBash 输入以下命令, 一路按 Enter 键即可完成，密钥对的生成，密钥默认存放 C:\\Users\\\u003c你的用户名\u003e\\.ssh 目录下 ssh-keygen -t rsa 不管那个系统平台使用的命令都是一样的 ","date":"2021-06-13","objectID":"/posts/ssh-key/:1:2","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"推送公钥文件到 linux 服务器中 [root@singhead ~]# cd .ssh/ [root@singhead .ssh]# ssh-copy-id -i id_rsa.pub root@192.168.1.20 [root@singhead .ssh]# ssh-copy-id -i id_rsa.pub root@192.168.1.21 Tips: 以上命令将公钥添加至主机的 /root/.ssh/authorized_keys 文件中 ","date":"2021-06-13","objectID":"/posts/ssh-key/:2:0","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"配置 ssh 调整 linux 服务器设置,禁用密码验证，启用密钥验证对验证，并重启 SSH 服务 [root@singhead ~]# vim /etc/ssh/sshd_config PasswordAuthentication no PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys [root@singhead ~]# service sshd restart ","date":"2021-06-13","objectID":"/posts/ssh-key/:3:0","tags":["ssh"],"title":"SSH 密钥对的使用过程","uri":"/posts/ssh-key/"},{"categories":["linux"],"content":"取消初次连接确认 在脚本中有时会使用 ssh 进行远程连接操作，如果是第一次 ssh 连接往往会提示你是否确认连接并要求你输入 yes, 才能继续。如何才能避免这个步骤呢？ 1. 通过 .ssh/config 配置文件 cat \u003e\u003e ~/.ssh/config \u003c\u003c EOF StrictHostKeyChecking no EOF 2. 在 ssh 命令加上一个参数 ssh username@ip_address -p 22 -o StrictHostKeyChecking=no ","date":"2021-06-13","objectID":"/posts/ssh-tips/:1:0","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"SSH 密钥 通过私钥计算公钥 ssh-keygen.exe -f ~/.ssh/id_rsa -y 查看公钥的指纹 ssh-keygen.exe -f ~/.ssh/id_rsa.pub -l ","date":"2021-06-13","objectID":"/posts/ssh-tips/:2:0","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"SSH agent 转发 通过 OpenSSH 的 agent 转发功能，我们可以从 A 服务器直接连接 B 服务器而不需要将私钥放在 A 服务器 前提条件 A，B 服务器上 authorized_keys 文件中有相同的钥，使用这个公钥的私钥进行连接. 通过 .ssh/config 配置文件 写入如下配置, 然后正常连接服务器即可 cat \u003e\u003e ~/.ssh/config \u003c\u003c EOF Host example.cn ForwardAgent yes EOF 命令行方式 \u003e ssh-add -K ~/.ssh/id_rsa \u003e ssh -A root@example.cn -A：启动 agent 转发，具体可以 man ssh 默认 SSH 是启动 agent 的。如果不成功请检查 /etc/ssh/sshd_config 配置文件 AllowAgentForwarding 选项及 /etc/ssh/ssh_config 文件是否有 ForwardAgent no 配置项，改为 yes 即可。 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:3:0","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"ssh 代理设置 实验环境 Server： 192.168.0.1 Gateway: 100.100.100.100 Client: 100.100.100.101 说明：其中 Server 不可以访问外网; Gateway 可以访问 Server， 同时与外网互通; Client 不能直接访问 Server 需要先连接 Gateway 才可以访问 Server。 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:4:0","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"ssh 端口转发 ssh 端口转发功能，实现 Clinet 直接访问 Server, 在 Gateway 执行命令如下： ssh -CfNg -L 2233:192.168.0.1:22 root@192.168.0.1 解释： -L 是本地端口转发，通过将本地 2233 与 Server 的 22 端口相关联，以使 Client 访问 2233 时自动转发到 Server 的 22 端口。 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:4:1","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"ssh ProxyCommand 通过配置 ~/.ssh/config 文件也可以达到 ssh 代理的功能，具体配置如下（在 Client 上配置） [root@localhost ~]# vim .ssh/config Host server HostName 192.168.0.1 Port 22 ProxyCommand ssh -l root -p 22 100.100.100.100 -W %h:%p IdentityFile /root/.ssh/id_rsa # 配置好后，就可以直接通过以下命令连接 Server [root@localhost ~]# ssh root@server 说明 Host 别名，取一个主别名 HostName 主机的ip地址，在此例中是 Server 的 ip 地址，也可以是域名 ProxyCommand ssh 代理的命令 -W 后面是 Server 的 ip 地址及端口，会自动替换 IdentityFile 表示连接使用的私钥 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:4:2","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"ssh 命令行实现中转代理 当然我们也可以不写配置文件直接通过命令也是可以进行 ssh 代理跳转的，命令如下： ssh -t -p 22 userb@123.456.789.110 \"ssh userc@192.168.1.111\" 注： 因为 ssh 是可以直接远程执行命令的, 不可以少 -t 参数 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:4:3","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"ssh socket5 代理 执行以下命令就可以创建一个基于 ssh 的 socket5 代理了,最好将此放入后台运行。 ssh -D 8080 -f -C -q -N fred@server.example.org # 放入后台运行 nohup ssh -D 8080 -f -C -q -N fred@server.example.org \u0026 ","date":"2021-06-13","objectID":"/posts/ssh-tips/:4:4","tags":["ssh"],"title":"SSH 使用小技巧","uri":"/posts/ssh-tips/"},{"categories":["linux"],"content":"规则表 filter表，包含三个规则链：INPUT、FORWARD、OUTPUT。主要用于对数据包进行过滤 nat表，包含三个规则链：PREROUTING、POSTROUTING、OUTPUT。主要用于网络地址转换（修改数据包的IP地址） mangle表，包含五个规则链：PREROUTING、POSTROUTING、INPUT、OUTPUT、FORWARD。主要用于修改数据包的TOS（服务类型）、TTL（生存周期）值以及为数据包设置Mark标记，以实现Qos调整以及策略路由等应用，由于需要相应的路由设备支持，因为应用并不广泛。 raw表，包含两条规则链：OUTPUT、PREROUTING。主要用于决定数据包是否被状态跟踪机制处理。在匹配数据包时，raw表优先于其他表。 ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:1:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"规则链 INPUT链：当接收到访问防火墙本机地址的数据包（入站）时，应用此链的规则 OUTPUT链：当防火墙本机向外发送数据包（出站）时应用此链的规则 FORWARD链：当接收到需要通过防火墙发送给其他地址的数据包（转发）时，应用此链的规则 PREROUTING链：在对数据包作路由选择之前，应用此链的规则 POSTROUTING链：在对数据包作路由选择之后，应用此链的规则 ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:2:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"应用顺序 1、规则表之间的应用顺序 当数据包抵达防火墙时，将依次应用raw、mangle、nat、filter表中对应链内的规则（如果有的话）。 2、规则链之间的应用顺序 入站数据流向：来自外界的数据包到达防火墙后，首先由PREROUTING规则链处理（是否修改数据包地址等），之后会进行路由选择（判断该数据包该发往何处），如果数据包的目标地址是防火墙本机（如Internet 用户访问防火墙中的Web服务的数据包），那么内核将其传递给INPUT链进行处理（决定是否允许通过等），通过以后再交给系统上层的应用程序（如httpd服务器）进行响应。 转发数据流向：来自外界的数据包到达防火墙后，首先被PREROUTING规则处理，之后会进行路由选择，如果数据包的目标地址是其他外部地址（如局域网用户通过网关访问QQ站点的数据包），则内核将其传给FORWARD链进行处理（是否转发或拦截），然后在交给POSTROUTING规则链（是否修改数据包的地址等）进行处理。 出站的数据流向：防火墙本机向外部地址发送的数据包（如防火墙主机中测试公网DNS服务时），首先被OUTPUT规则链处理，之后进行路由选择，然后传递给POSTROUTING规则链（是否修改数据包地址等）进行处理。 ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:3:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"iptables 基础语法 ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"iptable 参数说明 -A 在链的末尾添加一个规则 -I 在链中插入一条规则，如未指定规则序号，则插入在首行 -D 删除一个规则，按规则序号或内容删除 -F 清空链中所有规则，如未指定链则清空表中所有链的规则 -L 以列表的形式显示规则 -N 新建一个条用户自定义的规则链 -P 指定链的默认规则 -n 以数字的形式显示结果 -v 查看规则列表时显示详细信息 --line-numbers 查看规则列表时，同时显示规则序号 ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:1","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"添加及插入规则 在 filter 表的 INPUT 链中添加一条规则 iptables -t filter -A INPUT -p tcp -j ACCEPT 在 filter 表的 INPUT 链中插入一条规则 iptables -t filter -I INPUT -p udp -j ACCEPT 在 filter 表的 INPUT 链中插入一条规则（作为链中的第二条规则） iptables -t filter -I INPUT 2 -p icmp -j ACCEPT ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:2","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"显示规则列表 查看 filter 表 INPUT 链中的所有规则，同时显示各条规则的顺序号 iptables -L INPUT --line-numbers 查看 filter 表各链所有规则的详细信息，同时以数字（速度更快）的形式显示地址及端口信息 iptables -vnL ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:3","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"删除、清空规则 删除第二条规则 iptables -D INPUT 2 清空 filter 表中所有链内的规则 iptables -F iptables -t filter -F 清空 nat/mangle 表中所有链内的规则 iptables -t nat -F iptables -t mangle -F ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:4","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"设置规则链的默认策略 设置 filter 表的 INPUT 链默认策略为 DROP iptables -t filter -P INPUT DROP 设置 filter 表的 OUTPUT 链默认策略为 ACCEPT iptables -t filter -P OUTPUT ACCEPT ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:5","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"获得 iptables 相关选项用法的帮助信息 iptables -p icmp -h ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:4:6","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"iptables 条件匹配 协议匹配：用于检查数据包的网络协议，允许使用的协议名包含在 /etc/protocols 文件中。使用 -p 拒绝所有 icmp 包进入 iptables -I INPUT -p icmp -j REJECT 允许转发所有非 icmp 协议的数据包（！取反） iptables -A FORWARD -p ! icmp -j ACCEPT 地址匹配：用于检查数据包的IP地址、网络地址。使用 -s 拒绝转发源地址为192.168.1.11主机的数据 iptables -A FORWARD -s 192.168.1.11 -j REJECT 拒绝转发目标地址为 192.168.2.0/24 网段的数据 iptables -A FORWARD -d 192.168.2.0/24 -j REJECT 网络接口匹配：使用 -o 出接口 -i 进接口 iptables -A INPUT -i eth1 -s 192.168.0.11 -j DROP iptables -A INPUT -o eth0 -d 192.168.1.10 -j ACCEPT 端口匹配：使用 --dport --sport 需要以 “-p tcp” 或 “-p udp” 为前提 允许转发局域网的 DNS 请求 iptables -A FORWARD -s 192.168.0.0/24 -p udp --dport 53 -j ACCEPT iptables -A FORWARD -d 192.168.0.0/24 -p udp --sport 53 -j ACCEPT 允许开放本机从 TCP 端口 20~1024 提供服务 iptables -A INPUT -p tcp --dport 20:1024 -j ACCEPT iptables -A OUTPUT -p tcp --dport 20:1024 -j ACCEPT ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:5:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"示例 # Firewall configuration written by system-config-securitylevel # Manual customization of this file is not recommended. *filter # 默认策略 :INPUT DROP [5278:800028] :FORWARD DROP [5278:800028] :OUTPUT ACCEPT [5278:800028] :RH-Firewall-1-INPUT - [5278:800028] # 自定义规则链 -A INPUT -j RH-Firewall-1-INPUT -A FORWARD -j RH-Firewall-1-INPUT # 允许回环接口访问 -A RH-Firewall-1-INPUT -i lo -j ACCEPT # 状态检测，RELATED（相关的状态），ESTABLISHED(建立的) -A RH-Firewall-1-INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT # 允许icmp -A RH-Firewall-1-INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT # 允许icmp -A RH-Firewall-1-INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT # 开放相应服务的端口 -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 8001:8015 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 3000 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 3001 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 110 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT -A RH-Firewall-1-INPUT -s 183.62.255.122,183.62.255.123,120.236.168.22,10.30.0.167 -p tcp -m state --state NEW -m tcp --dport 3306 -j ACCEPT # 开放zabbix服务端口 -A RH-Firewall-1-INPUT -s 10.30.0.167 -m state --state NEW -m tcp -p tcp --dport 10050:10051 -j ACCEPT -A RH-Firewall-1-INPUT -s 10.30.0.167 -m state --state NEW -m udp -p udp --dport 10050:10051 -j ACCEPT COMMIT ","date":"2021-06-13","objectID":"/posts/iptable-introduction/:6:0","tags":["iptables"],"title":"iptable 简单入门","uri":"/posts/iptable-introduction/"},{"categories":["linux"],"content":"穷人的 VPN sshuttle 是一个使用简单的轻量级全局代理工具(穷人的vpn)，以 ubuntu 18.04 为例演示如何使用, 使用前提是你有一台远程的 linux 服务器 Github 官方仓库: https://github.com/sshuttle/sshuttle 安装 sudo apt install sshuttle 使用 使用前先将不需要网络代理地址段列出来，例如本地局域网 192.168.1.0/24 sshuttle --dns -r username@server_ipaddress:port -x 192.168.1.0/24 -x server_ipaddress -D 注意: 如果出现 fatal: server died with error code 255 错误，请使用 -x 选项排除服务器 ip https://github.com/sshuttle/sshuttle/issues/150 ","date":"2021-06-13","objectID":"/posts/sshuttle/:1:0","tags":["sshuttle"],"title":"sshuttle 轻量级全局代理工具","uri":"/posts/sshuttle/"},{"categories":["linux"],"content":"番外 其他的全局代理软件 Linux 下的有 proxychains 下载地址 redsocks 下载地址 tsocks 下载地址 sshuttle Github - sshuttle macOS 下的有 Proxifier 下载地址 ProxyCap 下载地址 Windows 下的有 Proxifier 下载地址 ProxyCap 下载地址 ","date":"2021-06-13","objectID":"/posts/sshuttle/:2:0","tags":["sshuttle"],"title":"sshuttle 轻量级全局代理工具","uri":"/posts/sshuttle/"},{"categories":["linux"],"content":"Proxifier Proxifier 允许不支持通过代理服务器工作的网络应用程序通过SOCKS或HTTPS代理和链进行操作。 操作参考 https://blog.csdn.net/wu_cai_/article/details/80271478 注册码 用户名可以随意填写 Windows 5EZ8G-C3WL5-B56YG-SCXM9-6QZAP G3ZC7-7YGPY-FZD3A-FMNF9-ENTJB YTZGN-FYT53-J253L-ZQZS4-YLBN9 macOS P427L-9Y552-5433E-8DSR3-58Z68 ","date":"2021-06-13","objectID":"/posts/sshuttle/:2:1","tags":["sshuttle"],"title":"sshuttle 轻量级全局代理工具","uri":"/posts/sshuttle/"},{"categories":["virtualization"],"content":"1. 安装 vmware-tools 工具 [root@localhost ~]# mount /dev/cdrom /mnt mount: block device /dev/sr0 is write-protected, mounting read-only [root@localhost ~]# cp /mnt/VMwareTools-10.0.0-2977863.tar.gz . [root@localhost ~]# tar xzf VMwareTools-10.0.0-2977863.tar.gz [root@localhost ~]# cd vmware-tools-distrib/ [root@localhost vmware-tools-distrib]# ./vmware-install.pl [root@localhost vmware-tools-distrib]# ./vmware-install.real.pl ","date":"2021-06-13","objectID":"/posts/vmware-linux-share/:1:0","tags":["vmware"],"title":"VMware 的 Linux 客户机中装载共享文件夹","uri":"/posts/vmware-linux-share/"},{"categories":["virtualization"],"content":"2. 重启虚拟机，设置共享文件夹，挂载共享文件夹 [root@localhost ~]# vmware-hgfsclient data [root@localhost ~]# mount -t vmhgfs .host:/data /mnt [root@localhost ~]# ls /mnt/ bssh env2.7 Fabric-1.13.1 pssh-2.3.1 pytest test WTools ","date":"2021-06-13","objectID":"/posts/vmware-linux-share/:2:0","tags":["vmware"],"title":"VMware 的 Linux 客户机中装载共享文件夹","uri":"/posts/vmware-linux-share/"},{"categories":["git"],"content":"当 ssh key 文件不放在标准目录下， git 进行 clone push 操作时如何使用指定位置的 ssh key ","date":"2021-06-13","objectID":"/posts/git-keys/:0:0","tags":["git"],"title":"Git 使用指定的 key 连接","uri":"/posts/git-keys/"},{"categories":["git"],"content":"最佳解决方案 在 ~/.ssh/config 中添加配置 host github.com HostName github.com IdentityFile /data/sshkey/github_rsa User git 保持密钥的权限为 400 ","date":"2021-06-13","objectID":"/posts/git-keys/:1:0","tags":["git"],"title":"Git 使用指定的 key 连接","uri":"/posts/git-keys/"},{"categories":["git"],"content":"次佳解决方案 环境变量 GIT_SSH_COMMAND 从 Git 版本 2.3.0 可以使用环境变量 GIT_SSH_COMMAND 如下所示 GIT_SSH_COMMAND=\"ssh -i /data/sshkey/github_rsa\" git clone git@github.com:test/test.git 请注意，-i 有时可以被您的配置文件覆盖，在这种情况下，您应该给 SSH 一个空配置文件，如下所示 GIT_SSH_COMMAND=\"ssh -i /data/sshkey/github_rsa -F /dev/null\" git clone git@github.com:test/test.git 配置 core.sshCommand： 从 Git 版本 2.10.0，您可以配置每个 repo 或全局，所以您不必再设置环境变量！ git config core.sshCommand \"ssh -i ~/.ssh/id_rsa_example -F /dev/null\" git pull git push 参考文档: https://blog.csdn.net/SCHOLAR_II/article/details/72191042 ","date":"2021-06-13","objectID":"/posts/git-keys/:2:0","tags":["git"],"title":"Git 使用指定的 key 连接","uri":"/posts/git-keys/"},{"categories":["git"],"content":"安装Git yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-devel yum install git 接下来我们 创建一个 git 用户组和用户，用来运行 git 服务： useradd -m git ","date":"2021-06-13","objectID":"/posts/git-repo/:1:0","tags":["git"],"title":"Git 服务器搭建","uri":"/posts/git-repo/"},{"categories":["git"],"content":"创建证书登录 收集所有需要登录的用户的公钥，公钥位于id_rsa.pub文件中，把我们的公钥导入到 /home/git/.ssh/authorized_keys 文件里，一行一个。 如果没有该文件创建它： su - git mkdir .ssh chmod 755 .ssh touch .ssh/authorized_keys chmod 644 .ssh/authorized_keys ","date":"2021-06-13","objectID":"/posts/git-repo/:2:0","tags":["git"],"title":"Git 服务器搭建","uri":"/posts/git-repo/"},{"categories":["git"],"content":"初始化 Git 仓库 首先我们选定一个目录作为 Git 仓库，假定是 /home/git/gitrepo/runoob.git，在 /home/git/gitrepo 目录下输入命令： mkdir gitrepo cd gitrepo git init --bare runoob.git 以上命令 Git 创建一个空仓库，服务器上的 Git 仓库通常都以 .git 结尾。 注意: 如果你操作过程使用的用户不是 git 记得将目录权限改为 git 所有 ","date":"2021-06-13","objectID":"/posts/git-repo/:3:0","tags":["git"],"title":"Git 服务器搭建","uri":"/posts/git-repo/"},{"categories":["git"],"content":"克隆仓库 git clone git@192.168.45.4:gitrepo/runoob.git 192.168.45.4 为 Git 所在服务器 ip ，你需要将其修改为你自己的 Git 服务 ip 这样我们的 Git 服务器安装就完成。 ","date":"2021-06-13","objectID":"/posts/git-repo/:4:0","tags":["git"],"title":"Git 服务器搭建","uri":"/posts/git-repo/"},{"categories":["git"],"content":"迁移 Git 仓库 如果你想从别的 Git 托管服务器那里复制一份源代码到新的 Git 托管服务器上的话，可以通过以下步骤来操作。 这个文档只是让我们知道手动如何操作，大部分的 Git 代码托管平台和管理软件(Github, Gitlab, Gitee，Gogs, Gitea) 都支持仓库的在线克隆 从原地址克隆一份裸版本库，比如原本托管于 GitHub git clone --bare git://github.com/username/project.git 然后到新的 Git 服务器上创建一个新项目，比如 GitCafe。 以镜像推送的方式上传代码到 GitCafe 服务器上。 cd project.git git push --mirror git@gitcafe.com/username/newproject.git 删除本地代码 cd .. rm -rf project.git 到新服务器 GitCafe 上找到 Clone 地址，直接 Clone 到本地就可以了。 git clone git@gitcafe.com/username/newproject.git 这种方式可以保留原版本库中的所有内容。提交前要删除本地 remotes 中的分支引用，这样就不会将 remotes 里面的远程分支也推到服务器上去: 来源: http://blog.csdn.net/candyguy242/article/details/45920111 ","date":"2021-06-13","objectID":"/posts/git-migration/:1:0","tags":["git"],"title":"Git 仓库完整迁移含历史记录","uri":"/posts/git-migration/"},{"categories":["git"],"content":"Gitea 的部署安装很简单，直接从官方下载 gitea 二进制包运行就可以了 官方文档: https://docs.gitea.io/zh-cn/install-from-binary/ ","date":"2021-06-13","objectID":"/posts/gitea/:0:0","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["git"],"content":"二进制部署 gitea ","date":"2021-06-13","objectID":"/posts/gitea/:1:0","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["git"],"content":"安装 git git 版本只能是 2.x 及以上的版本，由于 centos 仓库自带的 git 版本默认为 1.x 不满足 gitea 的需求，需要使用第三方 YUM 安装 git 创建仓库配置文件 /etc/yum.repos.d/wandisco-git.repo [wandisco-git] name=Wandisco GIT Repository baseurl=http://opensource.wandisco.com/centos/7/git/$basearch/ enabled=1 gpgcheck=1 gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco 导入验证密钥 rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco 安装 git yum install git git --version 参考文档: https://www.cnblogs.com/zhaoxxnbsp/p/12674339.html ","date":"2021-06-13","objectID":"/posts/gitea/:1:1","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["git"],"content":"安装 gitea 我们需要使用 git 用户运行 gitea，所以需要先创建 git 用户，执行以下命令 useradd -d /data/git-data -m git 接下来的操作我们都使用 git 用户进行，执行 su - git 基于二进制的安装非常简单，只要从 下载页面 选择对应平台，拷贝下载 URL，执行以下命令即可（以Linux为例）： wget -O gitea https://dl.gitea.io/gitea/1.14.2/gitea-1.14.2-linux-amd64 chmod +x gitea 让 gitea 跑起来，执行命令 ./gitea web 现在你可以在浏览器打开 http://\u003cyour_server_ip\u003e:3000 进行配置了 gitea 支持的数据库有 SQLite, MySQL 和 PostgreSQL，你可以选择你喜欢的数据库来存放 gitea 相关的数据，如果是测试可以直接使用 SQLite ","date":"2021-06-13","objectID":"/posts/gitea/:1:2","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["git"],"content":"配置反向代理 在日常使用过程最好还是做下反向代理的配置，使用标准的 http(s) 端口提供服务，下面列出了常的 web 应用配置反向的的配置 nginx 配置 server { listen 80; server_name \u003cyour_domain\u003e; location / { proxy_pass http://127.0.0.1:3000; } } apache 配置 \u003cVirtualHost *:80\u003e ServerName \u003cyour_domain\u003e ProxyRequests Off ProxyPreserveHost On ProxyPass / http://127.0.0.1:3000/ ProxyPassReverse / http://127.0.0.1:3000/ \u003cproxy *\u003e AllowOverride None Order Deny,Allow Allow from all \u003c/proxy\u003e \u003c/VirtualHost\u003e caddy2 配置 \u003cyour_domain\u003e { reverse_proxy localhost:3000 } ","date":"2021-06-13","objectID":"/posts/gitea/:1:3","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["git"],"content":"部署 Gitea 和 Gogs 遇到的坑 Gogs 和 Gitea 依赖于 git 2.0 及以上的版本 Gogs 和 Gitea 查找 git 相关命令的路径固定为 /usr/bin，只配置 PATH 环境变量是没有用的，有以下错误提示: Failed to execute git command: exec: \"git-upload-pack\": executable file not found in $PATH fatal: Could not read from remote repository. 解决方法：使用软链接将 git 相关命令链接至 /usr/bin 目录下(这个只针对 git 安装命令路径不是 /usr/bin 的情况)， 执行命令 ln -s /usr/local/git/bin/* /usr/bin/ ","date":"2021-06-13","objectID":"/posts/gitea/:2:0","tags":["gitea"],"title":"Gitea 部署安装","uri":"/posts/gitea/"},{"categories":["macOS"],"content":" 以目录名为 Code 进行讲解 在英文目录下新建 .localized 隐藏目录 进入localized 目录创建 zh_CN.strings 文件, 输入 “english” = “中文”; mkdir -p Code/.localized cat Code/.localized/zh_CN.strings \"Code\" = \"代码\"; 注意: 不能少了分号 最后将目录名改为以 .localized 结尾即可 mv Code Code.localized ","date":"2021-06-12","objectID":"/posts/macos-dir/:0:0","tags":["macOS"],"title":"macOS 英文目录显示为中文","uri":"/posts/macos-dir/"},{"categories":["macOS"],"content":"视频格式转换 ffmpeg 下载站点 ffmpeg -i 2020年MySQL数据库入门到精通.flv -codec copy 2020年MySQL数据库入门到精通.mov ","date":"2021-06-12","objectID":"/posts/macos-cli/:1:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"清理DNS缓存 sudo killall -HUP mDNSResponder; say DNS cache has been flushed ","date":"2021-06-12","objectID":"/posts/macos-cli/:2:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"brew 使用代理 如果碰巧你的 brew 更新缓慢，可以试试让 brew 走代理更新程序包。 export ALL_PROXY=socks5://127.0.0.1:your_port_number ","date":"2021-06-12","objectID":"/posts/macos-cli/:3:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"命令格式化 APFS 格式 U 盘 语法 diskutil eraseDisk format name [APM[Format]|MBR[Format]|GPT[Format]] format: 文件系统 name: 设备卷标名 格式化 U 盘前先用 diskutil list 命令查询设备号 sudo diskutil eraseDisk FAT32 san MBRFormat /dev/disk3 APFS 格式是无法直接进行格式化的，我们需要首先删除 APFS 容器。执行如下命令 sudo diskutil apfs deleteContainer /dev/disk3 然后你就会发现，你的 U 盘格式自动变成了 Mac OS 扩展(日志式) ","date":"2021-06-12","objectID":"/posts/macos-cli/:4:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"显示隐藏文件夹 在 Windows 上隐藏文件夹大家应该都是老手了，转到 Mac 后，却发现隐藏文件夹和自己想象有那么一些不一样。为了更好的把大家的「小秘密」藏到内心最深处的地方，也可以使用两段命令来完成操作。跟前文一样，我们需要获取文件夹的路径，然后在终端中输入以下代码： chflags hidden ~/Desktop/Hidden 你也可以使用 nohidden 重新让该文件夹显示。如果你要显示全部文件，推荐大家直接使用快捷键「Shift + Command + .」即可显示全部隐藏文件。 ","date":"2021-06-12","objectID":"/posts/macos-cli/:5:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"允许从陌生来源安装软件 sudo spctl --master-disable ","date":"2021-06-12","objectID":"/posts/macos-cli/:6:0","tags":["macOS"],"title":"macOS 命令行","uri":"/posts/macos-cli/"},{"categories":["macOS"],"content":"iterm2 终端配置 比较好的 Iterm2 终端配色 Cobalt2：适用 macOS 本地及远程主机 Cobalt Neon : 适合远程主机 ","date":"2021-06-12","objectID":"/posts/macos-terminal/:1:0","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"安装 brew 国外源 liwangguideMBP:~ liwanggui$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 国内源 # 安装 /bin/zsh -c \"$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)\" # 卸载 /bin/zsh -c \"$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/HomebrewUninstall.sh)\" ","date":"2021-06-12","objectID":"/posts/macos-terminal/:1:1","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"配置 zmodem（lrzsz） Mac 默认终端是不支持 sz rz 命令的，iterm2 终端可以配置支持 sz rz 命令，配置方法如下： 1. 安装 lrzsz brew install lrzsz 2. 安装 iterm2-zmodem 脚本 git clone https://github.com/aikuyun/iterm2-zmodem.git cp iterm2-zmodem/iterm2*.sh /usr/local/bin/ chmod +x /usr/local/bin/iterm2*.sh 3. 配置 iterm2 Triggers Regular expression: \\*\\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Regular expression: \\*\\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh 按 command + , 打开配置面板，然后点击 “Profiles”, “Advanced”， “Triggers -\u003e Edit” Tips: 正常连接服务器就可以使用 sz rz 命令了 通过添加 profile 配置主机列表 ","date":"2021-06-12","objectID":"/posts/macos-terminal/:1:2","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"常用快捷键 command + d 横向分屏 command + shift + d 水平分屏 command + enter 全屏，取消全屏 command + ; 打开输入历史记录 command + f 打开搜索框 option + command + i 分屏时同时操作多个窗口，重复取消 ","date":"2021-06-12","objectID":"/posts/macos-terminal/:1:3","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"oh-my-zsh ","date":"2021-06-12","objectID":"/posts/macos-terminal/:2:0","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"安装 sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" zsh 主题配置项: ZSH_THEME=\"ys\" ","date":"2021-06-12","objectID":"/posts/macos-terminal/:2:1","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"配置 zsh-syntax-highlighting brew install zsh-syntax-highlighting echo 'source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh export ZSH_HIGHLIGHT_HIGHLIGHTERS_DIR=/usr/local/share/zsh-syntax-highlighting/highlighters' \u003e\u003e ~/.zshrc source ~/.zshrc ","date":"2021-06-12","objectID":"/posts/macos-terminal/:2:2","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"配置从当前位置删除到行首， ctrl + u echo 'bindkey \\^U backward-kill-line' \u003e\u003e ~/.zshrc source ~/.zshrc ","date":"2021-06-12","objectID":"/posts/macos-terminal/:2:3","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"新增命令无法自动补全解决方法 rehash # 执行 rehash 后就可以了 ","date":"2021-06-12","objectID":"/posts/macos-terminal/:3:0","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["macOS"],"content":"配置 vim $ cat ~/.vimrc \" 语法高亮 syntax on \" 自动检测文件外部更改 set autoread \" 显示标尺 set ruler \" 设置编码 set encoding=utf-8 \" 共享剪切板 set clipboard=unnamed \" 总是显示状态栏 set laststatus=2 \" 配置(软)制表符为宽度为4 set tabstop=4 set softtabstop=4 \" 将制表符转为空格 set expandtab \" 设置缩进的空格数为4 set shiftwidth=4 \" 设置自动缩进：即每行的缩进值与上一行相等；使用 noautoindent 取消设置 set autoindent \" 使用 C/C++ 语言的自动缩进方式 set cindent \" 设置C/C++语言的具体缩进方式 set cinoptions={0,1s,t0,n-2,p2s,(03s,=.5s,\u003e1s,=1s,:1s \" 高亮搜索内容 set hlsearch \" 可以删除任意值 set backspace=2 \"记录上次编辑位置 au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif ","date":"2021-06-12","objectID":"/posts/macos-terminal/:4:0","tags":["macOS"],"title":"macOS 终端配置","uri":"/posts/macos-terminal/"},{"categories":["centos"],"content":" 重启开机 按 e 编辑启动选项 编辑修改两处：ro 改为 rw, 在 LANG=en_US.UFT-8 后面添加 init=/bin/bash 按 Ctrl+X 重启，并修改密码, 输入 passwd root 命令重置 root 密码 由于 selinux 开启着的需要执行以下命令更新系统信息, 否则重启之后密码未生效 touch /.autorelabel 重启系统  exec /sbin/init ","date":"2021-06-12","objectID":"/posts/centos7-reset-password/:0:0","tags":["centos"],"title":"CentOS 7 重置 root 密码","uri":"/posts/centos7-reset-password/"},{"categories":["windows"],"content":"好用的工具 Listary: https://www.listarypro.com/ PotPlayer: https://daumpotplayer.com/download/ Microsoft PowerToys: https://docs.microsoft.com/zh-cn/windows/powertoys/ VsCode: https://code.visualstudio.com/docs QuickLook: https://github.com/QL-Win/QuickLook Windows Terminal: 通过 Windows 应用商店安装 Deskreen: 将电脑屏幕共享到浏览器中，做第二块屏幕[Win/macOS/Linux] https://github.com/pavlobu/deskreen Barrier: 一套键鼠控制多台主机, https://github.com/debauchee/barrier ","date":"2021-06-12","objectID":"/posts/windows-tools/:1:0","tags":["windows"],"title":"Windows 必备工具","uri":"/posts/windows-tools/"},{"categories":["windows"],"content":"1. 为 Sublime Text 添加右键菜单 注册表位置： HKEY_CLASSES_ROOT\\*\\shell 菜单名,显示在右键菜单上 icon 字符串值，显示的图标 command (子项） 默认值： 操作的命令 D:\\Program Files\\Sublime Text 3\\sublime_text.exe \"%1\" 添加右键 shell 菜单注册表地址: 计算机\\HKEY_CLASSES_ROOT\\Directory ","date":"2021-06-12","objectID":"/posts/windows-contextmenu/:1:0","tags":["regedit"],"title":"Windows 右键菜单管理","uri":"/posts/windows-contextmenu/"},{"categories":["windows"],"content":"2. 添加 cmd 右键菜单 将以下注册表信息保存为 xxx.reg 右键导入即可。 Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\cmdPrompt] @=\"Cmd Here\" \"icon\"=\"\\\"C:\\\\Windows\\\\System32\\\\cmd.exe\\\"\" [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\cmdPrompt\\command] @=\"\\\"C:\\\\Windows\\\\System32\\\\cmd.exe\\\" \\\"--cd=%v.\\\"\" [HKEY_CLASSES_ROOT\\Directory\\shell\\cmdPrompt] @=\"Cmd Here\" \"icon\"=\"\\\"C:\\\\Windows\\\\System32\\\\cmd.exe\\\"\" [HKEY_CLASSES_ROOT\\Directory\\shell\\cmdPrompt\\command] @=\"\\\"C:\\\\Windows\\\\System32\\\\cmd.exe\\\" \\\"--cd=%v.\\\"\" ","date":"2021-06-12","objectID":"/posts/windows-contextmenu/:2:0","tags":["regedit"],"title":"Windows 右键菜单管理","uri":"/posts/windows-contextmenu/"},{"categories":["windows"],"content":" VNC Server 下载地址 VNC Viewer 下载地址 realvnc 支持两种授权方式 注册 realvnc 账号，在安装 realvnc server 中输入账号密码即可。通过realvnc平台集中式管理终端，免费账号最多添加5个终端。连接方式也是 realvnc 中转连接。 购买 license key，在安装 realvnc server 中输入 license key 即可, 此种方式可以通过 vnc 协议直连服务器. 在使用 realvnc 的时候需要进行用户设置，可以运行 realvnc 安装目录下的 vnclicensewiz.exe GUI 程序来配置账号或者 License Key. 以下 License Key 支持 4.6 以上 RealvncServer WHJRK-UXY7V-Q34M9-CZU8L-8KGFA 48R4P-NFZ46-NBCWY-Q2ZJT-3H9RA NGNW9-7Q8BK-UQGY7-J3KAA-6G39 Z456C-LMKTC-NLGWQ-H5CUR-ZVWEA A5HDP-LXKYN-UK4W6-XACZJ-ENWLA NRDX9-ZF9C5-JLGY7-CUC5J-77J2A 579R9-9B92W-4QHM9-6TK6D-H6F9A VETPD-HHC3S-63AH9-YAA26-8WVDA SSEWK-HBDM6-YYCWC-M3BQV-9XMDA LFKRU-DCTWH-6GJH2-7SWYR-D4CPA CQUTS-S5RDR-VT2WJ-9B6TU-DLHPA RR36V-7V29A-EVGJA-AYNEC-3DZYA UNLZ3-EHBVR-VACLK-S8QDH-JZMHA TPSNG-YEUGX-J4HZX-DPYSY-HZKXA UCUXY-TAFLN-YFBVV-D7VZE-9SHJA ANN2U-FM59S-DAGV4-4TK96-BDTKA F4X7H-CYLEV-XZ4ZW-USQ7D-KHMGA 63P3S-TGU8R-3C4ZE-WCKF4-S2W3A Q35YW-ZVH7L-Z94J4-9UJP9-77VFA 3TH6P-DV5AE-BLHY6-PNENS-B3AQA Tips: 以上 License Key 来源于互联网，可能随时会效 ","date":"2021-06-12","objectID":"/posts/windows-vnc/:0:0","tags":["realvnc"],"title":"Windows 安装 realvnc","uri":"/posts/windows-vnc/"},{"categories":["windows"],"content":"用户 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:1:0","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"激活用户 net user administrator /active:no ","date":"2021-06-12","objectID":"/posts/windows-cmd/:1:1","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"启用用户 net user administrator /active:yes ","date":"2021-06-12","objectID":"/posts/windows-cmd/:1:2","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"添加用户 net user username /add ","date":"2021-06-12","objectID":"/posts/windows-cmd/:1:3","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"删除用户 net user username /del ","date":"2021-06-12","objectID":"/posts/windows-cmd/:1:4","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"文件共享 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:0","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"查看系统共享 net share ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:1","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"删除共享 net share 共享名 /del ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:2","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"映射网络驱动器 net use H: \\\\192.168.31.141\\public /user:samba haiersamba ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:3","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"删除网络驱动器 net use /del H: ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:4","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"删除网络连接 net use \\\\192.168.31.141 /del ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:5","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"删除所有网络连接 net use * /del ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:6","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"查看无线 wifi 密码 # 查看已保存的wifi列表 netsh wlan show profiles # 查看指定wifi的密码 netsh wlan show profile TP-LINK_3E48 key=clear ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:7","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"重命名 rname q.txt q.rar //对单个文件重命名 rname *.png *.jpg //文件批量重命名 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:8","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"重启资源管理器 tskill explorer ","date":"2021-06-12","objectID":"/posts/windows-cmd/:2:9","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"系统服务 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:3:0","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"停止 Windows Update 服务 sc stop wuauserv ","date":"2021-06-12","objectID":"/posts/windows-cmd/:3:1","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"禁用 Windows Update 服务 sc config wuauserv start= disabled //start= 等号后有一个空格 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:3:2","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"创建 服务项 sc create [service name] [binPath= 程序路径] //binPath= 等号后有一个空格 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:3:3","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"网络 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:4:0","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"设置接口 IP 地址与 DNS netsh interface ip set address \"接口名\" static ip_address submask gateway netsh interface ip set address \"接口名\" source=dhcp //将接口设置为DHCP自动获取 ` netsh interface ip set dnsserver \"接口名\" static dns_ip primary ","date":"2021-06-12","objectID":"/posts/windows-cmd/:4:1","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"磁盘 ","date":"2021-06-12","objectID":"/posts/windows-cmd/:5:0","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":["windows"],"content":"diskpart 分区 shift + f10 diskpart #启动硬盘工具 list disk #选择磁盘 select disk 0 #选择磁盘 clean #清除分区 convert mbr #转换分区表 create partition primary size=102400 #创建主分区 active #激活分区 format fs=ntfs label=\"new volume\" quick compress #格式化 exit ","date":"2021-06-12","objectID":"/posts/windows-cmd/:5:1","tags":["cmd"],"title":"Windows 常用 CMD 命令","uri":"/posts/windows-cmd/"},{"categories":null,"content":"1. Cobbler介绍 Cobbler是一个Linux服务器安装的服务，可以通过网络启动(PXE)的方式来快速安装、重装物理服务器和虚拟机，同时还可以管理DHCP，DNS等。 Cobbler可以使用命令行方式管理，也提供了基于Web的界面管理工具(cobbler-web)，还提供了API接口，可以方便二次开发使用。 Cobbler是较早前的kickstart的升级版，优点是比较容易配置，还自带web界面比较易于管理。 Cobbler内置了一个轻量级配置管理系统，但它也支持和其它配置管理系统集成，如Puppet，暂时不支持SaltStack。 ","date":"2021-06-12","objectID":"/posts/cobbler/:1:0","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"1.1 Cobbler集成的服务 PXE服务支持 DHCP服务管理 DNS服务管理(可选bind,dnsmasq) 电源管理 Kickstart服务支持 YUM仓库管理 TFTP(PXE启动时需要) Apache(提供kickstart的安装源，并提供定制化的kickstart配置) ","date":"2021-06-12","objectID":"/posts/cobbler/:1:1","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"1.2 系统环境准备 [root@localhost ~]# cat /etc/redhat-release # 查看发行版本 CentOS Linux release 7.2.1511 (Core) [root@localhost ~]# uname -r # 查看内核版本 3.10.0-327.el7.x86_64 [root@localhost ~]# sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config # 关闭selinux功能,配置文件修改只有重启系统方可生效(如果不想重启，请使用此命令临时关闭selinux功能：setenforce 0) [root@localhost ~]# systemctl stop firewalld # 停止防火墙 [root@localhost ~]# systemctl disable firewalld # 禁用防火墙 [root@localhost ~]# ip addr show # 查看ip地址 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:4d:04:21 brd ff:ff:ff:ff:ff:ff inet 192.168.92.106/24 brd 192.168.92.255 scope global dynamic eth0 valid_lft 20823sec preferred_lft 20823sec inet6 fe80::20c:29ff:fe4d:421/64 scope link valid_lft forever preferred_lft forever [root@localhost ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo # 增加阿里云的 epel yum 源(没有此源将无法安装cobbler软件，软件安装完后不用可以删除) ","date":"2021-06-12","objectID":"/posts/cobbler/:1:2","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2. Cobbler的安装 ","date":"2021-06-12","objectID":"/posts/cobbler/:2:0","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2.1 安装Cobbler # 安装所需软件 [root@localhost ~]# yum -y install cobbler cobbler-web dhcp tftp-server pykickstart httpd xinetd [root@localhost ~]# rpm -ql cobbler # 查看安装的文件，下面列出部分。 /etc/cobbler # 配置文件目录 /etc/cobbler/settings # cobbler主配置文件，这个文件是YAML格式，Cobbler是python写的程序。 /etc/cobbler/dhcp.template # DHCP服务的配置模板 /etc/cobbler/tftpd.template # tftp服务的配置模板 /etc/cobbler/rsync.template # rsync服务的配置模板 /etc/cobbler/iso # iso模板配置文件目录 /etc/cobbler/pxe # pxe模板文件目录 /etc/cobbler/power # 电源的配置文件目录 /etc/cobbler/users.conf # Web服务授权配置文件 /etc/cobbler/users.digest # 用于web访问的用户名密码配置文件 /etc/cobbler/dnsmasq.template # DNS服务的配置模板 /etc/cobbler/modules.conf # Cobbler模块配置文件 /var/lib/cobbler # Cobbler数据目录 /var/lib/cobbler/config # 配置文件 /var/lib/cobbler/kickstarts # 默认存放kickstart文件 /var/lib/cobbler/loaders # 存放的各种引导程序 /var/www/cobbler # 系统安装镜像目录 /var/www/cobbler/ks_mirror # 导入的系统镜像列表 /var/www/cobbler/images # 导入的系统镜像启动文件 /var/www/cobbler/repo_mirror # yum源存储目录 /var/log/cobbler # 日志目录 /var/log/cobbler/install.log # 客户端系统安装日志 /var/log/cobbler/cobbler.log # cobbler日志 ","date":"2021-06-12","objectID":"/posts/cobbler/:2:1","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2.2 配置Cobbler [root@localhost ~]# systemctl start httpd # 启动 httpd 服务 [root@localhost ~]# systemctl start cobblerd # 启动 cobbler 服务 [root@localhost ~]# systemctl restart cobblerd # 此次重启是为了在执行 cobbler check 时不报错[root@localhost ~]# cobbler check The following are potential configuration items that you may want to fix: 1 : The 'server' field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work. This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it. 2 : For PXE to be functional, the 'next_server' field in /etc/cobbler/settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network. 3 : change 'disable' to 'no' in /etc/xinetd.d/tftp 4 : some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run 'cobbler get-loaders' to download them, or, if you only want to handle x86/x86_64netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The 'cobbler get-loaders' command is the easiest way to resolve these requirements. 5 : enable and start rsyncd.service with systemctl 6 : debmirror package is not installed, it will be required to manage debian deployments and repositories 7 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to 'cobbler' and should be changed, try: \"openssl passwd -1 -salt 'random-phrase-here' 'your-password-here'\" to generate new one 8 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use them Restart cobblerd and then run 'cobbler sync' to apply changes. # 上面提示的问题，我们一个一个来解决 [root@localhost ~]# cp /etc/cobbler/settings{,.ori} # 备份 # 第1个问题，server，Cobbler服务器的IP。 [root@localhost ~]# sed -i 's/server: 127.0.0.1/server: 192.168.92.106/' /etc/cobbler/settings # 第2个问题，next_server，如果用Cobbler管理DHCP，修改本项，作用：告知客户端TFTP服务器的ip。 [root@localhost ~]# sed -i 's/next_server: 127.0.0.1/next_server: 192.168.92.106/' /etc/cobbler/settings # 用Cobbler管理DHCP [root@localhost ~]# sed -i 's/manage_dhcp: 0/manage_dhcp: 1/' /etc/cobbler/settings # 防止循环装系统，适用于服务器第一启动项是PXE启动。 [root@localhost ~]# sed -i 's/pxe_just_once: 0/pxe_just_once: 1/' /etc/cobbler/settings # 第7个问题，设置新装系统的默认root密码123456。下面的命令来源于提示6。random-phrase-here为干扰码，可以自行设定。 [root@localhost ~]# openssl passwd -1 -salt 'root' '123456' $1$root$j0bp.KLPyr.u9kgQ428D10 [root@linux-node1 ~]# vim /etc/cobbler/settings default_password_crypted: \"$1$root$j0bp.KLPyr.u9kgQ428D10\" # 第4个问题，会自动从官网下载 [root@localhost ~]# cobbler get-loaders task started: 2017-02-26_113724_get_loaders task started (id=Download Bootloader Content, time=Sun Feb 26 11:37:24 2017) downloading https://cobbler.github.io/loaders/README to /var/lib/cobbler/loaders/README downloading https://cobbler.github.io/loaders/COPYING.elilo to /var/lib/cobbler/loaders/COPYING.elilo downloading https://cobbler.github.io/loaders/COPYING.yaboot to /var/lib/cobbler/loaders/COPYING.yaboot downloading https://cobbler.github.io/loaders/COPYING.syslinux to /var/lib/cobbler/loaders/COPYING.syslinux downloading https://cobbler.github.io/loaders/elilo-3.8-ia64.efi to /var/lib/cobbler/loaders/elilo-ia64.efi downloading https://cobbler.github.io/loaders/yaboot-1.3.17 to /var/lib/cobbler/loaders/yaboot downloading https://cobbler.github.io/loaders/pxelinux.0-3.86 to /var/lib/cobbler/loaders/pxelinux.0 downloading https://cobbler.github.io/loaders/menu.c32-3.86 to /var/lib/cobbler/loaders/menu.c32 downloading https://cobbler.github.io/loaders/grub-0.97-x86.efi to /var/lib/cobbler/loaders/grub-x86.efi downloading https://cobbler.github.io/loaders/grub-0.97-x86_64.efi to /var/lib/cobbler/loader","date":"2021-06-12","objectID":"/posts/cobbler/:2:2","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2.3 配置DHCP # 修改cobbler的dhcp模版，不要直接修改dhcp本身的配置文件，因为cobbler会覆盖。 [root@localhost ~]# vim /etc/cobbler/dhcp.template ...... # 仅列出修改过的字段 subnet 192.168.92.0 netmask 255.255.255.0 { option routers 192.168.92.2; # 指定网关 option domain-name-servers 192.168.92.2; # 指定DNS option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.92.200 192.168.92.254; # 分配的地址段（部署的服务器多可以给大点） ...... ","date":"2021-06-12","objectID":"/posts/cobbler/:2:3","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2.4 同步cobbler配置 # 同步最新cobbler配置，它会根据配置自动修改dhcp等服务。 [root@localhost ~]# cobbler sync task started: 2017-02-26_115318_sync task started (id=Sync, time=Sun Feb 26 11:53:18 2017) running pre-sync triggers cleaning trees removing: /var/lib/tftpboot/grub/images copying bootloaders trying hardlink /var/lib/cobbler/loaders/pxelinux.0 -\u003e /var/lib/tftpboot/pxelinux.0 trying hardlink /var/lib/cobbler/loaders/menu.c32 -\u003e /var/lib/tftpboot/menu.c32 trying hardlink /var/lib/cobbler/loaders/yaboot -\u003e /var/lib/tftpboot/yaboot trying hardlink /usr/share/syslinux/memdisk -\u003e /var/lib/tftpboot/memdisk trying hardlink /var/lib/cobbler/loaders/grub-x86.efi -\u003e /var/lib/tftpboot/grub/grub-x86.efi trying hardlink /var/lib/cobbler/loaders/grub-x86_64.efi -\u003e /var/lib/tftpboot/grub/grub-x86_64.efi copying distros to tftpboot copying images generating PXE configuration files generating PXE menu structure rendering DHCP files generating /etc/dhcp/dhcpd.conf rendering TFTPD files generating /etc/xinetd.d/tftp cleaning link caches running post-sync triggers running python triggers from /var/lib/cobbler/triggers/sync/post/* running python trigger cobbler.modules.sync_post_restart_services running: dhcpd -t -q received on stdout: received on stderr: running: service dhcpd restart received on stdout: received on stderr: Redirecting to /bin/systemctl restart dhcpd.service running shell triggers from /var/lib/cobbler/triggers/sync/post/* running python triggers from /var/lib/cobbler/triggers/change/* running python trigger cobbler.modules.scm_track running shell triggers from /var/lib/cobbler/triggers/change/* *** TASK COMPLETE *** ","date":"2021-06-12","objectID":"/posts/cobbler/:2:4","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"2.5 开机启动 # 设置为开机启动 [root@localhost ~]# systemctl enable httpd [root@localhost ~]# systemctl enable rsyncd [root@localhost ~]# systemctl enable xinetd [root@localhost ~]# systemctl enable cobblerd [root@localhost ~]# systemctl enable dhcpd # 重启所有服务 [root@localhost ~]# systemctl restart httpd [root@localhost ~]# systemctl restart rsyncd [root@localhost ~]# systemctl restart xinetd [root@localhost ~]# systemctl restart cobblerd [root@localhost ~]# systemctl restart dhcpd ","date":"2021-06-12","objectID":"/posts/cobbler/:2:5","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3. cobbler 命令行管理 ","date":"2021-06-12","objectID":"/posts/cobbler/:3:0","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3.1 查看命令帮助 [root@localhost ~]# cobbler usage ===== cobbler \u003cdistro|profile|system|repo|image|mgmtclass|package|file\u003e ... [add|edit|copy|getks*|list|remove|rename|report] [options|--help] cobbler \u003caclsetup|buildiso|import|list|replicate|report|reposync|sync|validateks|version|signature|get-loaders|hardlink\u003e [options|--help] [root@linux-node1 ~]# cobbler import --help # 导入镜像 Usage: cobbler [options] Options: -h, --help show this help message and exit --arch=ARCH OS architecture being imported --breed=BREED the breed being imported --os-version=OS_VERSION the version being imported --path=PATH local path or rsync location --name=NAME name, ex 'RHEL-5' --available-as=AVAILABLE_AS tree is here, don't mirror --kickstart=KICKSTART_FILE assign this kickstart file --rsync-flags=RSYNC_FLAGS pass additional flags to rsync cobbler check 核对当前设置是否有问题 cobbler list 列出所有的cobbler元素 cobbler report 列出元素的详细信息 cobbler sync 同步配置到数据目录,更改配置最好都要执行下 cobbler reposync 同步yum仓库 cobbler distro 查看导入的发行版系统信息 cobbler system 查看添加的系统信息 cobbler profile 查看配置信息 ","date":"2021-06-12","objectID":"/posts/cobbler/:3:1","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3.2 导入镜像 # 挂载镜像 [root@localhost ~]# mount /dev/cdrom /mnt mount: /dev/sr0 is write-protected, mounting read-only [root@localhost ~]# cobbler import --path=/mnt/ --name=BClinux-7.2-x86_64 --arch=x86_64 # --path 镜像路径 # --name 为安装源定义一个名字 # --arch 指定安装源是32位、64位、ia64, 目前支持的选项有: x86│x86_64│ia64 # 安装源的唯一标示就是根据name参数来定义，本例导入成功后，安装源的唯一标示就是：BClinux-7.2-x86_64，如果重复，系统会提示导入失败。 # 镜像存放目录，cobbler会将镜像中的所有安装文件拷贝到本地一份，放在/var/www/cobbler/ks_mirror下的BClinux-7.2-x86_64目录下。因此/var/www/cobbler目录必须具有足够容纳安装文件的空间。 [root@localhost ~]# cobbler distro list # 查看镜像列表 BClinux-7.2-x86_64 ","date":"2021-06-12","objectID":"/posts/cobbler/:3:2","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3.3 指定ks.cfg文件及调整内核参数 [root@localhost ~]# cd /var/lib/cobbler/kickstarts/ # Cobbler的ks.cfg文件存放位置 [root@localhost kickstarts]# ls # 自带很多 default.ks esxi5-ks.cfg legacy.ks sample_autoyast.xml sample_esx4.ks sample_esxi5.ks sample_old.seed esxi4-ks.cfg install_profiles pxerescue.ks sample_end.ks sample_esxi4.ks sample.ks sample.seed # 上传准备好的ks文件(anaconda-ks.cfg),上传方式自己选择（sftp,U盘...) [root@localhost kickstarts]# mv anaconda-ks.cfg BClinux-7.2-x86_64.cfg # 在第一次导入系统镜像后，Cobbler会给镜像指定一个默认的kickstart自动安装文件在/var/lib/cobbler/kickstarts下的sample_end.ks。 [root@localhost kickstarts]# cobbler profile report --name=BClinux-7.2-x86_64 Name : BClinux-7.2-x86_64 TFTP Boot Files : {} Comment : DHCP Tag : default Distribution : BClinux-7.2-x86_64 Enable gPXE? : 0 Enable PXE Menu? : 1 Fetchable Files : {} Kernel Options : {} Kernel Options (Post Install) : {} Kickstart : /var/lib/cobbler/kickstarts/sample_end.ks # 默认的ks文件 Kickstart Metadata : {} Management Classes : [] Management Parameters : \u003c\u003cinherit\u003e\u003e Name Servers : [] Name Servers Search Path : [] Owners : ['admin'] Parent Profile : Internal proxy : Red Hat Management Key : \u003c\u003cinherit\u003e\u003e Red Hat Management Server : \u003c\u003cinherit\u003e\u003e Repos : [] Server Override : \u003c\u003cinherit\u003e\u003e Template Files : {} Virt Auto Boot : 1 Virt Bridge : xenbr0 Virt CPUs : 1 Virt Disk Driver Type : raw Virt File Size(GB) : 5 Virt Path : Virt RAM (MB) : 512 Virt Type : kvm # 编辑profile，修改关联的ks文件 [root@localhost kickstarts]# cobbler profile edit --name=BClinux-7.2-x86_64 --kickstart=/var/lib/cobbler/kickstarts/BClinux-7.2-x86_64.cfg # 修改安装系统的内核参数，在CentOS7系统有一个地方变了，就是网卡名变成eno16777736这种形式，但是为了运维标准化，我们需要将它变成我们常用的eth0，因此使用下面的参数。但要注意是CentOS7才需要下面的步骤，CentOS6不需要。（改之前请确认是否需要修改，如不需要请跳过） [root@localhost kickstarts]# cobbler profile edit --name=BClinux-7.2-x86_64 --kopts='net.ifnames=0 biosdevname=0' [root@localhost kickstarts]# cobbler profile report CentOS-7.1-x86_64 Name : BClinux-7.2-x86_64 TFTP Boot Files : {} Comment : DHCP Tag : default Distribution : BClinux-7.2-x86_64 Enable gPXE? : 0 Enable PXE Menu? : 1 Fetchable Files : {} Kernel Options : {'biosdevname': '0', 'net.ifnames': '0'} Kernel Options (Post Install) : {} Kickstart : /var/lib/cobbler/kickstarts/BClinux-7.2-x86_64.cfg Kickstart Metadata : {} Management Classes : [] Management Parameters : \u003c\u003cinherit\u003e\u003e Name Servers : [] Name Servers Search Path : [] Owners : ['admin'] Parent Profile : Internal proxy : Red Hat Management Key : \u003c\u003cinherit\u003e\u003e Red Hat Management Server : \u003c\u003cinherit\u003e\u003e Repos : [] Server Override : \u003c\u003cinherit\u003e\u003e Template Files : {} Virt Auto Boot : 1 Virt Bridge : xenbr0 Virt CPUs : 1 Virt Disk Driver Type : raw Virt File Size(GB) : 5 Virt Path : Virt RAM (MB) : 512 Virt Type : kvm # 每次修改完都要同步一次 [root@localhost kickstarts]# cobbler sync ","date":"2021-06-12","objectID":"/posts/cobbler/:3:3","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3.4 安装系统 可以很愉快的告诉你到这里就可以安装系统了！ 修改Cobbler提示 非必须，不想修改请跳过直接开始安装系统 [root@localhost ~]# vim /etc/cobbler/pxe/pxedefault.template MENU TITLE Cobbler | http://wglee.org # 此处的网址可以修改为你公司的网址 [root@localhost ~]# cobbler sync # 修改配置都要同步 OK，选择第二项就可以开始装机了。 ","date":"2021-06-12","objectID":"/posts/cobbler/:3:4","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":null,"content":"3.5 ks.cfg 文件 关于ks.cfg文件详细说明请查看：ks.cfg 文件配置 文档 [root@localhost kickstarts]# cat BClinux-7.2-x86_64.cfg #version=DEVEL install # System authorization information auth --enableshadow --passalgo=sha512 # Use Network installation url --url=$tree # 这些$开头的变量都是调用配置文件里的值。 # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda # Keyboard layouts keyboard --vckeymap=us --xlayouts='us' # System language lang en_US.UTF-8 # Network information network --bootproto=dhcp --onboot=on network --hostname=localhost.localdomain # Root password rootpw --iscrypted $default_password_crypted # $开头的变量，调用配置文件里的值。 # System services services --disabled=\"chronyd\" firewall --disabled selinux --disabled # Reboot after installation reboot # System timezone timezone Asia/Shanghai --isUtc # System bootloader configuration bootloader --append=\" crashkernel=auto\" --location=mbr --boot-drive=sda # Partition clearing information clearpart --none --initlabel # Disk partitioning information part /boot --fstype=\"ext4\" --ondisk=sda --size=500 part pv.01 --fstype=\"lvmpv\" --ondisk=sda --grow --size=1 volgroup bclinux pv.01 logvol / --fstype=\"xfs\" --size=10240 --name=root --vgname=bclinux logvol swap --fstype=\"swap\" --size=1024 --name=swap --vgname=bclinux %packages @^minimal @core @security-tools kexec-tools vim wget %end %addon com_redhat_kdump --enable --reserve-mb='auto' %end %post wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo %end ","date":"2021-06-12","objectID":"/posts/cobbler/:3:5","tags":null,"title":"使用 cobbler 批量部署 bclinux7.2","uri":"/posts/cobbler/"},{"categories":["ansible"],"content":"playbook 示例1 使用 ansible-playbook (单文件)批量安装 vsftp 服务 playbook ---- hosts:all # 指定操作的主机vars:# 定义变量，此变量会传入模板userlist:/etc/vsftpd/user_listwelcome:/etc/vsftpd/welcome.txtremote_user:roottasks:- name:install vsftpdyum:pkg=vsftpd state=latest- name:write vsftp config filetemplate:src=/root/playbook/templates/vsftpd.conf dest=/etc/vsftpd/vsftpd.conf# 只有配置发生变化才会触发以下任务notify:# 触发任务，下面指定任务名- restart vsftpd- name:start vsftpdservice:name=vsftpd state=startedhandlers:# 定义触发任务- name:restart vsftpdservice:name=vsftpd state=restarted templates 模板 anonymous_enable=NO local_enable=YES write_enable=YES local_umask=022 local_root=/var/ftp userlist_enable=YES userlist_deny=YES userlist_file={{ userlist }} use_localtime=YES dirmessage_enable=YES xferlog_enable=YES connect_from_port_20=YES xferlog_std_format=YES listen=YES pam_service_name=vsftpd tcp_wrappers=YES banner_file={{ welcome }} chroot_local_user=YES pasv_min_port=65530 pasv_max_port=65535 ","date":"2021-06-12","objectID":"/posts/ansible-playbook/:1:0","tags":["ansible"],"title":"Ansible Playbook 示例","uri":"/posts/ansible-playbook/"},{"categories":["ansible"],"content":"playbook 示例2 将 playbook 分解成多个文件方便复用，文件夹结构如下： [root@localhost playbook]# tree . ├── handlers │ └── restart.yaml ├── services │ └── vsftpd.yaml ├── templates │ └── vsftpd.conf └── vsftpd.yaml vsftpd.yaml --- - hosts: all remote_user: root vars: userlist: /etc/vsftpd/user_list welcome: /etc/vsftpd/welcome.txt tasks: - include: /root/playbook/services/vsftpd.yaml handlers: - include: /root/playbook/handlers/restart.yaml server_name=vsftpd services/vsftpd.yaml --- - name: install vsftpd yum: name=vsftpd state=present - name: start vsftpd service: name=vsftpd state=started - name: configure vsftpd template: src=/root/playbook/templates/vsftpd.conf dest=/etc/vsftpd/vsftpd.conf notify: - restart vsftpd templates/vsftpd.conf anonymous_enable=NO local_enable=YES write_enable=YES local_umask=022 local_root=/var/ftp userlist_enable=YES userlist_deny=YES userlist_file={{ userlist }} use_localtime=YES dirmessage_enable=YES xferlog_enable=YES connect_from_port_20=YES xferlog_std_format=YES listen=YES pam_service_name=vsftpd tcp_wrappers=YES banner_file={{ welcome }} chroot_local_user=YES pasv_min_port=65530 pasv_max_port=65535 handlers/restart.yaml --- - name: restart {{ server_name }} service: name={{ server_name }} state=restarted ","date":"2021-06-12","objectID":"/posts/ansible-playbook/:2:0","tags":["ansible"],"title":"Ansible Playbook 示例","uri":"/posts/ansible-playbook/"},{"categories":["ansible"],"content":"ansible 命令格式： ansible all -m command -a \"uptime\" -m 指定使用的模块 -a 指定模块的参数 默认使用 /etc/ansible/hosts 文件中定义的主机，也可以使用 -i /path/hosts 主机清单文件的位置 ","date":"2021-06-12","objectID":"/posts/ansible-modules/:0:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"1. command command 命令模块, 不支持管道 “|” 与 变量 [root@localhost ~]# ansible all -m command -a 'uptime' 192.168.17.130 | SUCCESS | rc=0 \u003e\u003e 03:05:54 up 36 min, 3 users, load average: 0.02, 0.04, 0.05 192.168.17.131 | SUCCESS | rc=0 \u003e\u003e 03:05:54 up 32 min, 3 users, load average: 0.01, 0.03, 0.03 ","date":"2021-06-12","objectID":"/posts/ansible-modules/:1:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"2. shell shell 功能基本与 command 类似,但是 shell 使用远程主机的 /bin/sh 运行命令,支持管道 。 [root@localhost ~]# ansible webserver -m shell -a '/tmp/test.sh' 192.168.17.131 | SUCCESS | rc=0 \u003e\u003e UserName: root Ip address: 192.168.17.131 192.168.17.130 | SUCCESS | rc=0 \u003e\u003e UserName: root Ip address: 192.168.17.130 ","date":"2021-06-12","objectID":"/posts/ansible-modules/:2:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"3. script script 是对指定的远程主机执行\"本地脚本\"，执行\"远程脚本\"请使用 shell 模块 [root@localhost ~]# ls anaconda-ks.cfg test.sh [root@localhost ~]# ansible webserver -m script -a '/root/test.sh' 192.168.17.131 | SUCCESS =\u003e { \"changed\": true, \"rc\": 0, \"stderr\": \"Shared connection to 192.168.17.131 closed.\\r\\n\", \"stdout\": \"UserName: root\\r\\nIp address: 192.168.17.131\\r\\n\", \"stdout_lines\": [ \"UserName: root\", \"Ip address: 192.168.17.131\" ] } 192.168.17.130 | SUCCESS =\u003e { \"changed\": true, \"rc\": 0, \"stderr\": \"Shared connection to 192.168.17.130 closed.\\r\\n\", \"stdout\": \"UserName: root\\r\\nIp address: 192.168.17.130\\r\\n\", \"stdout_lines\": [ \"UserName: root\", \"Ip address: 192.168.17.130\" ] } ","date":"2021-06-12","objectID":"/posts/ansible-modules/:3:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"4. copy [root@localhost ~]# ansible webserver -m copy -a 'src=/root/test.sh dest=/tmp/test.sh owner=root group=root mode=0755' 192.168.17.131 | SUCCESS =\u003e { \"changed\": true, \"checksum\": \"15bd17b3f24fe74c6b2bd515469510a7c46423c9\", \"dest\": \"/tmp/test.sh\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"ef47fd4c2ae9500503156fea0fa11a12\", \"mode\": \"0644\", \"owner\": \"root\", \"size\": 150, \"src\": \"/root/.ansible/tmp/ansible-tmp-1498789225.9-220641470038608/source\", \"state\": \"file\", \"uid\": 0 } 192.168.17.130 | SUCCESS =\u003e { \"changed\": true, \"checksum\": \"15bd17b3f24fe74c6b2bd515469510a7c46423c9\", \"dest\": \"/tmp/test.sh\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"ef47fd4c2ae9500503156fea0fa11a12\", \"mode\": \"0644\", \"owner\": \"root\", \"size\": 150, \"src\": \"/root/.ansible/tmp/ansible-tmp-1498789225.91-98116970360767/source\", \"state\": \"file\", \"uid\": 0 } ","date":"2021-06-12","objectID":"/posts/ansible-modules/:4:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"5. stat 获取远程文件状态信息，包括：atime ctime mtime md5 uid git 等信息 [root@localhost ~]# ansible webserver -m stat -a 'path=/etc/hosts' ","date":"2021-06-12","objectID":"/posts/ansible-modules/:5:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"6. get_url 实现远程主机下载指定的 url 到本地 [root@localhost ~]# ansible all -m get_url -a 'url=http://www.baidu.com dest=/tmp/baidu.html mode=400 force=yes' ","date":"2021-06-12","objectID":"/posts/ansible-modules/:6:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"7. yum linux 平台软件包管理工具操作模块，常见的有 yum apt [root@localhost ~]# ansible all -m yum -a 'name=curl state=latest' [root@localhost ~]# ansible all -m apt -a 'pkg=curl state=latest' latest 表示最新版本 ","date":"2021-06-12","objectID":"/posts/ansible-modules/:7:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"8. cron 远程主机 crontab 配置 [root@localhost ~]# ansible all -m cron -a 'name=\"check dirs\" hour=\"5,2\" job=\"ls -lh\"' 效果如下 [root@localhost ~]# crontab -l #Ansible: check dirs * 5,2 * * * ls -lh ","date":"2021-06-12","objectID":"/posts/ansible-modules/:8:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"9. mount 远程主机分区挂载 [root@localhost ~]# ansible all -m mount -a 'name=/mnt src=/dev/sdb3 fstype=ext3 opts=ro state=present' ","date":"2021-06-12","objectID":"/posts/ansible-modules/:9:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"10. service 远程主机系统服务管理 [root@localhost ~]# ansible all -m service -a 'name=crond state=stopped' [root@localhost ~]# ansible all -m service -a 'name=crond state=started' [root@localhost ~]# ansible all -m service -a 'name=crond state=restarted' [root@localhost ~]# ansible all -m service -a 'name=crond state=reloaded' stopped 停止 started 启动 restarted 重启 reloaded 重载 ","date":"2021-06-12","objectID":"/posts/ansible-modules/:10:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"11. sysctl 远程主机 sysctl 配置 [root@localhost ~]# ansible all -m sysctl -a 'name=net.ipv4.ip_forward value=1 sysctl_file=/etc/sysctl.conf reload=yes' ","date":"2021-06-12","objectID":"/posts/ansible-modules/:11:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"12. user 远程主机用户管理 # 添加用户 [root@localhost ~]# ansible all -m user -a 'name=liwg shell=/sbin/bash home=/home/liwg' # 删除用户 [root@localhost ~]# ansible all -m user -a 'name=liwg state=absent remove=yes' ","date":"2021-06-12","objectID":"/posts/ansible-modules/:12:0","tags":["ansible"],"title":"Ansible 常用模块","uri":"/posts/ansible-modules/"},{"categories":["ansible"],"content":"目录结构 playbook 目录包括变量定义目录 group_vars、主机组定义文件hosts、全局配置文件site.yml、角色功能目录。 可以使用命令 ansible-galaxy init role_name 生成角色相应目录 [root@localhost ~]# tree ansible_roles/ ansible_roles/ ├── common # 公共角色目录 │ ├── handlers │ │ └── main.yml # 定义触发任务 │ ├── tasks │ │ └── main.yml # 定义任务 │ ├── templates │ │ └── ntp.conf.j2 # 定义模板 │ └── vars │ └── main.yml # 定义角色内的变量 ├── ftp # 安装ftp任务角色 │ ├── handlers │ │ └── main.yml │ ├── tasks │ │ └── main.yml │ ├── templates │ │ └── vsftpd2.conf │ └── vars ├── group_vars # 定义组变量，以组名命名文件 │ ├── all │ └── ftpservers # 定义 ftpservers 组主机变量 ├── hosts # 主机清单，非必须，可以使用默认。使用时加 -i 选项 └── site.yml # 程序入口文件 ","date":"2021-06-12","objectID":"/posts/ansible-role/:1:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"角色规范 角色定制以下规范，其中 x 为角色名。 如 roles/x/tasks/main.yml 文件存在，其中列出的任务将被添加到执行队列； 如 roles/x/handlers/main.yml 文件存在，其中所列的处理程序将被添加到执行队列； 如 roles/x/vars/main.yml 文件存在，其中列出的变量将被添加到执行队列； 如 roles/x/meta/main.yml 文件存在，所列任何作用的依赖关系将被添加到角色的列表（1.3及更高版本）； 任何副本任务可以引用 roles/x/files/无需写路径，默认相对或绝对引用； 任何模板任务可以引用文件中的 roles/x/templates/ 无需写路径，默认相对或绝对引用。 ","date":"2021-06-12","objectID":"/posts/ansible-role/:2:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"hosts 主机配置文件 [root@localhost ansible_roles]# cat hosts manages ansible_connection=local ansible_ssh_host=127.0.0.1 [ftpservers] 192.168.17.130 192.168.17.131 ","date":"2021-06-12","objectID":"/posts/ansible-role/:3:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"group_vars 定义主机组变量 group_vars/ftpservers 此配置文件内变量只对 ftpservers 组内的主机有效 [root@localhost ansible_roles]# cat group_vars/ftpservers userlist: /etc/vsftpd/user_list welcome: /etc/vsftpd/welcome.txt group_vars/all 对所有主机有效 [root@localhost ansible_roles]# cat group_vars/all ntpserver: ntp.sjtu.edu.cn ","date":"2021-06-12","objectID":"/posts/ansible-role/:4:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"site.yml 全局配置文件，程序入口 [root@localhost ansible_roles]# cat site.yml --- # 任务名 - name: apply common configuration to all nodes hosts: all # 任务操作主机或主机组 roles: # 指定任务角色 - common # 运行 common角色 - name: configure and deploy the ftpservers hosts: ftpservers roles: - ftp 全局配置文件 site.yml 引用了两个角色，一个为公共类的 common，另一个为 ftp 类，分别对应 nginx/common、nginx/web 目录。以此类推，可以引用更多的角色，如 db、nosql、hadoop 等，前提是我们先要进行定义，通常情况下一个角色对应着一个特定功能服务。通过 hosts 参数来绑定角色对应的主机或组. ","date":"2021-06-12","objectID":"/posts/ansible-role/:5:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"common (公共)角色文件示例 角色common定义了handlers、tasks、templates、vars 4个功能类，分别存放处理程序、任务列表、模板、变量的配置文件main.yml，需要注意的是，vars/main.yml中定义的变量优先级高于/nginx/group_vars/all，可以从ansible-playbook的执行结果中得到验证。各功能块配置文件定义如下： common/tasks/main.yml [root@localhost ansible_roles]# cat common/tasks/main.yml --- - name: install ntp yum: name=ntp state=present - name: configure ntp file template: src=ntp.conf.j2 dest=/etc/ntp.conf notify: restart ntp - name: start the ntp service service: name=ntpd state=started enabled=true - name: test to see if selinux is running command: getenforce register: sestatus changed_when: false common/handlers/main.yml [root@localhost ansible_roles]# cat common/handlers/main.yml --- - name: restart ntp service: name=ntpd state=restarted common/templates/main.yml [root@localhost ansible_roles]# cat common/templates/ntp.conf.j2 driftfile /var/lib/ntp/drift restrict 127.0.0.1 restrict -6 ::1 server {{ ntpserver }} includefile /etc/ntp/crypto/pw keys /etc/ntp/keys {{ ntpserver }} 变量将从 common/vars/main.yml 中获取 common/vars/main.yml [root@localhost ansible_roles]# cat common/vars/main.yml ntpserver: 210.72.145.44 Tips: 此处定义的变量优先级要高于 group_vars 中定义的变量 ","date":"2021-06-12","objectID":"/posts/ansible-role/:6:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"ftp 角色文件示例 ftp/tasks/main.yml [root@localhost ansible_roles]# cat ftp/tasks/main.yml - name: install vsftpd yum: pkg=vsftpd state=latest - name: write the vsftpd config file template: src=vsftpd2.conf dest=/etc/vsftpd/vsftpd.conf notify: - restart vsftpd - name: start vsftpd service: name=vsftpd state=started ftp/handlers/main.yml [root@localhost ansible_roles]# cat ftp/handlers/main.yml --- - name: restart vsftpd service: name=vsftpd state=restarted ftp/templates/vsftpd2.conf [root@localhost ansible_roles]# cat ftp/templates/vsftpd2.conf anonymous_enable=NO local_enable=YES write_enable=YES local_umask=022 local_root=/var/ftp userlist_enable=YES userlist_deny=YES userlist_file={{ userlist }} use_localtime=YES dirmessage_enable=YES xferlog_enable=YES connect_from_port_20=YES xferlog_std_format=YES listen=YES pam_service_name=vsftpd tcp_wrappers=YES banner_file={{ welcome }} chroot_local_user=YES pasv_min_port=65530 pasv_max_port=65535 ","date":"2021-06-12","objectID":"/posts/ansible-role/:7:0","tags":["ansible"],"title":"Ansible playbook 角色","uri":"/posts/ansible-role/"},{"categories":["ansible"],"content":"主机与组基本配置 ansible 默认使用的主机配置文件路径为 /etc/ansible/hosts，使用 ini 文件格式，主机可以使用域名，IP，别名进行标识。 mail.example.com 192.168.1.10 [webserver] 192.168.1.11 192.168.1.12 [dbserver] 192.168.1.13:7733 其中 192.168.1.13:7733 的意思是定义一个ssh服务端口为7733的主机。 有时我们也可以使用别名的方式来描述一台主机 db1 ansible_ssh_port=4422 ansible_ssh_host=192.168.1.14 db1 为定义一个别名，ansible_ssh_port 为主机 ssh 端口，ansible_ssh_host 为主机 ip 地址，更多保留主机变量如下： ansible_ssh_host，连接目标主机的地址。 ansible_ssh_port，连接目标主机SSH端口，端口22无需指定。 ansible_ssh_user，连接目标主机默认用户。 ansible_ssh_pass，连接目标主机默认用户密码。 ansible_connection，目标主机连接类型，可以是local、ssh或paramiko。 ansible_ssh_private_key_file, 连接目标主机的ssh私钥。 ansible_*_interpreter，指定采用非Python的其他脚本语言，如 Ruby、Perl或其他类似 ansible_python_interpreter 解释器。 当然正则也是可以使用的 [webservers] www[01:50].example.com [databases] db-[a:f].example.com [01:50] 表示匹配 01 至 50 所有主机 [a:f] 表示匹配 a 至 f 当中所有的字母 ","date":"2021-06-12","objectID":"/posts/ansible-hosts/:1:0","tags":["ansible"],"title":"Ansible 资源配置清单","uri":"/posts/ansible-hosts/"},{"categories":["ansible"],"content":"定义主机变量 主机可以指定变量，以便后面供Playbooks配置使用，比如定义主机 hosts1 及 hosts2 上 Apache 参数 http_port 及 maxRequestsPerChild ，目的是让两台主机产生 Apache 配置文件 httpd.conf 差异化，定义格式如下： [atlanta] host1 http_port=80 maxRequestsPerChild=808 host2 http_port=303 maxRequestsPerChild=909 ","date":"2021-06-12","objectID":"/posts/ansible-hosts/:2:0","tags":["ansible"],"title":"Ansible 资源配置清单","uri":"/posts/ansible-hosts/"},{"categories":["ansible"],"content":"定义组变量 组变量的作用域是覆盖组所有成员，通过定义一个新块，块名由组名+“:vars”组成，定义格式如下： [atlanta] host1 host2 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com ","date":"2021-06-12","objectID":"/posts/ansible-hosts/:3:0","tags":["ansible"],"title":"Ansible 资源配置清单","uri":"/posts/ansible-hosts/"},{"categories":["ansible"],"content":"组嵌套组 同时 Ansible 支持组嵌套组，通过定义一个新块，块名由组名+\":children\"组成，格式如下： [atlanta] host1 host2 [raleigh] host2 host3 [southeast:children] atlanta raleigh [southeast:vars] some_server=foo.southeast.example.com halon_system_timeout=30 self_destruct_countdown=60 escape_pods=2 [usa:children] southeast northeast southwest southeast 嵌套组只能使用在 /usr/bin/ansible-playbook 中，在 /usr/bin/ansible 中不起作用。 ","date":"2021-06-12","objectID":"/posts/ansible-hosts/:4:0","tags":["ansible"],"title":"Ansible 资源配置清单","uri":"/posts/ansible-hosts/"},{"categories":["rsync"],"content":"需求 有两台 A, B 服务器其中有个目录需要实时双向同步，即 服务器A 目录添加或删除文件需同步给 服务器B，同理 服务器B 也一样 ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:0:0","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"安装 yum install epel-release yum install lsyncd ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:1:0","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"rsync ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:2:0","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"rsyncd.conf 配置示例 以下给出其中一台服务器的配置，另一台只需要修改下 hosts allow 配置即可， 配置文件 /etc/rsyncd.conf uid = nobody gid = nobody use chroot = no max connections = 10 strict modes = yes pid file = /var/run/rsyncd.pid lock file = /var/run/rsync.lock log file = /data/rsync/rsyncd.log [pu] path = /data/www/platform_admin/Uploads comment = platform uploads ignore errors read only = no write only = no hosts allow = 10.100.1.16 hosts deny = * list = false uid = www gid = www 注意用户权限 ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:2:1","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"启动 rsyncd 服务 systemctl start rsyncd.service systemctl enable rsyncd.service ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:2:2","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"配置密钥 ssh-keygen -t rsa -C rsync ssh-copy-id -i ~/.ssh/id_rsa.pub localhost ssh-copy-id -i ~/.ssh/id_rsa.pub 10.100.1.16 scp -r .ssh 10.100.1.16:/root/ 注意: 将公钥加两台机的 ~/.ssh/authorized_keys 文件中，并复制私钥至另一台的 ~/.ssh 目录下 ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:2:3","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"lsyncd Lysncd 实际上是 lua 语言封装了 inotify 和 rsync 工具，采用了 Linux 内核（2.6.13 及以后）里的 inotify 触发机制，然后通过 rsync 去差异同步，达到实时的效果。我认为它最令人称道的特性是，完美解决了 inotify + rsync 海量文件同步带来的文件频繁发送文件列表的问题 —— 通过时间延迟或累计触发事件次数实现。另外，它的配置方式很简单，lua 本身就是一种配置语言，可读性非常强。lsyncd 也有多种工作模式可以选择，本地目录 cp，本地目录 rsync，远程目录 rsyncssh 。 ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:3:0","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"lsyncd.conf 配置 编辑配置文件 /etc/lsyncd.conf ---- -- User configuration file for lsyncd. -- -- Simple example for default rsync, but executing moves through on the target. -- -- For more examples, see /usr/share/doc/lsyncd*/examples/ -- -- sync{default.rsyncssh, source=\"/var/www/html\", host=\"localhost\", targetdir=\"/tmp/htmlcopy/\"} settings { logfile =\"/var/log/lsyncd/lsyncd.log\", statusFile =\"/var/local/lsyncd.status\", inotifyMode = \"CloseWrite\", maxProcesses = 7, -- nodaemon =true, } sync { default.rsync, source = \"/data/www/platform_admin/Uploads\", target = \"www@10.100.1.16::pu\", rsync = { binary = \"/usr/bin/rsync\", archive = true, compress = true, verbose = true } } ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:3:1","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"启动 lsyncd 服务 systemctl start lsyncd systemctl enable lsyncd ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:3:2","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["rsync"],"content":"测试 只需要在其中一台服务器的 /data/www/platform_admin/Uploads 中添加文件，然后在另一台服务器查看是否有同步过来，最后在颠倒顺序测试即可。 ","date":"2021-06-12","objectID":"/posts/rsync-lsyncd/:4:0","tags":["rsync","lsyncd"],"title":"lsyncd 配合 rsync 实现目录实时双向同步","uri":"/posts/rsync-lsyncd/"},{"categories":["cli"],"content":"基本语法 格式 sed [option]... 'script' inputfile... 常用选项： -n: 不输出模式空间内容到屏幕，即不自动打印 -e: 多点编辑 -f: 从指定的文件中读取编辑脚本 -r: 支持扩展正则表达式 -i.bak: 备份文件并原处编辑 (.bak 字符是自定义的) ","date":"2021-06-12","objectID":"/posts/sed/:1:0","tags":["sed"],"title":"Linux 文本三剑客：sed","uri":"/posts/sed/"},{"categories":["cli"],"content":"示例 打印文件的最后一行 sed -n '$p' /etc/passwd $ 表示最后一行 打印第2行及以下4行 seq 10 | sed -n '2,+4p' 打印第2行到第4行 seq 10 | sed -n '2,4p' 查找文件中指定字符串行 sed -n '/^auth/p' /etc/pam.d/su 判断 /etc/pam.d/su 文件中是否有 auth required pam_securetty.so 配置行 修改行 sed -i.bak 's/PermitRoot*/PermitRoot no/g' /etc/ssh/sshd_config -i.bak 在修改文件时会先备份，本例备份文件名为 sshd_config.bak 删除行 sed -i '/^PATH/d' /etc/profile ","date":"2021-06-12","objectID":"/posts/sed/:2:0","tags":["sed"],"title":"Linux 文本三剑客：sed","uri":"/posts/sed/"},{"categories":["cli"],"content":"查看 rename 命令帮助信息 [root@localhost ~]# rename --help Usage: rename [options] expression replacement file... rename \u003c要替换的字符\u003e \u003c替换后的字符\u003e \u003c要修改的文件（可以使用通配符批量操作）\u003e Options: -v, --verbose explain what is being done -s, --symlink act on symlink target -h, --help display this help and exit -V, --version output version information and exit 示例 批量将 file 开头的文件, 由 file 改为 linux [root@localhost tmp]# ls file* file1 file2 file3 file4 file5 [root@localhost tmp]# rename file linux file* [root@localhost tmp]# ls linux* linux1 linux2 linux3 linux4 linux5 ","date":"2021-06-12","objectID":"/posts/rename/:0:0","tags":["rename"],"title":"利用 rename 批量重命名","uri":"/posts/rename/"},{"categories":["cli"],"content":" 官方站点：https://iperf.fr/ 支持平台： windows linux macOS unix 安装方式可以选择二进制文件安装，也可以源码编译安装 ./configure \u0026\u0026 make \u0026\u0026 make install iperf 需要两台服务器，一台作服务器，一台作客户端，默认监听 5201 端口 ","date":"2021-06-12","objectID":"/posts/iperf/:0:0","tags":["iperf"],"title":"使用 iperf 进行网络性能评估","uri":"/posts/iperf/"},{"categories":["cli"],"content":"1、启动服务器 iperf3 -s -D 说明： -s 以服务的方式运行， -D 以守护进程的方式运行 ","date":"2021-06-12","objectID":"/posts/iperf/:1:0","tags":["iperf"],"title":"使用 iperf 进行网络性能评估","uri":"/posts/iperf/"},{"categories":["cli"],"content":"2、客户端连接测试 iperf3 -c iperf3_server_ipaddress 说明： -c 以客户端的方式运行测试 客户端选项 -c 以客户端的方式运行 -u 使用udp协议 -b [K|M|G] 指定udp使用的带宽，单位bits/sec。 此选项与-u相关。默认值是1Mbits/sec -t 指定传输数据包的总时间，默认是10s -n [K|M|G] 指定传输数据包的字节数 -l 指定读写缓冲区的长度，TCP方式默认大小为8kb，udp方式默认大小为1470B -P 指定客户端与服务器端之间的线程数，默认是1个线程 -R 切换发送，接收模式，默认客户端发送，服务器端接收。设置此参数将反转。 -w 指定套接字缓冲区大小 -B 用来绑定一个主机地址或接口，适用于有多个网络接口的主机 -M 设置TCP最大信息段的值 -N 设置TCP无延时 客户端与服务器端共用选项 -f [k|m|g|K|M|G] 指定带宽输入单位 -p 指定服务器使用的端口或者客户端连接的端口 -i 指定每次报告间隔的时间 -F 指定文件作为数据流进行带宽测试 ","date":"2021-06-12","objectID":"/posts/iperf/:2:0","tags":["iperf"],"title":"使用 iperf 进行网络性能评估","uri":"/posts/iperf/"},{"categories":["cli"],"content":"首先我们知道通过添加 key 的方式可以实现 ssh 远程免密码执行命令，但是如果我们使用密码的方式该如何不提示输入密码进行 ssh 远程执行命令呢？ 答案就是通过使用 sshpass 工具来实现 ","date":"2021-06-12","objectID":"/posts/sshpass/:0:0","tags":["sshpass"],"title":"使用 sshpass 免密码远程执行命令","uri":"/posts/sshpass/"},{"categories":["cli"],"content":"1. 安装sshpass yum install sshpass Tips: 如果系统 yum 源没有 sshpass 包，可以添加阿里云的源。 阿里云开源站点 ","date":"2021-06-12","objectID":"/posts/sshpass/:1:0","tags":["sshpass"],"title":"使用 sshpass 免密码远程执行命令","uri":"/posts/sshpass/"},{"categories":["cli"],"content":"2. 示例 通过ssh远程登录来测试 sshpass 功能 # 本机ip地址 [root@localhost ~]# ifconfig eth0 eth0 Link encap:Ethernet HWaddr 00:0C:29:7B:7C:7E inet addr:192.168.92.132 Bcast:192.168.92.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe7b:7c7e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:15089 errors:0 dropped:0 overruns:0 frame:0 TX packets:8054 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:17225172 (16.4 MiB) TX bytes:797990 (779.2 KiB) Interrupt:19 Base address:0x2000 # 输入这条命令不会提示输入密码及确认添加 known_hosts [root@localhost ~]# sshpass -p liwanggui ssh root@192.168.92.133 -o StrictHostKeyChecking=no Last login: Sat Mar 11 07:28:41 2017 from 192.168.92.132 # ssh连接主机的ip地址 [root@localhost ~]# eth0 Link encap:Ethernet HWaddr 00:0C:29:F2:3A:2D inet addr:192.168.92.133 Bcast:192.168.92.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fef2:3a2d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:225 errors:0 dropped:0 overruns:0 frame:0 TX packets:105 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:23814 (23.2 KiB) TX bytes:14940 (14.5 KiB) Interrupt:19 Base address:0x2000 ","date":"2021-06-12","objectID":"/posts/sshpass/:2:0","tags":["sshpass"],"title":"使用 sshpass 免密码远程执行命令","uri":"/posts/sshpass/"},{"categories":["cli"],"content":"安装 parted 工具 [root@localhost ~]# yum install parted # 包含以下命令 [root@localhost ~]# rpm -ql parted | grep bin /sbin/parted # 分区工具 /sbin/partprobe # 分区表刷新工具 ","date":"2021-06-12","objectID":"/posts/parted/:1:0","tags":["parted"],"title":"使用 parted 对 gpt 磁盘分区","uri":"/posts/parted/"},{"categories":["cli"],"content":"使用 parted 分区 [root@localhost ~]# parted /dev/sdb GNU Parted 2.1 Using /dev/sdb Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mklabel gpt # 设置硬盘分区表 Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes (parted) mkpart # 开始分区 Partition name? []? 1 # 分区表名 File system type? [ext2]? ext4 # 文件系统类型 Start? 1 # 分区空间起始位置 End? 2G # 分区空间结束位置 (parted) mkpart Partition name? []? 2 File system type? [ext2]? ext4 Start? 2G # 这个位置是上分区的结束位置大小 End? 12G # 这个分区的大小加上起始位置就是结束位置大小 (parted) p Model: VMware, VMware Virtual S (scsi) Disk /dev/sdb: 21.5GB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 1049kB 2000MB 1999MB 1 2 2000MB 12.0GB 10.0GB 2 ","date":"2021-06-12","objectID":"/posts/parted/:2:0","tags":["parted"],"title":"使用 parted 对 gpt 磁盘分区","uri":"/posts/parted/"},{"categories":["cli"],"content":"脚本化分区 设置 /dev/sdb 分区表为 gpt, 将所有空间划为一个分区，分区名称为 d1 parted -s /dev/sdb mklabel gpt parted -s /dev/sdb mkpart d1 1 100% ","date":"2021-06-12","objectID":"/posts/parted/:3:0","tags":["parted"],"title":"使用 parted 对 gpt 磁盘分区","uri":"/posts/parted/"},{"categories":["cli"],"content":"Ncdu 是一个具有 ncurses 接口的磁盘使用率分析器。它的目的是在没有完整图形设置的远程服务器上查找空间占用者，但即使在常规桌面系统上，它也是一个有用的工具。 Ncdu 的目标是快速、简单和易于使用，并且应该能够在安装了 ncurses 的任何最小的类似 POSIX 的环境中运行。 软件官网地址：https://dev.yorhel.nl/ncdu ","date":"2021-06-12","objectID":"/posts/ncdu/:0:0","tags":["ncdu"],"title":"使用 ncdu 查找 linux 下最占空间的文件","uri":"/posts/ncdu/"},{"categories":["cli"],"content":"安装 yum install ncdu 你也可以使用源码包进行编译安装 wget https://dev.yorhel.nl/download/ncdu-1.15.1.tar.gz tar xzvf ncdu-1.15.1.tar.gz cd ncdu-1.15.1 ./configure make \u0026\u0026 make install ","date":"2021-06-12","objectID":"/posts/ncdu/:1:0","tags":["ncdu"],"title":"使用 ncdu 查找 linux 下最占空间的文件","uri":"/posts/ncdu/"},{"categories":["cli"],"content":"使用方法 ncdu [dirname] 提示: 按 j k 进行上下移动或者使用上下方向键，Enter 键进行 ","date":"2021-06-12","objectID":"/posts/ncdu/:2:0","tags":["ncdu"],"title":"使用 ncdu 查找 linux 下最占空间的文件","uri":"/posts/ncdu/"},{"categories":["kubernetes"],"content":"部署 Jenkins rbac.yaml 创建 ServiceAccount: jenkins-ci 授予 cluster-admin 权限， jenkins 在 kubernetes 集群中创建工作节点需要权限 你也可以在 kubernetes 插件中配置验证信息 apiVersion:v1kind:ServiceAccountmetadata:name:jenkins-cinamespace:devops---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:jenkins-cinamespace:devopsroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:adminsubjects:- kind:ServiceAccountname:jenkins-cinamespace:devops pvc.yaml 为 jenkins 划分一块存储，用于持久化 jenkins 数据 ---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:jenkins-datanamespace:devopsspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:5Gi jenkins-ci.yml ---apiVersion:apps/v1kind:Deploymentmetadata:name:jenkinsnamespace:devopsspec:selector:matchLabels:app:jenkinstemplate:metadata:labels:app:jenkinsspec:terminationGracePeriodSeconds:10securityContext:runAsUser:0containers:- name:jenkinsimage:jenkinsci/blueocean:1.24.6imagePullPolicy:IfNotPresentports:- containerPort:8080name:webprotocol:TCP- containerPort:50000name:agentprotocol:TCPlivenessProbe:httpGet:path:/loginport:8080initialDelaySeconds:60timeoutSeconds:5failureThreshold:12readinessProbe:httpGet:path:/loginport:8080initialDelaySeconds:60timeoutSeconds:5failureThreshold:12volumeMounts:- name:datamountPath:/var/jenkins_homevolumes:- name:datapersistentVolumeClaim:claimName:jenkins-data---apiVersion:v1kind:Servicemetadata:labels:app:jenkinsname:jenkinsnamespace:devopsspec:ports:- name:\"web\"port:8080protocol:TCPtargetPort:8080- name:\"agent\"port:50000protocol:TCPtargetPort:50000selector:app:jenkinstype:ClusterIP ingress-route.yml apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:jenkinsnamespace:devopsspec:entryPoints:- webroutes:- match:Host(`jenkins.host.com`)kind:Ruleservices:- name:jenkinsport:8080 应用资源配置清单 kubectl apply -f rbac.yaml kubectl apply -f pvc.yaml kubectl apply -f configmap.yml kubectl apply -f jenkins-ci.yml kubectl apply -f ingress-route.yml ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:1:0","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"配置 jenkins ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:2:0","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"首次登录配置 在浏览器打开 http://jenkins.host.com 开始配置 jenkins 下一步安装推荐插件即可，等待插件安装完成 创建管理用户 ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:2:1","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"配置管理节点 (kubernetes) kubernetes Cloud 通过 kubernetes 插件可以让 jenkins 在 kubernetes 集群以 pod 的方式运行工作节点，下面我们安装 kubernetes 插件, 安装完成后重启生效 现在我们配置 Jenkins 使用 Kubernetes Pod 运行管理节点 依次点击 系统管理 -\u003e 节点管理 -\u003e Configure Clouds -\u003e Add a new cloud 添加 kubernetes 集群 点击 Kubernetes Cloud details 配置 kubernetes 集群 Kubernetes 地址: https://kubernetes.default.svc Jenkins 地址: http://jenkins.devops.svc.cluster.local:8080 Jenkins 通道: jenkins.devops.svc.cluster.local:50000 点击 Save 保存 POD 模板 在配置 POD 模板之前，我们需要规划好需要此 POD 工作节点执行哪些操作？ 对项目进行编译 (以 Java 项目为例, 使用 maven 完成) 将编译好的项目制作成 Docker 镜像并推送至 harbor 仓库 (需要调用 docker 命令) 更新 kubernetes 资源配置清单，并应用至 kubernetes 集群中 (需要调用 kubectl 命令) 理清楚步骤后我们开始配置 POD 模板， 依次点击 系统管理 -\u003e 节点管理 -\u003e Configure Clouds -\u003e POD Template -\u003e 添加 POD 模板 名称: maven-3.6 命名空间: devops (默认就是这个；和 jenkins 部署所在 namespace 一致) 容器名: maven Docker 镜像: maven:3.6-openjdk-11 准备挂载 (mount) 资源 给 maven POD 分配一个存储用于缓存从互联网下载的的资源，以便重复利用减少网络带宽占用节约等待时间 maven-pvc.yaml apiVersion:v1kind:PersistentVolumeClaimmetadata:name:maven-datanamespace:devopsspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:5Gi 创建 PVC kubectl apply -f maven-pvc.yaml 配置 docker push 镜像时所需要的验证信息，并挂载至 pod 容器的 /root/.docker 目录 docker 验证配置以 configmap 的形式存放在 集群中: configmap.yaml apiVersion:v1data:config.json:\"{\\n\\t\\\"auths\\\": {\\n\\t\\t\\\"harbor.wfugui.com\\\": {\\n\\t\\t\\t\\\"auth\\\": \\\"YWRtaW46SGFyYm9yMTIzNDU=\\\"\\n\\t\\t}\\n\\t},\\n\\t\\\"HttpHeaders\\\":{\\n\\t\\t\\\"User-Agent\\\": \\\"Docker-Client/19.03.15 (linux)\\\"\\n\\t}\\n}\"kind:ConfigMapmetadata:creationTimestamp:nullname:docker-authnamespace:devops 创建 configmap kubectl apply -f configmap.yaml 点击添加卷，添加如下卷 Persistent Volume Claim 申请值: maven-data 挂载路径: /root/.m2 Config Map Volume: Config Map 名称: docker-auth 挂载路径: /root/.docker Host Path Volume: 主机路径: /var/run/docker.sock 挂载路径: /root/.docker Host Path Volume: 主机路径: /usr/bin/docker 挂载路径: /usr/bin/docker Host Path Volume: 主机路径: /usr/bin/kubectl 挂载路径: /usr/bin/kubectl 配置 Server Account 为 jenkins-ci ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:2:2","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"使用 Jenkins 实现 CI ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:3:0","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"准备测试项目 登录 http://git.host.com , 创建一个 test 组织, 克隆 https://github.com/liwanggui/spring-boot-helloworld 项目到 test 组织下 创建一个名 jenkins 的用户，将其加入到 test 组织中 项目文件结构 ├── Dockerfile # 使用此 Dockerfile build docker 镜像 ├── jenkins │ ├── deliver.sh # jenkins 流水线中会调用此脚本 │ ├── Jenkinsfile # jenkins 声明式流水线配置 │ └── k8s # 项目 kubernetes 资源配置清单 │ ├── deployment.yaml │ ├── ingress.yaml │ └── service.yaml ├── pom.xml ├── README.md └── src ├── main │ └── java │ └── hello │ ├── Application.java │ └── HelloController.java └── test └── java └── hello ├── HelloControllerIT.java └── HelloControllerTest.java ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:3:1","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"配置流水线 在 Jenkins 管理界面，点击新建任务创建一个名称为 spring-boot-helloworld 类型为流水线(pipeline)的任务 配置 spring-boot-helloworld 任务配置界面点击流水线，选择 Pipenline script from SCM SCM: Git Repository Url: http://gogs.devops.svc.cluster.local:3000/test/spring-boot-helloworld Credentials: 配置 git 仓库验证信息，使用上面创建的 jenkins 用户 脚本本路径: jenkins/Jenkinsfile 保存 ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:3:2","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"构建项目和查看结果 点击开始构建，在构建过程可以点进去查看实时输出日志信息 ","date":"2021-04-29","objectID":"/posts/kubernetes-jenkinsci/:3:3","tags":["kubernetes","jenkins"],"title":"使用 jenkins 实现 Kubernetes CI","uri":"/posts/kubernetes-jenkinsci/"},{"categories":["kubernetes"],"content":"部署 Gogs 互联网常用的 git 代码仓库管理软件有 gitlab, gogs, gitea(gogs 的克隆版) 等，本例为了简单点使用 gogs 作为 git 仓库管理工作部署在 kubernetes 集群中 提示: gitea 部署过和 gogs 基本一致 准备 gogs 资源部署清单，由于 git 代码仓库需要使用持久化存储，因此我们需要为 gogs 创建一个 pvc, gitops 资源统一放在 devops 名称空间下。 kubectl create namespace devops pvc.yaml apiVersion:v1kind:PersistentVolumeClaimmetadata:name:gogs-datanamespace:devopsspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:10Gi deployment.yaml apiVersion:apps/v1kind:Deploymentmetadata:name:gogsnamespace:devopsspec:selector:matchLabels:app:gogstemplate:metadata:labels:app:gogsspec:terminationGracePeriodSeconds:10# nodeSelector:# workrole: cicdsecurityContext:runAsUser:0containers:- name:gogsimage:gogs/gogs:0.12.3imagePullPolicy:IfNotPresentports:- containerPort:3000name:webprotocol:TCP- containerPort:22name:sshprotocol:TCPvolumeMounts:- name:datamountPath:/datavolumes:- name:datapersistentVolumeClaim:claimName:gogs-data---apiVersion:v1kind:Servicemetadata:labels:app:gogsname:gogsnamespace:devopsspec:ports:- name:\"web\"port:3000protocol:TCPtargetPort:3000- name:\"ssh\"port:22protocol:TCPtargetPort:22selector:app:gogstype:ClusterIP ingress-route.yaml apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:gogsnamespace:devopsspec:entryPoints:- webroutes:- match:Host(`git.host.com`)kind:Ruleservices:- name:gogsport:3000 应用资源清单 kubectl apply -f pvc.yaml kubectl apply -f deployment.yaml kubectl apply -f ingress-route.yaml ","date":"2021-04-29","objectID":"/posts/kubernetes-git/:1:0","tags":["kubernetes","gogs"],"title":"使用 Gogs 服务实现代码仓库管理","uri":"/posts/kubernetes-git/"},{"categories":["kubernetes"],"content":"配置 gogs 浏览器打开 http://git.host.com 进入 gogs 配置页面 点击 “立即安装” ，安装完成后，进入管理界面 ","date":"2021-04-29","objectID":"/posts/kubernetes-git/:2:0","tags":["kubernetes","gogs"],"title":"使用 Gogs 服务实现代码仓库管理","uri":"/posts/kubernetes-git/"},{"categories":["kubernetes"],"content":"访问 git 仓库 在集群内可以使用 gogs 的集群内部名称来进行访问: http://gogs.devops.svc.cluster.local:3000 在集群外直接使用域名访问即可 如需要使用 ssh 连接 git 仓库需要配置 traefik 的 ingressroutetcp 规则 ","date":"2021-04-29","objectID":"/posts/kubernetes-git/:3:0","tags":["kubernetes","gogs"],"title":"使用 Gogs 服务实现代码仓库管理","uri":"/posts/kubernetes-git/"},{"categories":["kubernetes"],"content":"简介 Kubernetes 支持动态申请 PV 和 PVC 的功能，但是 NFS 存储本身不支持这个功能，但是 NFS 存储又是非常常用的一种共享存储。nfs-subdir-external-provisioner 使得 NFS 具备对外提供动态 PV 的能力。 生成的目录为: ${namespace}-${pvcName}-${pvName} 官方主页: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner ","date":"2021-04-29","objectID":"/posts/kubernetes-nfs-client/:1:0","tags":["kubernetes","nfs-subdir-external-provisioner","nfs"],"title":"扩展 NFS 向 Kubernetes 提供动态 PVC 功能","uri":"/posts/kubernetes-nfs-client/"},{"categories":["kubernetes"],"content":"安装 ","date":"2021-04-29","objectID":"/posts/kubernetes-nfs-client/:2:0","tags":["kubernetes","nfs-subdir-external-provisioner","nfs"],"title":"扩展 NFS 向 Kubernetes 提供动态 PVC 功能","uri":"/posts/kubernetes-nfs-client/"},{"categories":["kubernetes"],"content":"部署 NFS 文件服务 部署过程请参考 Ubuntu Server 网络文件系统（NFS） ","date":"2021-04-29","objectID":"/posts/kubernetes-nfs-client/:2:1","tags":["kubernetes","nfs-subdir-external-provisioner","nfs"],"title":"扩展 NFS 向 Kubernetes 提供动态 PVC 功能","uri":"/posts/kubernetes-nfs-client/"},{"categories":["kubernetes"],"content":"部署 nfs-subdir-external-provisioner 部署使用的资源配置清单官方仓库有提供，地址: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/tree/master/deploy 提取 rbac.yaml 和 deployment.yaml 即可 rbac.yaml : https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/rbac.yaml apiVersion:v1kind:ServiceAccountmetadata:name:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:default---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:nfs-client-provisioner-runnerrules:- apiGroups:[\"\"]resources:[\"persistentvolumes\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"delete\"]- apiGroups:[\"\"]resources:[\"persistentvolumeclaims\"]verbs:[\"get\",\"list\",\"watch\",\"update\"]- apiGroups:[\"storage.k8s.io\"]resources:[\"storageclasses\"]verbs:[\"get\",\"list\",\"watch\"]- apiGroups:[\"\"]resources:[\"events\"]verbs:[\"create\",\"update\",\"patch\"]---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:run-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:ClusterRolename:nfs-client-provisioner-runnerapiGroup:rbac.authorization.k8s.io---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultrules:- apiGroups:[\"\"]resources:[\"endpoints\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"update\",\"patch\"]---kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultsubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:Rolename:leader-locking-nfs-client-provisionerapiGroup:rbac.authorization.k8s.io deployment.yaml :https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/deployment.yaml 注意: 配置文件中的 nfs server 地址及目录路径需要修改为自己的 apiVersion:apps/v1kind:Deploymentmetadata:name:nfs-client-provisionerlabels:app:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultspec:replicas:1strategy:type:Recreateselector:matchLabels:app:nfs-client-provisionertemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2volumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:k8s-sigs.io/nfs-subdir-external-provisioner- name:NFS_SERVERvalue:10.7.79.148- name:NFS_PATHvalue:/data/k8svolumes:- name:nfs-client-rootnfs:server:10.7.79.148path:/data/k8s 修改完成后，向集群应用 nfs-subdir-external-provisioner 资源配置清单 kubectl apply -f rbac.yaml kubectl apply -f deployment.yaml ","date":"2021-04-29","objectID":"/posts/kubernetes-nfs-client/:2:2","tags":["kubernetes","nfs-subdir-external-provisioner","nfs"],"title":"扩展 NFS 向 Kubernetes 提供动态 PVC 功能","uri":"/posts/kubernetes-nfs-client/"},{"categories":["kubernetes"],"content":"测试 创建 class.yaml apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:k8s-sigs.io/nfs-subdir-external-provisioner# or choose another name, must match deployment's env PROVISIONER_NAME'parameters:archiveOnDelete:\"false\" 创建 pvc.yaml kind:PersistentVolumeClaimapiVersion:v1metadata:name:test-claimspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:1Mi 创建测试 pod.yaml kind:PodapiVersion:v1metadata:name:test-podspec:containers:- name:test-podimage:gcr.io/google_containers/busybox:1.24command:- \"/bin/sh\"args:- \"-c\"- \"touch /mnt/SUCCESS \u0026\u0026 exit 0 || exit 1\"volumeMounts:- name:nfs-pvcmountPath:\"/mnt\"restartPolicy:\"Never\"volumes:- name:nfs-pvcpersistentVolumeClaim:claimName:test-claim 应用 kubectl apply -f class.yaml kubectl apply -f pvc.yaml kubectl apply -f pod.yaml 最后进入 pod 查看使用测试下 ","date":"2021-04-29","objectID":"/posts/kubernetes-nfs-client/:3:0","tags":["kubernetes","nfs-subdir-external-provisioner","nfs"],"title":"扩展 NFS 向 Kubernetes 提供动态 PVC 功能","uri":"/posts/kubernetes-nfs-client/"},{"categories":["ubuntu"],"content":"NFS 允许系统通过网络与他人共享目录和文件。通过使用 NFS，用户和程序可以访问远程系统上的文件，就好像它们是本地文件一样。 NFS 可以提供的一些最显著的好处是： 本地工作站使用较少的磁盘空间，因为常用数据可以存储在单台计算机上，并且仍然可以通过网络为其他人访问。 用户无需在每个网络机器上单独拥有 home 目录。home 目录可在 NFS 服务器上设置，并在整个网络中提供。 软盘、CDROM 驱动器和 USB 拇指驱动器等存储设备可用于网络上的其他计算机。这可能会减少整个网络中的可移动媒体驱动器的数量。 ","date":"2021-04-24","objectID":"/posts/ubuntu-nfs/:0:0","tags":["nfs"],"title":"Ubuntu Server 网络文件系统（NFS）","uri":"/posts/ubuntu-nfs/"},{"categories":["ubuntu"],"content":"安装 在终端提示下输入以下命令以安装 NFS 服务器： sudo apt install nfs-kernel-server 要启动 NFS 服务器，您可以在终端提示下运行以下命令： sudo systemctl start nfs-kernel-server.service ","date":"2021-04-24","objectID":"/posts/ubuntu-nfs/:1:0","tags":["nfs"],"title":"Ubuntu Server 网络文件系统（NFS）","uri":"/posts/ubuntu-nfs/"},{"categories":["ubuntu"],"content":"配置 您可以通过将目录添加到 NFS 配置文件中来配置要共享的目录。默认配置文件：/etc/exports /srv *(ro,sync,subtree_check) /home *.hostname.com(rw,sync,no_subtree_check) /scratch *(rw,async,no_subtree_check,no_root_squash,noexec) 确保已创建您添加的任何自定义安装点（/srv 和/home 将存在）： sudo mkdir /scratch 重载 NFS 配置文件 sudo exportfs -a 您可以用具体的主机名替换 *。 使主机名声明尽可能具体。 同步/异步选项可控制更改是否在回复请求之前被限制为稳定存储。因此，async 会带来性能优势，但可能会造成数据丢失或损坏。尽管同步是默认的，但值得设置，因为如果未指定，exportfs 将发出警告。 subtree_check 和 no_subtree_check 启用或禁用安全验证，即客户端尝试为 exported 文件系统安装的子指示器是允许他们进行的安全验证。此验证步骤对某些使用案例（例如文件重命名频繁的家庭目录）有一些性能影响。仅读文件系统更适合启用 subtree_check 。与同步一样，如果未指定，exportfs 将发出警告。 NFS 有许多可选设置，用于调整性能、加强安全性或提供便利。这些设置各有其自身的权衡，因此谨慎使用它们非常重要，仅针对特定用例需要。例如， no_root_squash 增加了便利性，允许任何客户端系统的根用户修改根拥有的文件：在允许在共享安装点上执行可执行项的多用户环境中，这可能导致安全问题。noexec 阻止可执行文件从安装点运行。 ","date":"2021-04-24","objectID":"/posts/ubuntu-nfs/:2:0","tags":["nfs"],"title":"Ubuntu Server 网络文件系统（NFS）","uri":"/posts/ubuntu-nfs/"},{"categories":["ubuntu"],"content":"NFS 客户端配置 要启用客户端系统的 NFS 支持，请在终端提示下输入以下命令： sudo apt install nfs-common 使用 mount 命令从另一台机器上挂载共享的 NFS 目录，在终端提示处键入类似于以下命令行： sudo mkdir /opt/example sudo mount example.hostname.com:/srv /opt/example ","date":"2021-04-24","objectID":"/posts/ubuntu-nfs/:3:0","tags":["nfs"],"title":"Ubuntu Server 网络文件系统（NFS）","uri":"/posts/ubuntu-nfs/"},{"categories":["ubuntu"],"content":"警告 挂载点目录必须存在。目录中不应有任何文件或子目录，否则它们将无法访问，直到 nfs 文件系统卸载。/opt/example/opt/example 挂载 NFS 共享目录的另一种方法是向 /etc/fstab 文件添加一行配置。该行必须说明 NFS 服务器的主机名、及 NFS 共享目录的路径(在 NFS 服务器上的路径)。 /etc/fstab 配置如下 example.hostname.com:/srv /opt/example nfs rsize=8192,wsize=8192,timeo=14,intr ","date":"2021-04-24","objectID":"/posts/ubuntu-nfs/:4:0","tags":["nfs"],"title":"Ubuntu Server 网络文件系统（NFS）","uri":"/posts/ubuntu-nfs/"},{"categories":["cli"],"content":"CFSSL 简介 CFSSL 是 CloudFlare 开源的一款 PKI/TLS 瑞士军刀工具。 CFSSL 既是命令行工具，又是用于签名，验证和捆绑 TLS 证书的 HTTP API 服务器。 使用 Go 1.12+ 语言编写。 官方源码仓库: https://github.com/cloudflare/cfssl ","date":"2021-04-22","objectID":"/posts/cfssl/:1:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["cli"],"content":"安装 cfssl wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -O /usr/local/bin/cfssl-json wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -O /usr/local/bin/cfssl wget https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -O /usr/local/bin/cfssl-certinfo chmod +x /usr/local/bin/cfssl* ","date":"2021-04-22","objectID":"/posts/cfssl/:2:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["cli"],"content":"自签证书 ","date":"2021-04-22","objectID":"/posts/cfssl/:3:0","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["cli"],"content":"签发 CA 证书 生成 CA 证书签名请求文件 ca-csr.json mkdir certs cd certs/ cat \u003e ca-csr.json \u003c\u003cEOF { \"CN\": \"CA\", \"hosts\": [ ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"BeiJing\", \"O\": \"BJ\", \"ST\": \"BeiJing\", \"OU\": \"CA\" } ] } EOF 证书签名请求文件可以使用 cfssl print-defaults csr 创建，然后在进行相应的修改 生成 CA 证书 cfssl gencert -initca ca-csr.json | cfssl-json -bare ca ","date":"2021-04-22","objectID":"/posts/cfssl/:3:1","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["cli"],"content":"签发域名证书 自签发一个域名证书，以 host.com 域名为例 生成证书配置文件 默认配置可以使用 cfssl print-defaults config 命令生成 cat \u003e config.json \u003c\u003cEOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"www\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] }, \"client\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ] } } } } EOF 生成 host.com 域名证书签名请求文件 host-csr.json cat \u003e host-csr.json \u003c\u003cEOF { \"CN\": \"host.com\", \"hosts\": [ \"host.com\", \"*.host.com\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"BeiJing\", \"O\": \"BJ\", \"ST\": \"BeiJing\", \"OU\": \"HOST\" } ] } EOF 签发 host.com 域名证书 cfssl gencert -ca ca.pem -ca-key ca-key.pem -config config.json -profile www host-csr.json | cfssl-json -bare host 使用 cfssl-certinfo 命令查看证书信息 root@10-7-79-148:~/certs# cfssl-certinfo -cert host.pem { \"subject\": { \"common_name\": \"host.com\", \"country\": \"CN\", \"organization\": \"BJ\", \"organizational_unit\": \"HOST\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"BJ\", \"HOST\", \"host.com\" ] }, \"issuer\": { \"common_name\": \"CA\", \"country\": \"CN\", \"organization\": \"BJ\", \"organizational_unit\": \"CA\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"BJ\", \"CA\", \"CA\" ] }, \"serial_number\": \"50106944723092673296745532281502755453871335123\", \"sans\": [ \"host.com\", \"*.host.com\" ], \"not_before\": \"2021-04-22T13:28:00Z\", \"not_after\": \"2031-04-20T13:28:00Z\", \"sigalg\": \"SHA256WithRSA\", \"authority_key_id\": \"46:6C:D3:F9:1A:89:A0:B6:11:82:DA:E2:8B:8D:00:24:3E:8F:9E:3D\", \"subject_key_id\": \"6A:E8:F5:D9:E5:14:C0:2E:AE:53:DF:41:AF:9E:FF:A7:9B:D4:6A:80\", \"pem\": \"-----BEGIN CERTIFICATE-----\\nMIID3jCCAsagAwIBAgIUCMbfhCW8BG+QACtbh8V8YVcoJtMwDQYJKoZIhvcNAQEL\\nBQAwWDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl\\naUppbmcxCzAJBgNVBAoTAkJKMQswCQYDVQQLEwJDQTELMAkGA1UEAxMCQ0EwHhcN\\nMjEwNDIyMTMyODAwWhcNMzEwNDIwMTMyODAwWjBgMQswCQYDVQQGEwJDTjEQMA4G\\nA1UECBMHQmVpSmluZzEQMA4GA1UEBxMHQmVpSmluZzELMAkGA1UEChMCQkoxDTAL\\nBgNVBAsTBEhPU1QxETAPBgNVBAMTCGhvc3QuY29tMIIBIjANBgkqhkiG9w0BAQEF\\nAAOCAQ8AMIIBCgKCAQEA3ZfbPOW2hzTi3Ec/gpufnhaOkRiCZYIcGe5BJx+cip8c\\nh553anDZts2i1ZTYMeTjwtgHbojHqgqGgcF3xsCHQidRwoOhp7UHRgwfAacfmv0U\\nF5qmoPfNcbQzyZXhDJZAZqWLGqDBhCR/hVVugahXmZb8XzkpreTYTGHAiwAgUKXq\\nDEtEDr0D6LRw27+dR/1bwFs0ad2aEeJxvdH5Y40hO796VoPbX6PCI/TPkMnUsdTF\\nL51Ge+WEKk4TwEEghV1fl6+gGg3dmTcHpb8S5/zhe1bDI7Zs9/ErTAxd1HDdlPxt\\n66HtiygfKEjy8qVtsCIz+hzCxn9bZsmwNRdvV0QitQIDAQABo4GXMIGUMA4GA1Ud\\nDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0G\\nA1UdDgQWBBRq6PXZ5RTALq5T30Gvnv+nm9RqgDAfBgNVHSMEGDAWgBRGbNP5Gomg\\nthGC2uKLjQAkPo+ePTAfBgNVHREEGDAWgghob3N0LmNvbYIKKi5ob3N0LmNvbTAN\\nBgkqhkiG9w0BAQsFAAOCAQEAluByuUmRaPi1+SxjosQI8w6CvJC0N5XbAjsyXrDo\\netwpKKty0745aKyCtkFu6KW7bQohoX4JBdSrqve9V1Psm7Iwh6P8LKBRckBn6lMq\\ndavsgoGkyD/RwRMLUpi0TW8bvd0m+BOO2iHb+BSID7C+WPxflZb2Z8z1ljyzFaM6\\nmfevfYMqUiiRP/ztHvrHcZnk9pQi3kserPJg5DIzNvsvMd1T8IwJg36iIt6j4pi1\\nbtmXSWssMSR1vc7ZPWjS3Jc+2nDVjyPvARJsoAy6BBg07Pd41FhgKPgQE8il1oxc\\n3ep1OXlIC5IjfoZWrp80kznOaj++cOzl1Mg3k+eVyKmx1w==\\n-----END CERTIFICATE-----\\n\" } ","date":"2021-04-22","objectID":"/posts/cfssl/:3:2","tags":["cfssl","cfssl-json","cfssl-certinfo"],"title":"使用 cfssl 自签证书","uri":"/posts/cfssl/"},{"categories":["elasticstack"],"content":"安装配置 filebeat ","date":"2021-04-14","objectID":"/posts/filebeat/:1:0","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"安装 root@ubuntu:/opt# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.12.0-amd64.deb root@ubuntu:/opt# dpkg -i filebeat-7.12.0-amd64.deb ","date":"2021-04-14","objectID":"/posts/filebeat/:1:1","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"配置 filebeat.yml root@ubuntu:/etc/filebeat# cat filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /usr/local/nginx/logs/access.log json.keys_under_root: true json.overwrite_keys: true #filebeat.config.modules: # path: ${path.config}/modules.d/*.yml # reload.enabled: true setup.template.settings: index.number_of_shards: 3 # 配置索引分片数 # #setup.kibana: # output.elasticsearch: hosts: [\"192.168.16.102:9200\",\"192.168.16.103:9200\",\"192.168.16.104:9200\"] # 配置索引名为 nginx-日期，用于区分应用 index: \"nginx-%{+YYYY-MM}\" setup.template.enable: true setup.template.name: \"nginx\" setup.template.pattern: \"nginx-*\" setup.ilm.enabled: false #setup.ilm.rollover_alias: \"nginx\" #setup.ilm.pattern: \"{now/d}-000001\" # #processors: # - add_host_metadata: # when.not.contains.tags: forwarded # - add_cloud_metadata: ~ # - add_docker_metadata: ~ # - add_kubernetes_metadata: ~ ","date":"2021-04-14","objectID":"/posts/filebeat/:1:2","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"配置 nginx 日志格式 root@nginx-1:~# cat /etc/nginx/log-json log_format json '{\"remote_addr\":\"$remote_addr\", \"time_local\": \"$time_local\", \"domain\":\"$host\", \"request\":\"$request\", ' '\"status\":\"$status\", \"body_bytes_sent\":\"$body_bytes_sent\", \"method\":\"$request_method\", ' '\"http_referer\":\"$http_referer\", \"request_time\":\"$request_time\", ' '\"http_user_agent\":\"$http_user_agent\", \"http_x_forwarded_for\":\"$http_x_forwarded_for\", ' '\"upstream_addr\":\"$upstream_addr\", \"upstream_response_time\":\"$upstream_response_time\"}'; # 在 nginx 配置文件中引入，并指定 access_log 使用 json 格式记录日志 root@nginx-1:~# vim /etc/nginx/nginx.conf include /etc/nginx/log-json; access_log /var/log/nginx/access.log json; ","date":"2021-04-14","objectID":"/posts/filebeat/:2:0","tags":["filebeat"],"title":"使用 Filebeat 收集 nginx 日志","uri":"/posts/filebeat/"},{"categories":["elasticstack"],"content":"环境准备 本文使用 Ubuntu 20.04 安装 elasticsearch 集群，准备三台机。 192.168.16.102 192.168.16.103 192.168.16.104 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:1:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"安装配置 elasticsearch 集群 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"安装 elasticsearch root@ubuntu:/opt# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-linux-x86_64.tar.gz root@ubuntu:/opt# tar xzf elasticsearch-7.12.0-linux-x86_64.tar.gz root@ubuntu:/opt# ln -s elasticsearch-7.12.0 elasticsearch ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:1","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"配置 elasticsearch root@ubuntu:/opt# cd elasticsearch/config root@ubuntu:/opt/elasticsearch/config# cat \u003e elasticsearch.yml \u003c\u003cEOF # 集群名 cluster.name: my-application # 集群内同时启动的数据任务个数，默认是2个 cluster.routing.allocation.cluster_concurrent_rebalance: 16 # 添加或删除节点为及负载均衡时并发恢复的线程个数，默认是4个 cluster.routing.allocation.node_concurrent_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个 cluster.routing.allocation.node_initial_primaries_recoveries: 16 # 节点名 node.name: node-1 # 是否有资格成为主节点 node.master: true # 是否为数据节点 node.data: true path.data: /data/elasticsearch/data path.logs: /data/elasticsearch/logs # 监听的网络地址 network.host: 0.0.0.0 network.tcp.keep_alive: true network.tcp.no_delay: true transport.tcp.compress: true gateway.recover_after_nodes: 2 # 用于 HTTP 客户端通信的端口 http.port: 9200 # 用于节点之间通信的端口 transport.port: 9300 # head 管理插件需要打开跨域配置 http.cors.allow-origin: \"*\" http.cors.enabled: true http.max_content_length: 200mb # 节点发现 discovery.seed_hosts: [\"192.168.16.102:9300\",\"192.168.16.103:9300\",\"192.168.16.104:9300\"] # 初始化一个集群时需要此配置来选举 master cluster.initial_master_nodes: [\"node-1\"] EOF # 配置 jvm 堆内存大小 root@ubuntu:/opt/elasticsearch/config# grep '^-Xm' jvm.options -Xms4g -Xmx4g ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:2","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"系统配置 root@ubuntu:/opt/elasticsearch# useradd -m -s /bin/bash elasticsearch root@ubuntu:/opt/elasticsearch# mkdir -p /data/elasticsearch/{data,logs} root@ubuntu:/opt/elasticsearch# chown -R elasticsearch.elasticsearch /data/elasticsearch root@ubuntu:/opt/elasticsearch# chown -R elasticsearch.elasticsearch /opt/elasticsearch-7.12.0 root@ubuntu:/opt/elasticsearch/config# cat \u003e\u003e /etc/security/limits.conf \u003c\u003cEOF * soft nproc unlimited * hard nproc unlimited * soft core unlimited * soft nofile 65535 * hard nofile 65535 EOF root@ubuntu:/opt/elasticsearch/config# echo 'vm.max_map_count=262144' \u003e\u003e /etc/sysctl.conf root@ubuntu:/opt/elasticsearch/config# sysctl -p ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:3","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"启动 elasticsearch 服务 root@ubuntu:~# su - elasticsearch elasticsearch@ubuntu:~$ cd /opt/elasticsearch elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch elasticsearch 默认不允许使用 root 用户启动， 需要切换至 elasticsearch 用户启动； -d 选项可以将程序设置为守护进程 提示：以相同的操作方法配置另两台节点，注意节点名不要重复 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:2:4","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"elasticsearch 安全验证 elasticsearch 7.7 以后的版本将安全认证功能免费开放了。 并将 X-pack 插件集成了到了开源的 ElasticSearch 版本中。下面介绍如何利用 X-pack 给 ElasticSearch 相关组件设置用户名和密码 # 配置 xpack, 启用验证功能 root@ubuntu:/opt/elasticsearch# cat \u003e\u003econfig/elasticsearch.yml \u003c\u003cEOF xpack.security.enabled: true xpack.security.transport.ssl.enabled: true EOF # 重启 elasticsearch 服务 root@ubuntu:/opt/elasticsearch# kill \u003celasticsearch-pid\u003e root@ubuntu:/opt/elasticsearch# su - elasticsearch elasticsearch@ubuntu:~$ cd /opt/elasticsearch elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch -d # 使用交互命令行模式配置验证密码 elasticsearch@ubuntu:/opt/elasticsearch$ ./bin/elasticsearch-setup-passwords interactive Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user. You will be prompted to enter passwords as the process progresses. Please confirm that you would like to continue [y/N]y Enter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana_system]: Reenter password for [kibana_system]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: 到此已经完成ES及相关组件的加密了, 后续访问和使用相关组件都需要验证用户名和密码了 (请记好你配置的密码) 验证密码信息存储在 .security-7 索引中 不带密码访问时 root@ubuntu:~# curl -I localhost:9200 HTTP/1.1 401 Unauthorized WWW-Authenticate: Basic realm=\"security\" charset=\"UTF-8\" content-type: application/json; charset=UTF-8 content-length: 381 带密码访问 root@ubuntu:~# curl -i localhost:9200 -u elastic:JEd01cn6hj0qm2mO HTTP/1.1 200 OK content-type: application/json; charset=UTF-8 content-length: 530 { \"name\" : \"node-1\", \"cluster_name\" : \"my-application\", \"cluster_uuid\" : \"G6jxloWFR1SpCJ5cqb4EKA\", \"version\" : { \"number\" : \"7.12.0\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\", \"build_date\" : \"2021-03-18T06:17:15.410153305Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } ","date":"2021-04-10","objectID":"/posts/elasticsearch/:3:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["elasticstack"],"content":"重置 elasticsearch 密码 官方文档: https://www.elastic.co/guide/en/elasticsearch/reference/7.4/security-api-change-password.html 修改 elastic 用户的密码 root@ubuntu:~# curl -X POST localhost:9200/_security/user/elastic/_password \\ -d '{\"password\":\"123456\"}' \\ -u elastic:JEd01cn6hj0qm2mO \\ -H 'content-type: application/json' 也可以使用 ./bin/elasticsearch-setup-passwords interactive 命令重新设置 ","date":"2021-04-10","objectID":"/posts/elasticsearch/:4:0","tags":["elasticsearch"],"title":"部署 ElasticSearch 集群","uri":"/posts/elasticsearch/"},{"categories":["nginx"],"content":"alias 与 root 区别 nginx 是通过 alias 设置虚拟目录，在 nginx 的配置中， alias 目录和 root 目录是有区别的 alias 指定的目录是准确的，即 location 匹配访问的 path 目录下的文件直接是在 alias 目录下查找的； root 指定的目录是 location 匹配访问的 path 目录的上一级目录, 这个 path 目录一定要是真实存在 root 指定目录下的； 使用 alias 标签的目录块中不能使用 rewrite 的 break（具体原因不明）；另外， alias 指定的目录后面必须要加上 \"/\" 符号！ alias 虚拟目录配置中，location 匹配的 path 目录如果后面不带 \"/\" ，那么访问的url地址中这个 path 目录后面加不加 \"/\" 不影响访问，访问时它会自动加上 \"/\" ； 但是如果 location 匹配的 path 目录后面加上 \"/\"，那么访问的 url 地址中这个 path 目录必须要加上 \"/\"，访问时它不会自动加上 \"/\"。如果不加上 \"/\"，访问就会失败！ root 目录配置中，location 匹配的 path 目录后面带不带 \"/\"，都不会影响访问。 ","date":"2021-04-01","objectID":"/posts/nginx-alias-root/:1:0","tags":["nginx"],"title":"NGINX alias 与 root 区别","uri":"/posts/nginx-alias-root/"},{"categories":["nginx"],"content":"举例说明 比如 nginx 配置的域名是 liwanggui.com location /huan/ { alias /home/www/huan/; } 在上面 alias 虚拟目录配置下，访问 http://liwanggui.com/huan/a.html 实际指定的是 /home/www/huan/a.html。 注意: alias 指定的目录后面必须要加上 \"/\"，即 /home/www/huan/ 不能改成 /home/www/huan 上面的配置也可以改成 root 目录配置，如下 这样 nginx 就会去 /home/www/huan 下寻找 http://liwanggui.com/huan/a.html 资源，两者配置后的访问效果是一样的！ location /huan/ { root /home/www/; } ","date":"2021-04-01","objectID":"/posts/nginx-alias-root/:2:0","tags":["nginx"],"title":"NGINX alias 与 root 区别","uri":"/posts/nginx-alias-root/"},{"categories":["nginx"],"content":"域名重定向 当用户访问 http://liwanggui.com 时 url 重定向至 https://liwanggui.com， 实现 http -\u003e https 重定向，实现方式有两种： 通过 rewrite 模块的 permanent 参数实现永久重定向的 http 状态 301 通过 return 指令实现 （推荐） rewrite 实现 server { listen 80; server_name liwanggui.com; access_log off; rewrite ^/(.*)$ https://$host/$1 permanent; # 匹配以斜杠开头之后的所有字符 $1 表示小括号内匹配的字符 permanent 表示永久301跳转 } return 实现: 推荐做法 server { listen 80; server_name liwanggui.com; access_log off; return 301 https://$host$request_uri; } $host: 表示 HTTP 请求头中的 Host 值 $request_uri: 表示 HTTP 请求 uri ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:1:0","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"多域名跳转应用实例 使用 nginx 做反向代理，当用户访问 www.liwanggui.com 时就代理到 192.168.1.100:8080 的 web 目录下， 当用户访问 http://www.liwanggui.com/admin 时就代理到 192.168.1.100:8080 的 admin 目录下， 当用户访问 wap.liwanggui.com 时就代理到 192.168.1.100:8080 的 wap 目录下 server_name www.liwanggui.com; location / { proxy_pass http://192.168.1.100:8080/web/; } location /admin { proxy_pass http://192.168.1.100:8080/admin; } server_name wap.liwanggui.com; location / { proxy_pass http://192.168.1.100:8080/wap/; } 注意：在 proxy_pass 配置两个代理目录 web 和 wap 后面必须加一个斜杠，否则 nginx 会报错，仔细看上面代理配置中两种写法的区别就明白了 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:2:0","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"nginx 常用指令 nginx 的 URL 重写模块是用得比较多的模块之一，常用的 URL 重写模块命令有 if 、rewrite、 set、 break ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:0","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"if 命令 语法： if (condition) {....} 默认值： none 使用字段: server 、location 默认情况下，if 命令默认值为空，可在 nginx 配置文件的 server、location 部分使用，另外，if 命令可以在在判断语句中指定正则或匹配条件等，相关匹配条件如下： if 与小括号之间有一个空格 正则表达式匹配 ~ 表示区分大小写匹配 ~* 表示不区分大小写匹配 !~ 表示区分大小写不匹配， !~* 表示不区分大小写不匹配 文件及目录匹配 -f 和 !-f 用来判断是否存在文件 -d 和 !-d 用来判断是否存在目录 -e 和 !-e 用来判断是否存在文件和目录 -x 和 !-x 用来判断文件是否可执行 nginx 配置文件中有很多内置变量，这些变量经常和if命令一起使用。常见的内置变量有如下几种: $args, 此变量与请求行中的参数相等 $document_root， 此变量等同于当前请求的 root 命令指定的值 $uri, 此变量等同于当前 request 中的 uri $document_uri， 此变量与 $uri 含义一样 $host， 此变量与请求头部中的 “Host” 行指定的值一致 $limit_rate, 此变量用来设置限制连接的速率 $request_method, 此变量等同于 request 的 method，通常是’GET',‘POST’ $remote_addr, 此变量表示客户端的IP地址 $remote_port, 此变量表示客户端端口 $remote_user, 此变量等同于用户名，由 ngx_http_auth_basic_module 认证 $request_filename, 此变量表示当前请求的文件的路径名，由 root 或 alias 与 URI request 组合而成 $request_uri, 此变量表示含有参数的完整的初始 URI $query_string, 此变量与$args含义一致 $server_name, 此变量表示请求到达的服务器名 $server_port, 此变量表示请示到达的服务器端口 例：uri为：http://localhost:88/test1/test2/test.php 各变量值如下： $host： localhost $server_port： 88 $request_uri： http://localhost:88/test1/test2/test.php $document_uri： /test1/test2/test.php $document_root： /var/www/html $request_filename： /var/www/html/test1/test2/test.php 配置实例 server { listen 80; server_name www.liwanggui.com; access_log logs/host.access.log main; index index.html index.htm; root /var/www/html; location ~*\\.(gif|jpg|jpeg|png|bmp|swf|htm|html|css|js)$ { root /usr/local/nginx/www/img; if (!-f $request_filename){ root /var/www/html/img; } if (!-f $request_filename){ root /apps/images; } } location ~*\\.(jsp)${ root /webdata/webapp/www/ROOT; if (!-f $request_filename){ root /usr/local/nginx/www/jsp; } proxy_pass http://127.0.0.1:8888; } } 这段代码主要完成对 www.liwanggui.com 这个域名的资源访问配置， www.liwanggui.com 这个域名的根目录为 /var/www/html, 而静态资源分别位于 /usr/local/nginx/www/img, /var/www/html/img, /apps/images 三个目录下， 请求静态资源的方式依次在三个目录中找，如果第一个目录找不到，就找第二目录，以此类推，如果都找不到，将提示404错误； 动态资源分别位于 /webdata/webapp/www/ROOT,/usr/local/nginx/www/jsp, 两个目录下，如果客户端请求的的资源是以 .jsp 结尾的，那么将依次在这两个动态程序目录下查找资源。 而于没有在这两个目录中定义的资源，将全部从根目录 /var/www/html 进行查找。 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:1","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"rewrite 命令 nginx 通过 ngx_http_rewrite_module 模块支持URL重写和if条件判断，但要使用 rewrite 功能，需要 pcre 支持，应在编译 nginx 时指定 pcre 源码目录. rewrite 的使用语法如下： 语法： rewrite regex flag 默认值： none 使用字段： server location if 在默认情况下，rewrite 命令默认值为空，可以 nginx 配置文件的 server,location,if 部分使用，rewrite 命令的最后一项参数为 flag 标记,其支持的 flag 标记主要有以下几种： last, 相当于 apache 里的 L 标记，表示完成 rewrite 之后搜索相应的 uri 或 location break, 表示终止匹配，不再匹配后面的规则 redirect, 将返回 302 临时重定向，在浏览器地址会显示跳转后的 URL 地址。 permanent, 将返回 301 永久重定向，在浏览器地址会显示跳转后的 URL 地址。 last 一般写在 server 和 if 中，而 break 一般使用在 location 中 last 不终止重写后的 url 匹配，即新的 url 会再从 server 走一遍匹配流程，而 break 终止重写后的匹配 break 和 last 都能组织继续执行后面的 rewrite 指令 其中 last 和 break 用来实现 URL 重写，浏览器地址不变。下面是一个示例配置： location ~ ^/best/ { rewrite ^/best/(.*)$ /best/$1 break; proxy_pass http://www.liwanggui.com; } 这个例子使用了 break 标记，可实现将请求为 http://www.lwg.com/best/webinfo.html 的页面重定向到 http://www.liwanggui.com/best/webinfo.html 页面而不引起浏览器地址栏中 URL 的变化。 这个功能在新旧网站交替的时候非常有用（最好实践下，感觉有问题） ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:2","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"set 命令 通过 set 命令可以设置一个变量并为其赋值，其值可以是文本、变量或他们的组合。也可以使用set定义一个新的变量，但是不能使用 set 设置 $http_xxx 头部变量 set 的使用方法如下： 语法： set variable value 默认值： none 使用字段： server location if 在默认情况下，set 命令默认值为空，可以 nginx 配置文件的 server location if 部分使用，下面是一个示例配置 location / { proxy_pass http://127.0.0.1:8080/; set $query $query_string; rewrite /dede /wordpress?$query?; } 在这个例子中，要实现将请求 http://www.liwanggui.com/dede/wp?p=160 的页面，重写到地址 http://www.liwanggui.com/wordpress/?p=160, 也就是重写带参数的 URL. 这里涉及 $query_string 变量，这个变量相当于请求行中的参数，也就是？ 后面的内容。也可以用 $args 代替 $query_string 变量 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:3","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"break 命令 break 的用法在前面的介绍中其实已经出现过，它表示完成当前设置的规则后，不再匹配后面的重写规则。 break的使用语法如下： 语法： break 默认值： none 使用字段： server lcoation if 在默认情况下，break 命令的值为空，可以 nginx 配置文件的 server lcoation if 部分使用，下面是一个示例配置 server { listen 80; server_name www.lwg.com www.liwanggui.com; if ($host != 'www.wb.com'){ rewrite ^/(.*)$ http://www.lwg.com/error.txt break; rewrite ^/(.*)$ http://www.lwg.com/$1 permanent; } } 这个例子定义了两个域名 www.lwg.com 和 www.liwanggui.com, 当通过域名 www.liwanggui.com 访问网站时，会将请求重定向到 http://www.lwg.com/error.txt 页面，由于设置了 break 命令，因此下面的 rewrite 规则不再执行，直接退出。 ","date":"2021-04-01","objectID":"/posts/nginx-syntax/:3:4","tags":["nginx"],"title":"NGINX 常用指令语法","uri":"/posts/nginx-syntax/"},{"categories":["nginx"],"content":"本文将介绍 NGINX 常用的二种安装方式，分为二进制包和源码编译，根据自己的使用的场景选择即可。 ","date":"2021-04-01","objectID":"/posts/nginx-install/:0:0","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"二进制包 二进制方式安装比效简单，直接使用系统包管理工具安装即可 Ubuntu apt install nginx CentOS yum install nginx ","date":"2021-04-01","objectID":"/posts/nginx-install/:1:0","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"源码编译 ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:0","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"准备编译环境 本文使用 Ubuntu 20.04.2 LTS 编译安装. root@ubuntu:~# apt install gcc make openssl libssl-dev libpcre3-dev zlib1g-dev root@ubuntu:~# cd /usr/local/src root@ubuntu:/usr/local/src# wget http://nginx.org/download/nginx-1.18.0.tar.gz root@ubuntu:/usr/local/src# tar xzf nginx-1.18.0.tar.gz ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:1","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"开始编译安装 root@ubuntu:/usr/local/src# cd nginx-1.18.0 root@ubuntu:/usr/local/src/nginx-1.18.0# useradd -r www root@ubuntu:/usr/local/src/nginx-1.18.0# ./configure --prefix=/usr/local/nginx \\ --user=www \\ --group=www \\ --with-threads \\ --with-file-aio \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_auth_request_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_degradation_module \\ --with-http_slice_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --with-stream_ssl_preread_module root@ubuntu:/usr/local/src/nginx-1.18.0# make \u0026\u0026 make install ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:2","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"Nginx 服务管理 启动 nginx 服务 root@ubuntu:/usr/local/nginx# /usr/local/nginx/sbin/nginx root@ubuntu:/usr/local/nginx# curl -I localhost HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Thu, 01 Apr 2021 08:16:00 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Thu, 01 Apr 2021 08:07:38 GMT Connection: keep-alive ETag: \"60657f4a-264\" Accept-Ranges: bytes 管理 nginx 服务 # 测试 nginx 配置文件语法 root@ubuntu:~# /usr/local/nginx/sbin/nginx -t nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful # 重载 nginx 配置 root@ubuntu:~# /usr/local/nginx/sbin/nginx -s reload # 停止 nginx 服务 root@ubuntu:~# /usr/local/nginx/sbin/nginx -s stop ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:3","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["nginx"],"content":"Systemd 配置 root@ubuntu:~# cat \u003e /usr/lib/systemd/system/nginx.service \u003c\u003cEOF [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/usr/local/nginx/logs/nginx.pid ExecStartPre=/usr/local/nginx/sbin/nginx -t ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target EOF ","date":"2021-04-01","objectID":"/posts/nginx-install/:2:4","tags":["nginx"],"title":"NGINX 安装","uri":"/posts/nginx-install/"},{"categories":["centos"],"content":"yum 主要用于自动安装、升级 rpm 软件包，它能自动查找并解决 rpm 包之间的依赖关系。要成功的使用 yum 工具安装更新软件或系统，就需要有一个包含各种rpm软件包的 repository（软件仓库），这个软件仓库我们习惯称为 yum 源。网络上有大量的 yum 源，但由于受到网络环境的限制，导致软件安装耗时过长甚至失败。特别是当有大量服务器大量软件包需要安装时，缓慢的进度条令人难以忍受。因此我们在优化系统时，都会更换国内的源。 相比较而言，本地 yum 源服务器最大优点是局域网的快速网络连接和稳定性。有了局域网中的 yum 源服务器，即便在 Internet 连接中断的情况下，也不会影响其他 yum 客户端的软件安装和升级。 ","date":"2021-04-01","objectID":"/posts/yum-repo/:0:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"创建 yum 仓库目录 mkdir -p /data/yum/centos/{6,7}/x86_64 上传 rpm 包到 /data/yum/centos/6/x86_64 和 /data/yum/centos/7/x86_64 目录 ","date":"2021-04-01","objectID":"/posts/yum-repo/:1:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"安装 createrepo 软件 yum install createrepo ","date":"2021-04-01","objectID":"/posts/yum-repo/:2:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"初始化 repodata 索引文件 createrepo -pdo /data/yum/centos/6/x86_64/ /data/yum/centos/6/x86_64/ createrepo -pdo /data/yum/centos/7/x86_64/ /data/yum/centos/7/x86_64/ ","date":"2021-04-01","objectID":"/posts/yum-repo/:3:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"提供 yum 服务 提供 yum 服务很简单，只需要使用 nginx 开启目录浏览器功能即可, 测试时可以使用 python 模块实现 # python 2.x python2 -m SimpleHTTPServer 80 # python 3.x python3 -m http.server 80 ","date":"2021-04-01","objectID":"/posts/yum-repo/:4:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"添加新 rpm 包 每当添加新的 rpm 包时都需要执行以下命令, 为了方便可以将以下加入计划任务中 createrepo --update /data/yum/centos/6/x86_64/ createrepo --update /data/yum/centos/7/x86_64/ ","date":"2021-04-01","objectID":"/posts/yum-repo/:5:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["centos"],"content":"客户端配置 客户端需要将 yum 仓库地址写成 yum 源配置文件，并放入 /etc/yum.repos.d 目录中 cat \u003e /etc/yum.repos.d/devops.repo \u003c\u003c REPO [devops] name=CentOS-$releasever - DEVOPS baseurl=http://your_domain_name/centos/$releasever/x86_64/ enable=1 gpgcheck=0 REPO 之后就可以使用 yum 安装 devops 仓库中的 rpm 包了 ","date":"2021-04-01","objectID":"/posts/yum-repo/:6:0","tags":["yum"],"title":"部署 YUM 本地仓库","uri":"/posts/yum-repo/"},{"categories":["cli"],"content":" 参考文档 - 1 参考文档 - 2 软件编译安装可以很大程序上定制符合实际需求的软件包，但由于编译时间过长依赖关系复杂常常会耽误太多的时间，为了达到快速部署安装的需求我们需要定制符合需求的 rpm 包， rpm 默认是通过 rpmbuild 工具配合 spec 配置文件生成。下面将介绍如何使用 rpmbild 工具生成定制 rpm 包， 以 nginx 为例 ","date":"2021-04-01","objectID":"/posts/rpmbuild/:0:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["cli"],"content":"环境准备 安装所需工具 yum install gcc rpm-build rpm-devel rpmlint make python bash coreutils diffutils patch rpmdevtools 准备制作环境 [root@build ~]# useradd -m build [root@build ~]# su - build [build@build ~]# rpmdev-setuptree [build@build ~]# ls -R rpmbuild/ rpmbuild/: BUILD RPMS SOURCES SPECS SRPMS rpmbuild/BUILD: # 源码编译工作目录 rpmbuild/RPMS: # 最终 rpm 包生成目录 rpmbuild/SOURCES: # 源码包及附加文件放置目录 rpmbuild/SPECS: # spec 配置文件目录 rpmbuild/SRPMS: # 最终端 srpm 包生成目录 rpmbuild/BUILDROOT: rpm 打包工作目录 生成 nginx.spec 生成 nginx.spec 配置文件，并根据情况进行修改 [build@build ~]# cd rpmbuild/SPECS [build@SPECS ~]# rpmdev-newspec nginx [build@SPECS ~]# cat nginx.spec Name: nginx Version: 1.14.2 Release: 1%{?dist} Summary: A high performance web server and reverse proxy server Group: System Environment/Daemons License: GPLv2 URL: https://nginx.org # 制作 rpm 包所需文件 Source0: nginx-1.14.2.tar.gz Source1: nginx.conf Source2: limit.conf Source3: proxy.conf Source4: pathinfo.conf Source5: enable-php.conf Source6: geoip2.conf Source7: upstream.conf.example Source8: enable-ssl.conf.example Source9: nginx-status.conf.example Source10: nginx.init Source11: nginx.logrotate # 编译时需要的依赖包 BuildRequires: gcc BuildRequires: gcc-c++ BuildRequires: make BuildRequires: libmaxminddb-devel # rpm 安装时需要的依赖包 Requires: libmaxminddb-devel %description Nginx is a web server and a reverse proxy server for HTTP, SMTP, POP3 and IMAP protocols, with a strong focus on high concurrency, performance and low memory usage. # 制作前准备，解包和路径切换工具 %prep %setup -q # 软件包编译过程 %build ./configure --prefix=/usr/local/nginx / --user=www --group=www / --with-http_stub_status_module / --with-http_sub_module / --with-http_ssl_module / --with-http_v2_module / --with-http_realip_module / --with-openssl=./openssl-1.1.1b / --with-pcre=./pcre-8.42 / --with-zlib=./zlib-1.2.11 / --add-module=./nginx-sticky-module-ng-1.2.6/ / --add-module=./nginx-upstream-check-module/ / --add-module=./ngx-http-geoip2-module-3.2/ make %{?_smp_mflags} # 编译完安装软件包至指定目录等待打包 %install rm -rf $RPM_BUILD_ROOT make install DESTDIR=$RPM_BUILD_ROOT %{__install} -p -D -m 0644 %{SOURCE1} %{buildroot}/usr/local/nginx/conf/nginx.conf %{__install} -p -D -m 0644 %{SOURCE2} %{buildroot}/usr/local/nginx/conf/limit.conf %{__install} -p -D -m 0644 %{SOURCE3} %{buildroot}/usr/local/nginx/conf/proxy.conf %{__install} -p -D -m 0644 %{SOURCE4} %{buildroot}/usr/local/nginx/conf/pathinfo.conf %{__install} -p -D -m 0644 %{SOURCE5} %{buildroot}/usr/local/nginx/conf/enable-php.conf %{__install} -p -D -m 0644 %{SOURCE6} %{buildroot}/usr/local/nginx/conf/geoip2.conf %{__install} -p -D -m 0644 %{SOURCE7} %{buildroot}/usr/local/nginx/conf/upstream.conf.example %{__install} -p -D -m 0644 %{SOURCE8} %{buildroot}/usr/local/nginx/conf/enable-ssl.conf.example %{__install} -p -D -m 0644 %{SOURCE9} %{buildroot}/usr/local/nginx/conf/nginx-status.conf.example %{__install} -p -D -m 0755 %{SOURCE10} %{buildroot}/etc/init.d/nginx %{__install} -p -D -m 0644 %{SOURCE11} %{buildroot}/etc/logrotate.d/nginx # 清理工作 %clean rm -rf $RPM_BUILD_ROOT # 安装前执行的命令 %pre if ! id www \u0026\u003e/dev/null; then useradd -r -M -s /sbin/nologin www fi # 安装后 %post /sbin/chkconfig --add %{name} /sbin/chkconfig %{name} on # 卸载前 %preun /etc/init.d/nginx stop /sbin/chkconfig --del %{name} # rpm 打包的文件列表 %files %defattr(-,root,root,-) /usr/local/nginx/ /etc/logrotate.d/nginx %attr(0755,root,root) /etc/init.d/nginx %config(noreplace) /usr/local/nginx/conf/nginx.conf %config(noreplace) /usr/local/nginx/conf/limit.conf %config(noreplace) /usr/local/nginx/conf/geoip2.conf %config(noreplace) /usr/local/nginx/conf/enable-php.conf # 更新日志 %changelog 准备 nginx 源码包及相关文件 [build@build ~]$ cd rpmbuild/SOURCES/ [build@build SOURCES]$ ls -l total 12356 -rw-r--r-- 1 build build 207 Mar 17 17:32 enable-php.conf -rw-r--r-- 1 build build 1137 Mar 17 18:11 enable-ssl.conf.example -rw-r--r-- 1 build build 1077 Mar 17 21:05 geoip2.conf -rw-r--r-- 1 build build 1488 Mar 17 17:53 limit.conf -rw-rw-r-- 1 build build 12589977 Mar 17 20:19 nginx-1.14.2.tar.gz -rw-r--r-- 1 ","date":"2021-04-01","objectID":"/posts/rpmbuild/:1:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["cli"],"content":"制作 rpm 包 [build@build ~]$ cd rpmbuild/SPECS/ [build@SPECS ~]# rpmbuild -ba nginx.spec rpmbuild -bp nginx.spec # 制作到%prep段 rpmbuild -bc nginx.spec # 制作到%build段 rpmbuild -bi nginx.spec # 执行 spec 文件的 “%install” 阶段 (在执行了 %prep 和 %build 阶段之后)。这通常等价于执行了一次 “make install” rpmbuild -bb nginx.spec # 制作二进制包 rpmbuild -ba nginx.spec # 表示既制作二进制包又制作src格式包 Tips: 更新多选项说明使用 rpmbuild -h ","date":"2021-04-01","objectID":"/posts/rpmbuild/:2:0","tags":["rpmbuild"],"title":"使用 rpmbuild 的 RPM 包","uri":"/posts/rpmbuild/"},{"categories":["cli"],"content":"linux 下最受欢迎的多线程归档器是 pigz（而不是gzip）和 pbzip2（而不是bzip2) ","date":"2021-04-01","objectID":"/posts/pigz/:0:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["cli"],"content":"开始使用 pigz 可以当做是 gzip 高级版，可以执行 gzip 的工作，但是在压缩时会将工作分散到多个处理器和内核上。 pbzip2 可以当做是 bzip2 高级版, 在和 tar 命令一起使用时需要手动使用的压缩程序, 信息如下: -I, --use-compress-program=PROG 默认情况系统并没有安装 pigz，pbzip2, 需要手动安装执行下以下命令安装 $ yum install -y pigz pbzip2 ","date":"2021-04-01","objectID":"/posts/pigz/:1:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["cli"],"content":"示例 $ tar -I pbzip2 -cf OUTPUT_FILE.tar.bz2 paths_to_archive $ tar --use-compress-program=pigz -cf OUTPUT_FILE.tar.gz paths_to_archive Archiver 必须接受 -d 如果替换实用程序没有此参数和/或您需要指定其他参数，则使用管道（如有必要，添加参数）： $ tar cf - paths_to_archive | pbzip2 \u003e OUTPUT_FILE.tar.gz $ tar cf - paths_to_archive | pigz \u003e OUTPUT_FILE.tar.gz 解压只需要将 -c 替换为 -x 即可. ","date":"2021-04-01","objectID":"/posts/pigz/:2:0","tags":["pigz","pbzip2"],"title":"利用多核 CPU 进行解/压缩","uri":"/posts/pigz/"},{"categories":["redis"],"content":"Redis Replication ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:0","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Replication 原理 副本库通过 slaveof 127.0.0.1 6380 命令, 连接主库,并发送 SYNC 给主库 主库收到 SYNC,会立即触发 BGSAVE,后台保存 RDB,发送给副本库 副本库接收后会应用 RDB 快照 主库会陆续将中间产生的新的操作,保存并发送给副本库 到此,我们主复制集就正常工作了 再此以后,主库只要发生新的操作,都会以命令传播的形式自动发送给副本库. 所有复制相关信息,从 info 信息中都可以查到. 即使重启任何节点, 他的主从关系依然都在. 如果发生主从关系断开时,从库数据没有任何损坏, 在下次重连之后, 从库发送 PSYNC 给主库 主库只会将从库缺失部分的数据同步给从库应用,达到快速恢复主从的目的 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:1","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"准备多实例 Redis 服务 实验采用单机多实例的方式进行，创建三个实例。端口 6380-6382, 可以使用 redis 源码包中的脚本（install_server.sh）创建 [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6380 Please select the redis config file name [/data/redis/etc/6380.conf] Selected default - /data/redis/etc/6380.conf Please select the redis log file name [/data/redis/log/redis_6380.log] Selected default - /data/redis/log/redis_6380.log Please select the data directory for this instance [/data/redis/6380] Selected default - /data/redis/6380 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6380 Config file : /data/redis/etc/6380.conf Log file : /data/redis/log/redis_6380.log Data dir : /data/redis/6380 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6380.conf =\u003e /etc/init.d/redis_6380 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6381 Please select the redis config file name [/data/redis/etc/6381.conf] Selected default - /data/redis/etc/6381.conf Please select the redis log file name [/data/redis/log/redis_6381.log] Selected default - /data/redis/log/redis_6381.log Please select the data directory for this instance [/data/redis/6381] Selected default - /data/redis/6381 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6381 Config file : /data/redis/etc/6381.conf Log file : /data/redis/log/redis_6381.log Data dir : /data/redis/6381 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6381.conf =\u003e /etc/init.d/redis_6381 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! [root@localhost utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] 6382 Please select the redis config file name [/data/redis/etc/6382.conf] Selected default - /data/redis/etc/6382.conf Please select the redis log file name [/data/redis/log/redis_6382.log] Selected default - /data/redis/log/redis_6382.log Please select the data directory for this instance [/data/redis/6382] Selected default - /data/redis/6382 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6382 Config file : /data/redis/etc/6382.conf Log file : /data/redis/log/redis_6382.log Data dir : /data/redis/6382 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6382.conf =\u003e /etc/init.d/redis_6382 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! 本例为了使用自定义的目录存放 redis 数据及配置文件对 install_server.sh 脚本中的路径做了修改 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:2","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Replication 配置 6380 为主库，6381-6382 为从库，配置如下: 主库配置 [root@localhost log]# redis-cli -p 6380 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE 从库配置 [root@localhost log]# redis-cli -p 6381 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE OK 127.0.0.1:6381\u003e slaveof 127.0.0.1 6380 OK [root@localhost log]# redis-cli -p 6382 127.0.0.1:6381\u003e config set requirepass 123 OK 127.0.0.1:6381\u003e auth 123 OK 127.0.0.1:6381\u003e config set masterauth 123 OK 127.0.0.1:6381\u003e CONFIG REWRITE OK 127.0.0.1:6381\u003e slaveof 127.0.0.1 6380 OK 如果想解除主从关系可以使用 slaveof no one 指令 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:3","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"检查 Replication 状态 [root@localhost etc]# redis-cli -p 6380 -a 123 info replication # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=6381,state=online,offset=3442,lag=1 slave1:ip=127.0.0.1,port=6382,state=online,offset=3442,lag=1 master_repl_offset:3575 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:3574 [root@localhost etc]# redis-cli -p 6381 -a 123 info replication # Replication role:slave master_host:127.0.0.1 master_port:6380 master_link_status:up master_last_io_seconds_ago:0 master_sync_in_progress:0 slave_repl_offset:6291 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 可以在主库写入数据然后到从库读取测试同步 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:1:4","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Redis Sentinel 高可用 由于 redis replication 默认状态下如果主库宕机，是不会自动切换主从身份的，需要手动干预，这是生产环境下不允许的。为了解决此问题 redis 引入哨兵模式。 redis sentinel 有以下作用： 监控节点 自动选主，切换主从身份 从库自动指向新的主库 对应用透明 自动处理故障节点 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:0","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"配置 sentinel 准备 sentinel 配置文件 源码包中有示例配置文件可用，直接复制修改即可 mkdir /data/sentinel cat \u003e /data/sentinel/26379.conf \u003c\u003cEOF protected-mode no daemonize yes logfile \"/data/sentinel/26379.log\" port 26379 dir \"/tmp\" sentinel myid 994ea01af0f13692c13eeda116a0668084bb5e68 # mymaster 是一个自定义名称，客户端通过 sentinel 连接 redis 时需要使用 # 127.0.0.1 6380 是主节点的 ip 和 端口号 # 1 是 sentinel failover 投票数，默认为 sentinel 节点数/2 + 1, 1个节点时为1 # 为了能正常投票得出结果 sentinel 节点数得为奇数个 sentinel monitor mymaster 127.0.0.1 6380 1 sentinel down-after-milliseconds mymaster 5000 sentinel auth-pass mymaster 123 sentinel config-epoch mymaster 3 sentinel leader-epoch mymaster 3 sentinel known-slave mymaster 127.0.0.1 6382 sentinel known-slave mymaster 127.0.0.1 6381 sentinel current-epoch 3 EOF 启动 sentinel 服务 /usr/local/bin/redis-sentinel /data/sentinel/26379.conf redis-sentinel 是 redis-server 的软链接 ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:1","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"连接测试 sentinel sentinel 管理命令 [root@localhost sentinel]# redis-cli -p 26379 # 列出所有被监视的主服务器 127.0.0.1:26379\u003e SENTINEL masters # 列出所有被监视的从服务器 127.0.0.1:26379\u003e SENTINEL slaves \u003cmaster name\u003e # 强制开启 主从切换, 危险 127.0.0.1:26379\u003e SENTINEL failover \u003cmaster name\u003e 测试主从切换 模拟故障停止主库 [root@localhost etc]# redis-cli -p 6380 -a 123 shutdown 查看 sentinel 日志 3963:X 28 Mar 15:31:13.864 # +sdown master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +odown master mymaster 127.0.0.1 6380 #quorum 1/1 3963:X 28 Mar 15:31:13.864 # +new-epoch 6 3963:X 28 Mar 15:31:13.864 # +try-failover master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +vote-for-leader 994ea01af0f13692c13eeda116a0668084bb5e68 6 3963:X 28 Mar 15:31:13.864 # +elected-leader master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.864 # +failover-state-select-slave master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.948 # +selected-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:13.948 * +failover-state-send-slaveof-noone slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.007 * +failover-state-wait-promotion slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.943 # +promoted-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:14.943 # +failover-state-reconf-slaves master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.004 * +slave-reconf-sent slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.994 * +slave-reconf-inprog slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:15.994 * +slave-reconf-done slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:16.055 # +failover-end master mymaster 127.0.0.1 6380 3963:X 28 Mar 15:31:16.055 # +switch-master mymaster 127.0.0.1 6380 127.0.0.1 6382 3963:X 28 Mar 15:31:16.055 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:31:16.055 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:31:21.064 # +sdown slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 可以看到 sentinel 检测到主库服务停止，将 6382 选为主库，进行了切换操作， 6381 也自动与 6382 建立主从关系 6380 修复好后，重新启动服务 sentinel 会自动检测到并 6380 加入主从环境中 [root@localhost etc]# /etc/init.d/redis_6380 start Starting Redis server... 此时可以从 sentinel 日志中看到以下信息 3963:X 28 Mar 15:35:05.234 # -sdown slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 3963:X 28 Mar 15:35:15.229 * +convert-to-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6382 sentinel 会自动在 6380 实例的配置文件最后一行加入一行配置 [root@localhost etc]# tail -n 3 6380.conf requirepass \"123\" masterauth \"123\" slaveof 127.0.0.1 6382 配置多节点 sentinel 此时 redis 的高可用解决了，但 sentinel 只有一个节点存在单点故障，为了解决这个问题，可以通过部署多个 sentinel 节点解决。 配置多个 sentinel 节点时需要注意节点数据及配置文件中的 sentinel monitor mymaster 127.0.0.1 6380 1 配置项的最后一个值的配置。 客户端连接 redis 以 python 客户端为例说明, 参考文档: https://pypi.org/project/redis/ 安装 redis 库 pip install redis 连接至单实例 reids \u003e\u003e\u003e import redis \u003e\u003e\u003e r = redis.Redis(host='localhost', port=6379, db=0) \u003e\u003e\u003e r.set('foo', 'bar') True \u003e\u003e\u003e r.get('foo') b'bar' Sentinel 模式 \u003e\u003e\u003e from redis.sentinel import Sentinel \u003e\u003e\u003e sentinel = Sentinel([('localhost', 26379)], socket_timeout=0.1) \u003e\u003e\u003e sentinel.discover_master('mymaster') ('127.0.0.1', 6379) \u003e\u003e\u003e sentinel.discover_slaves('mymaster') [('127.0.0.1', 6380)] \u003e\u003e\u003e master = sentinel.master_for('mymaster', socket_timeout=0.1, password='123') \u003e\u003e\u003e slave = sentinel.slave_for('mymaster', socket_timeout=0.1, password='123') \u003e\u003e\u003e master.set('foo', 'bar') \u003e\u003e\u003e slave.get('foo') b'bar' ","date":"2021-03-28","objectID":"/posts/redis-sentinel/:2:2","tags":["redis","sentinel"],"title":"Redis Sentinel 高可用","uri":"/posts/redis-sentinel/"},{"categories":["redis"],"content":"Redis 命令 ","date":"2021-03-28","objectID":"/posts/redis-cli/:1:0","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"管理命令 info: 查看 Redis 当前状态信息 127.0.0.1:6379\u003e info # Server redis_version:3.2.9 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:f0e03a357ae83877 redis_mode:standalone os:Linux 3.10.0-862.el7.x86_64 x86_64 arch_bits:64 multiplexing_api:epoll gcc_version:4.8.5 process_id:2502 run_id:c913615b9c199a0d7ba4454c38e1a65427525c60 tcp_port:6379 uptime_in_seconds:2047 uptime_in_days:0 hz:10 lru_clock:6276784 executable:/usr/local/bin/redis-server config_file:/etc/redis/6379.conf # Clients connected_clients:1 client_longest_output_list:0 client_biggest_input_buf:0 blocked_clients:0 ...(略) # 只查看特定的信息 127.0.0.1:6379\u003e info Memory # Memory used_memory:821088 used_memory_human:801.84K used_memory_rss:7888896 used_memory_rss_human:7.52M used_memory_peak:822064 used_memory_peak_human:802.80K total_system_memory:1021902848 total_system_memory_human:974.56M used_memory_lua:37888 used_memory_lua_human:37.00K maxmemory:128000000 maxmemory_human:122.07M maxmemory_policy:noeviction mem_fragmentation_ratio:9.61 mem_allocator:jemalloc-4.0.3 client list: 查看当前连接的客户端列表 client kill \u003cip:port\u003e: 结束客户端连接 127.0.0.1:6379\u003e CLIENT LIST id=3 addr=127.0.0.1:39676 fd=7 name= age=1340 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client # 危险，不要轻易操作 127.0.0.1:6379\u003e CLIENT KILL 127.0.0.1:39676 OK config get/set/rewrite: 在线获取/修改配置信息 config resetstat: 重置统计信息 dbsize: 查看键值对数量 flushall: 清空所有数据，危险操作 flushdb: 清空当前库数据，危险操作 select \u003c序号\u003e: 默认有16个库，切换库 127.0.0.1:6379\u003e select 1 OK 127.0.0.1:6379[1]\u003e select 15 OK monitor: 实时监控操作指令 shutdown: 停止 redis 服务 ","date":"2021-03-28","objectID":"/posts/redis-cli/:1:1","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"key 通用操作命令 keys: 查看所有 key 127.0.0.1:6379\u003e KEYS * 1) \"user.age\" 2) \"user.email\" 3) \"user.name\" 127.0.0.1:6379\u003e KEYS *n* 1) \"user.name\" 127.0.0.1:6379\u003e KEYS user* 1) \"user.age\" 2) \"user.email\" 3) \"user.name\" 注意: 不建议直接使用 keys * 直接查看，数量多的情况下会影响 redis 性能 type: 查看 key 的类型 127.0.0.1:6379\u003e TYPE user.name string 127.0.0.1:6379\u003e TYPE hbb hash expire/pexpire: 以秒/毫秒设置 key 的存活时间 ttl/pttl: 以秒/毫秒返回 key 的存活时间 persist: 取消生存时间设置 127.0.0.1:6379\u003e set te 123 OK 127.0.0.1:6379\u003e EXPIRE te 20 (integer) 1 127.0.0.1:6379\u003e ttl te (integer) 16 127.0.0.1:6379\u003e persist te (integer) 1 127.0.0.1:6379\u003e ttl te (integer) -1 del: 删除一个 key exists: 检查 key 是否存在 rename: 重命名 key 127.0.0.1:6379\u003e exists te (integer) 1 127.0.0.1:6379\u003e del te (integer) 1 127.0.0.1:6379\u003e exists te (integer) 0 127.0.0.1:6379\u003e set te adf OK 127.0.0.1:6379\u003e rename te tem OK 127.0.0.1:6379\u003e get tem \"adf\" 127.0.0.1:6379\u003e exists tem (integer) 1 ","date":"2021-03-28","objectID":"/posts/redis-cli/:1:2","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"Redis 数据类型 redis 支持的数据类型如下 String： 字符类型 Hash：字典类型 List：列表 Set：集合 Sorted set：有序集合 ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:0","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"string 应用场景: session 共享 计数器：微博数，粉丝数，订阅、礼物 字符串类型操作命令 # 设置键的字符串值 127.0.0.1:6379\u003e set mykey 0 OK # 获取键的值 127.0.0.1:6379\u003e get mykey \"0\" # 设置键的字符串值并返回其旧值 127.0.0.1:6379\u003e getset key2 2 (nil) # 设置键的值和有效期(单位秒) 127.0.0.1:6379\u003e setex time 10 10 OK 127.0.0.1:6379\u003e ttl time (integer) 7 # 仅当密钥不存在时设置密钥的值 127.0.0.1:6379\u003e setnx k3 3 (integer) 1 127.0.0.1:6379\u003e get k3 \"3\" # 将键的整数值加1 127.0.0.1:6379\u003e incr sar (integer) 1 # 将键的整数值减1 127.0.0.1:6379\u003e decr sar (integer) 0 # 将键的整数值增加给定的数量 127.0.0.1:6379\u003e incrby sar 100 (integer) 100 # 将键的整数值减少给定的数量 127.0.0.1:6379\u003e decrby sar 15 (integer) 85 # 同时为多个键设置多个值 127.0.0.1:6379\u003e mset k1 1 k2 2 k3 3 OK # 同时获取多个键的值 127.0.0.1:6379\u003e mget k1 k2 k3 1) \"1\" 2) \"2\" 3) \"3\" # 检测键是否存在 127.0.0.1:6379\u003e exists k1 (integer) 1 # 获取键值的长度 127.0.0.1:6379\u003e strlen k2 (integer) 1 ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:1","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"hash 应用场景: 存储部分变更的数据，如用户信息等, 最接近 mysql 表结构的一种类型,主要是可以做数据库缓存 hash 字典类型操作命令 # 设置字典 127.0.0.1:6379\u003e hmset stu id 101 name zhangsan age 20 gender m OK # 获取字典多个字段值 127.0.0.1:6379\u003e hmget stu name age gender 1) \"zhangsan\" 2) \"20\" 3) \"m\" # 获取 stu 键的字段数量 127.0.0.1:6379\u003e hlen stu (integer) 4 # 判断 stu 键中是否存在 name 的字段 127.0.0.1:6379\u003e hexists stu name (integer) 1 # 返回 stu 键的所有字段和值 127.0.0.1:6379\u003e hgetall stu 1) \"id\" 2) \"101\" 3) \"name\" 4) \"zhangsan\" 5) \"age\" 6) \"20\" 7) \"gender\" 8) \"m\" # 获取 stu 所有字段名称 127.0.0.1:6379\u003e hkeys stu 1) \"id\" 2) \"name\" 3) \"age\" 4) \"gender\" # 获取 stu 所有字段的值 127.0.0.1:6379\u003e hvals stu 1) \"101\" 2) \"zhangsan\" 3) \"20\" 4) \"m\" ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:2","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"list 应用场景: 消息队列系统，社交类朋友圈 创建一个列表 每次插入的数据都会放在最前面，第一个索引号为 0 127.0.0.1:6379\u003e lpush wechat \"today is nice day !\" (integer) 1 127.0.0.1:6379\u003e lpush wechat \"today is bad day !\" (integer) 2 127.0.0.1:6379\u003e lpush wechat \"today is good day !\" (integer) 3 127.0.0.1:6379\u003e lpush wechat \"today is rainy day !\" (integer) 4 127.0.0.1:6379\u003e lpush wechat \"today is friday day !\" (integer) 5 获取列表中的数据 # 取最新1条数据 127.0.0.1:6379\u003e lrange wechat 0 0 1) \"today is friday day !\" # 取所有数据 127.0.0.1:6379\u003e lrange wechat 0 -1 1) \"today is friday day !\" 2) \"today is rainy day !\" 3) \"today is good day !\" 4) \"today is bad day !\" 5) \"today is nice day !\" # 取最新的前3条数据 127.0.0.1:6379\u003e lrange wechat 0 2 1) \"today is friday day !\" 2) \"today is rainy day !\" 3) \"today is good day !\" # 取最后2条数据 127.0.0.1:6379\u003e lrange wechat -2 -1 1) \"today is bad day !\" 2) \"today is nice day !\" 列表的 增、删、改、查 操作命令 # 增 lpush mykey a b 若key不存在,创建该键及与其关联的List,依次插入a ,b， 若List类型的key存在,则插入value中 lpushx mykey2 e 若key不存在,此命令无效， 若key存在,则插入value中 linsert mykey before a a1 在 a 的前面插入新元素 a1 linsert mykey after e e2 在e 的后面插入新元素 e2 rpush mykey a b 在链表尾部先插入b,在插入a rpushx mykey e 若key存在,在尾部插入e, 若key不存在,则无效 rpoplpush mykey mykey2 将mykey的尾部元素弹出,再插入到mykey2 的头部(原子性的操作) # 删 del mykey 删除已有键 lrem mykey 2 a 从头部开始找,按先后顺序,值为a的元素,删除数量为2个,若存在第3个,则不删除 ltrim mykey 0 2 从头开始,索引为0,1,2的3个元素,其余全部删除 # 改 lset mykey 1 e 从头开始, 将索引为1的元素值,设置为新值 e,若索引越界,则返回错误信息 rpoplpush mykey mykey 将 mykey 中的尾部元素移到其头部 # 查 lrange mykey 0 -1 取链表中的全部元素，其中0表示第一个元素,-1表示最后一个元素。 lrange mykey 0 2 从头开始,取索引为0,1,2的元素 lrange mykey 0 0 从头开始,取第一个元素,从第0个开始,到第0个结束 lpop mykey 获取头部元素,并且弹出头部元素,出栈 lindex mykey 6 从头开始,获取索引为6的元素 若下标越界,则返回nil ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:3","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"set 案例：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合中。 Redis 还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能， 对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 127.0.0.1:6379\u003e sadd lxl pg1 jnl baoqiang gsy alexsb (integer) 5 127.0.0.1:6379\u003e sadd jnl baoqiang ms bbh yf wxg (integer) 5 # 求并集 127.0.0.1:6379\u003e sunion lxl jnl 1) \"bbh\" 2) \"baoqiang\" 3) \"pg1\" 4) \"alexsb\" 5) \"gsy\" 6) \"ms\" 7) \"yf\" 8) \"jnl\" 9) \"wxg\" # 求交集 127.0.0.1:6379\u003e sinter lxl jnl 1) \"baoqiang\" # 求差集 127.0.0.1:6379\u003e sdiff lxl jnl 1) \"alexsb\" 2) \"pg1\" 3) \"gsy\" 4) \"jnl\" 增、删、改、查 # 增 sadd myset a b c 若 key 不存在, 创建该键及与其关联的 set, 依次插入a ,b, 若key存在, 则插入value中, 若 a 在myset中已经存在,则插入了 d 和 e 两个新成员。 # 删 spop myset 尾部的 b 被移出, 事实上 b 并不是之前插入的第一个或最后一个成员 srem myset a d f 若 f 不存在, 移出 a、d ,并返回2 # 改 smove myset myset2 a 将a从 myset 移到 myset2， # 查 sismember myset a 判断 a 是否已经存在，返回值为 1 表示存在。 smembers myset 查看 set 中的内容 scard myset 获取 set 集合中元素的数量 srandmember myset 随机的返回某一成员 sdiff myset1 myset2 myset3 1和2得到一个结果,拿这个集合和3比较,获得每个独有的值 sdiffstore diffkey myset myset2 myset3 3个集和比较,获取独有的元素,并存入diffkey 关联的Set中 sinter myset myset2 myset3 获得3个集合中都有的元素 sinterstore interkey myset myset2 myset3 把交集存入interkey 关联的Set中 sunion myset myset2 myset3 获取3个集合中的成员的并集 sunionstore unionkey myset myset2 myset3 把并集存入unionkey 关联的Set中 ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:4","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["redis"],"content":"sorted set 应用场景： 排行榜应用，取 TOP N 操作 这个需求与上面需求的不同之处在于，前面操作以时间为权重，这个是以某个条件为权重，比如按顶的次数排序， 这时候就需要我们的 sorted set 出马了，将你要排序的值设置成 sorted set 的score，将具体的数据设置成相应的 value，每次只需要执行一条 ZADD 命令即可。 127.0.0.1:6379\u003e zadd topN 0 smlt 0 fskl 0 fshkl 0 lzlsfs 0 wdhbx 0 wxg (integer) 6 127.0.0.1:6379\u003e ZINCRBY topN 100000 smlt \"100000\" 127.0.0.1:6379\u003e ZINCRBY topN 10000 fskl \"10000\" 127.0.0.1:6379\u003e ZINCRBY topN 1000000 fshkl \"1000000\" 127.0.0.1:6379\u003e ZINCRBY topN 100 lzlsfs \"100\" 127.0.0.1:6379\u003e ZINCRBY topN 100000000 wxg \"100000000\" 127.0.0.1:6379\u003e ZREVRANGE topN 0 2 1) \"wxg\" 2) \"fshkl\" 3) \"smlt\" 127.0.0.1:6379\u003e ZREVRANGE topN 0 2 withscores 1) \"wxg\" 2) \"100000000\" 3) \"fshkl\" 4) \"1000000\" 5) \"smlt\" 6) \"100000\" 增、删、改、查 # 增 zadd myzset 2 \"two\" 3 \"three\" 添加两个分数分别是 2 和 3 的两个成员 # 删 zrem myzset one two 删除多个成员变量,返回删除的数量 # 改 zincrby myzset 2 one 将成员 one 的分数增加 2，并返回该成员更新后的分数 # 查 zrange myzset 0 -1 WITHSCORES 返回所有成员和分数,不加WITHSCORES,只返回成员 zrank myzset one 获取成员one在Sorted-Set中的位置索引值。0表示第一个位置 zcard myzset 获取 myzset 键中成员的数量 zcount myzset 1 2 获取分数满足表达式 1 \u003c= score \u003c= 2 的成员的数量 zscore myzset three 获取成员 three 的分数 zrangebyscore myzset 1 2 获取分数满足表达式 1 \u003c score \u003c= 2 的成员 #-inf 表示第一个成员，+inf最后一个成员 #limit限制关键字 #2 3 是索引号 zrangebyscore myzset -inf +inf limit 2 3 返回索引是2和3的成员 zremrangebyscore myzset 1 2 删除分数 1\u003c= score \u003c= 2 的成员，并返回实际删除的数量 zremrangebyrank myzset 0 1 删除位置索引满足表达式 0 \u003c= rank \u003c= 1 的成员 zrevrange myzset 0 -1 WITHSCORES 按位置索引从高到低,获取所有成员和分数 #原始成员:位置索引从小到大 one 0 two 1 #执行顺序:把索引反转 位置索引:从大到小 one 1 two 0 #输出结果: two one zrevrange myzset 1 3 获取位置索引,为1,2,3的成员 #相反的顺序:从高到低的顺序 zrevrangebyscore myzset 3 0 获取分数 3\u003e=score\u003e=0的成员并以相反的顺序输出 zrevrangebyscore myzset 4 0 limit 1 2 获取索引是1和2的成员,并反转位置索引 ","date":"2021-03-28","objectID":"/posts/redis-cli/:2:5","tags":["redis"],"title":"Redis 基础操作","uri":"/posts/redis-cli/"},{"categories":["kubernetes"],"content":"部署 Prometheus ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:1:0","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"准备资源配置清单 我们将 prometheus.yml 配置文件以 configmap 的形式存储在 kubernetes 集群中。(configmap.yaml) apiVersion:v1kind:ConfigMapmetadata:name:prometheus-cfgnamespace:monitordata:prometheus.yml:|global: scrape_interval: 30s scrape_timeout: 30s evaluation_interval: 1m scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 30s scrape_timeout: 30s metrics_path: /metrics scheme: http follow_redirects: true static_configs: - targets: - localhost:9090 - job_name: kubernetes-node-exporter honor_timestamps: true scrape_interval: 30s scrape_timeout: 30s metrics_path: /metrics scheme: http bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false follow_redirects: true relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_role] separator: ; regex: (.*) target_label: kubernetes_role replacement: $1 action: replace - source_labels: [__address__] separator: ; regex: (.*):10250 target_label: __address__ replacement: ${1}:9100 action: replace kubernetes_sd_configs: - role: node follow_redirects: true - job_name: kubernetes-node-kubelet honor_timestamps: true scrape_interval: 30s scrape_timeout: 30s metrics_path: /metrics scheme: https bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true follow_redirects: true relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap kubernetes_sd_configs: - role: node follow_redirects: true - job_name: traefik honor_timestamps: true scrape_interval: 30s scrape_timeout: 30s metrics_path: /metrics scheme: http follow_redirects: true static_configs: - targets: - traefik.kube-system.svc.cluster.local:8080 - job_name: kubernetes-apiservers kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: keep source_labels: - __meta_kubernetes_namespace - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name regex: default;kubernetes;https - job_name: kubernetes-service-endpoints kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scrape - action: replace regex: (https?) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_service_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_service_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: kubernetes_namespace - action: replace source_labels: - __meta_kubernetes_service_name target_label: kubernetes_name # 监控 service - job_name: kubernetes-services kubernetes_sd_configs: - role: service metrics_path: /probe params: module: - http_2xx relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_probe - source_labels: - __address__ target_label: __param_target - replacement: blackbox target_label: __address__ - source_labels: - __param_target target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: - __meta_kubernetes_namespace target_label: kubernetes_namespace - source_labels: - __meta_kubernetes_service_name target_label: kubernetes_name # 监控 pod - job_name: kubernetes-pods kubernetes_sd_configs: - role: pod relabel_configs: - action: keep regex: true ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:1:1","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"部署 Prometheus 相关资源 按顺序应用资源配置清单 kubectl apply -f rbac.yaml kubectl apply -f pvc.yaml kubectl apply -f configmap.yaml kubectl apply -f deployment.yaml 使用 traefik 暴露 prometheus 服务: ingress-route.yaml apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:prometheusnamespace:monitorspec:entryPoints:- webroutes:- match:Host(`mon.host.com`)kind:Ruleservices:- name:prometheusport:9090 应用 kubectl apply -f ingress-route.yaml ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:1:2","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"部署 node-exporter 监控所有节点 准备 node-exporter.yaml apiVersion:apps/v1kind:DaemonSetmetadata:labels:app:node-exportername:node-exporternamespace:monitorspec:selector:matchLabels:app:node-exportertemplate:metadata:labels:app:node-exporterspec:hostPID:truehostIPC:truehostNetwork:truecontainers:- name:node-exporterimage:prom/node-exporter:v1.1.2command:- \"/bin/node_exporter\"args:- \"--path.procfs=/host/proc\"- \"--path.sysfs=/host/sys\"- \"--collector.filesystem.ignored-mount-points='^/(dev|proc|sys|host|etc)($|/)'\"ports:- containerPort:9100protocol:TCPname:httpsecurityContext:privileged:truevolumeMounts:- name:devmountPath:\"/host/dev\"- name:procmountPath:\"/host/proc\"- name:sysmountPath:\"/host/sys\"- name:rootfsmountPath:\"/rootfs\"resources:requests:cpu:100mmemory:128Milimits:cpu:100mmemory:128Mitolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"volumes:- name:prochostPath:path:/proc- name:devhostPath:path:/dev- name:syshostPath:path:/sys- name:rootfshostPath:path:/ 由于我们需要监控所有节点包括主节点，所有需要忽略主节点上的污点, 加入 tolerations 配置项，使用 DaemonSet 方式部署 kubectl apply -f node-exporter.yaml 查看 Prometheus 监控 Targets 状态 ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:2:0","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"使用 Grafana 展示 Prometheus 监控数据 ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:3:0","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"部署 Grafana grafana.yaml ---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:grafananamespace:monitorspec:storageClassName:managed-nfs-storageaccessModes:- ReadWriteManyresources:requests:storage:1Gi---apiVersion:apps/v1kind:Deploymentmetadata:name:grafananamespace:monitorlabels:app:grafanaspec:revisionHistoryLimit:10selector:matchLabels:app:grafanatemplate:metadata:labels:app:grafanaspec:containers:- name:grafanaimage:grafana/grafana:7.5.4imagePullPolicy:IfNotPresentports:- containerPort:3000name:grafanaenv:- name:GF_SECURITY_ADMIN_USERvalue:admin- name:GF_SECURITY_ADMIN_PASSWORDvalue:admin321readinessProbe:failureThreshold:10httpGet:path:/api/healthport:3000scheme:HTTPinitialDelaySeconds:60periodSeconds:10successThreshold:1timeoutSeconds:30livenessProbe:failureThreshold:3httpGet:path:/api/healthport:3000scheme:HTTPperiodSeconds:10successThreshold:1timeoutSeconds:1resources:limits:cpu:100mmemory:256Mirequests:cpu:100mmemory:256MivolumeMounts:- mountPath:/var/lib/grafanasubPath:grafananame:storagesecurityContext:runAsUser:0volumes:- name:storagepersistentVolumeClaim:claimName:grafana---apiVersion:v1kind:Servicemetadata:labels:app:grafananame:grafananamespace:monitorspec:ports:- name:\"http\"port:3000protocol:TCPtargetPort:3000selector:app:grafanatype:ClusterIP---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:grafananamespace:monitorspec:entryPoints:- webroutes:- match:Host(`grafana.host.com`)kind:Ruleservices:- name:grafanaport:3000 应用 grafana.yaml kubectl apply -f grafana.yml ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:3:1","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes"],"content":"安装 Grafana 插件 插件安装有两种方式： 进入 Container 中，执行 grafana-cli plugins install $plugin_name 手动下载插件zip包，访问 https://grafana.com/grafana/plugins/ 下载 zip 包解压到 /var/lib/grafana/plugins 目录中 插件安装完毕后，需要重启 Grafana 的 Pod 进入容器运行如下命令安装插件 grafana-cli plugins install grafana-kubernetes-app grafana-cli plugins install grafana-clock-panel grafana-cli plugins install grafana-piechart-panel grafana-cli plugins install briangann-gauge-panel grafana-cli plugins install natel-discrete-panel 配置 prometheus 数据源 浏览器打开 grafana.host.com 使用用户名: admin, 密码: admin321 登录，进入 Configuration -\u003e Data Sources -\u003e Add Data Source 选择 Prometheus Url 为 http://prometheus:9090 Access: Server(default) 其他均为默认。 配置 kubernetes 插件 进入插件管理页面，先启用 kubernetes 插件 配置 kubernetes 插件 ","date":"2021-03-19","objectID":"/posts/kubernetes-prometheus/:3:2","tags":["kubernetes","prometheus"],"title":"使用 Prometheus 监控 Kubernetes 集群","uri":"/posts/kubernetes-prometheus/"},{"categories":["kubernetes","traefik"],"content":"Traefik 灰度发布概述 Traefik2.0 的一个更强大的功能就是灰度发布，灰度发布我们有时候也会称为金丝雀发布（Canary），主要就是让一部分测试的服务也参与到线上去，经过测试观察看是否符号上线要求 ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-canary/:1:0","tags":["kubernetes","traefik"],"title":"使用 Traefik-2.4 进行灰度发布","uri":"/posts/kubernetes-traefik-canary/"},{"categories":["kubernetes","traefik"],"content":"测试灰度发布 比如现在我们有两个名为 appv1 和 appv2 的服务，我们希望通过 Traefik 来控制我们的流量，将 3⁄4 的流量路由到 appv1，1/4 的流量路由到 appv2 去，这个时候就可以利用 Traefik2.0 中提供的带权重的轮询（WRR）来实现该功能，首先在 Kubernetes 集群中部署上面的两个服务。为了对比结果我们这里提供的两个服务一个是 whoami，一个是 nginx，方便测试。 appv1 服务的资源清单如下所示：（appv1.yaml） apiVersion:apps/v1kind:Deploymentmetadata:name:appv1spec:selector:matchLabels:app:appv1template:metadata:labels:use:testapp:appv1spec:containers:- name:whoamiimage:traefik/whoamiports:- containerPort:80name:portv1---apiVersion:v1kind:Servicemetadata:name:appv1spec:selector:app:appv1ports:- name:httpport:80targetPort:portv1 appv2 服务的资源清单如下所示：（appv2.yaml） apiVersion:apps/v1kind:Deploymentmetadata:name:appv2spec:selector:matchLabels:app:appv2template:metadata:labels:use:testapp:appv2spec:containers:- name:nginximage:nginxports:- containerPort:80name:portv2---apiVersion:v1kind:Servicemetadata:name:appv2spec:selector:app:appv2ports:- name:httpport:80targetPort:portv2 直接创建上面两个服务： kubectl apply -f appv1.yaml kubectl apply -f appv2.yaml 在 Traefik 2.1 中新增了一个 TraefikService 的 CRD 资源，我们可以直接利用这个对象来配置 WRR，之前的版本需要通过 File Provider，比较麻烦，新建一个描述 WRR 的资源清单：(wrr.yaml) apiVersion:traefik.containo.us/v1alpha1kind:TraefikServicemetadata:name:app-wrrspec:weighted:services:- name:appv1weight:3# 定义权重port:80kind:Service # 可选，默认就是 Service- name:appv2weight:1port:80 然后为我们的灰度发布的服务创建一个 IngressRoute 资源对象：(ingressroute.yaml) apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:wrringressroutenamespace:defaultspec:entryPoints:- webroutes:- match:Host(`wrr.wglee.cn`)kind:Ruleservices:- name:app-wrrkind:TraefikService 不过需要注意的是现在我们配置的 Service 不再是直接的 Kubernetes 对象了，而是上面我们定义的 TraefikService 对象，直接创建上面的两个资源对象，这个时候我们对域名 wrr.wglee.cn 做上解析，去浏览器中连续访问 4 次，我们可以观察到 appv1 这应用会收到 3 次请求，而 appv2 这个应用只收到 1 次请求，符合上面我们的 3:1 的权重配置。 ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-canary/:2:0","tags":["kubernetes","traefik"],"title":"使用 Traefik-2.4 进行灰度发布","uri":"/posts/kubernetes-traefik-canary/"},{"categories":["kubernetes","traefik"],"content":"简单 TCP 服务 首先部署一个普通的 mongo 服务，资源清单文件如下所示：（mongo.yaml） apiVersion:apps/v1kind:Deploymentmetadata:name:mongo-traefiklabels:app:mongo-traefikspec:selector:matchLabels:app:mongo-traefiktemplate:metadata:labels:app:mongo-traefikspec:containers:- name:mongoimage:mongo:4.0ports:- containerPort:27017---apiVersion:v1kind:Servicemetadata:name:mongo-traefikspec:selector:app:mongo-traefikports:- port:27017 直接创建 mongo 应用： kubectl apply -f mongo.yaml ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-tcp/:1:0","tags":["kubernetes","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 TCP 协议","uri":"/posts/kubernetes-traefik-tcp/"},{"categories":["kubernetes","traefik"],"content":"ingressroute-tcp 创建成功后就可以来为 mongo 服务配置一个路由了。由于 Traefik 中使用 TCP 路由配置需要 SNI，而 SNI 又是依赖 TLS 的，所以我们需要配置证书才行，如果没有证书的话，我们可以使用通配符 * 进行配置，我们这里创建一个 IngressRouteTCP 类型的 CRD 对象（前面我们就已经安装了对应的 CRD 资源）：(mongo-ingressroute-tcp.yaml) apiVersion:traefik.containo.us/v1alpha1kind:IngressRouteTCPmetadata:name:mongo-traefik-tcpspec:entryPoints:- tcpeproutes:- match:HostSNI(`*`)services:- name:mongo-traefikport:27017 要注意的是这里的 entryPoints 部分，是根据我们启动的 Traefik 的静态配置中的 entryPoints 来决定的，我们当然可以使用前面我们定义得 80 和 443 这两个入口点，但是也可以可以自己添加一个用于 mongo 服务的专门入口点, 这里我们使用 tcpep 这个入口 关于 entryPoints 入口点的更多信息，可以查看文档 entrypoints 了解更多信息。 然后更新 Traefik 后我们就可以直接创建上面的资源对象： kubectl apply -f mongo-ingressroute-tcp.yaml 创建完成后，同样我们可以去 Traefik 的 Dashboard 页面上查看是否生效, 然后我们配置一个域名 mongo.local 解析到 Traefik 所在的节点，然后通过 8000 端口来连接 mongo 服务： mongo --host mongo.local --port 8000 mongo(75243,0x1075295c0) malloc: *** malloc_zone_unregister() failed for 0x7fffa56f4000 MongoDB shell version: 2.6.1 connecting to: mongo.local:8000/test \u003e show dbs admin 0.000GB config 0.000GB local 0.000GB ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-tcp/:2:0","tags":["kubernetes","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 TCP 协议","uri":"/posts/kubernetes-traefik-tcp/"},{"categories":["kubernetes","traefik"],"content":"部署测试 web 应用 使用 Deployment 部署 nginx， 启动两个 pod 实例， 资源配置清单 nginx.yaml 如下： apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginxname:nginxnamespace:defaultspec:replicas:2selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:stable-alpine---apiVersion:v1kind:Servicemetadata:labels:app:nginxname:nginxnamespace:defaultspec:ports:- name:\"http\"port:80protocol:TCPtargetPort:80selector:app:nginxtype:ClusterIP 在集群中应用 nginx.yaml kubectl apply -f nginx.yaml ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-web/:1:0","tags":["kubernetes","k8s","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 Web 服务","uri":"/posts/kubernetes-traefik-web/"},{"categories":["kubernetes","traefik"],"content":"配置 traefik IngressRoute ingress-route.yaml apiVersion:traefik.containo.us/v1alpha1# 使用 ingress routekind:IngressRoutemetadata:name:nginxnamespace:defaultspec:# 指定使用的入口，由于是 http 流量，我们使用 web 入口（入口在 traefik 命令行配置参数自行配置）entryPoints:- webroutes:# 指定匹配规则- match:Host(`www.host.com`)kind:Ruleservices:- name:nginxport:80 在集群中应用 ingress-route.yaml kubectl apply -f ingress-route.yaml 在内部使用 http 可以减少管理证书的麻烦及简化配置的步骤，我们只要在最外围的反代上配置成 https 即可。 如果需要使用 traefik 暴露 https 流量，可以下面步骤操作 1.使用 cfssl 命令自签 www.host.com 域名证书，参考 使用 cfssl 自签证书 2.将自签证书存入 k8s 集群中，以 secret 资源存储 kubectl create secret tls host-com-tls --cert=host.pem --key=host-key.pem 要注意证书文件名称必须是 tls.crt 和 tls.key 3.配置 IngressRoute 的 entryPoints 为 websecure, 并启用 tls 配置 apiVersion:traefik.containo.us/v1alpha1# 使用 ingress routekind:IngressRoutemetadata:name:nginxnamespace:defaultspec:# 指定使用的入口，由于是 http 流量，我们使用 web 入口（入口在 traefik 命令行配置参数自行配置）entryPoints:- websecureroutes:# 指定匹配规则- match:Host(`www.host.com`)kind:Ruleservices:- name:nginxport:80tls:secretName:host-com-tls 在配置 nginx 反向代理时后端 web 服务的端口此时就成了 443 了，协议为 https ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-web/:2:0","tags":["kubernetes","k8s","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 Web 服务","uri":"/posts/kubernetes-traefik-web/"},{"categories":["kubernetes","traefik"],"content":"配置负载均衡反向代理 我们把只要是 www.host.com 这个域名的流量（配置中使用通配符）统一反代到 traefik 的 web 入口 upstream k8s_services { server 10.7.50.17; server 10.7.149.184; } server { listen 80; server_name *.host.com; location / { proxy_pass http://k8s_services; include proxy.conf; } } ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-web/:3:0","tags":["kubernetes","k8s","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 Web 服务","uri":"/posts/kubernetes-traefik-web/"},{"categories":["kubernetes","traefik"],"content":"查看及测试 打开 traefik-dashboard 页面，查看配置的规则是否生效，使用域名访问此 web 服务。 ","date":"2021-03-10","objectID":"/posts/kubernetes-traefik-web/:4:0","tags":["kubernetes","k8s","traefik"],"title":"使用 Traefik-2.4 暴露 Kubernetes 内部 Web 服务","uri":"/posts/kubernetes-traefik-web/"},{"categories":["kubernetes"],"content":"部署 Metrics Server 在 kubernetes 中 HPA 自动伸缩指标依据，kubectl top 命令的资源使用率，可以通过 metrics-server 来获取。 dashboard 也会引用 metrics-server 展示资源负载情况图表。但是官方明确表示，该指标不应该用于监控指标采集。 官方主页: https://github.com/kubernetes-sigs/metrics-server 在大部分情况下，使用 deployment 部署一个副本即可，最多支持5000个 node，每个 node 消耗 3m CPU 和 3M 内存 可以从官方主页获取资源配置清单文件，也可以直接使用下面的配置文件。 metrics-server 配置参数 - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --kubelet-insecure-tls 部署 metrics-server kubectl apply -f metrics-server.yaml 由于国内环境导致无法下载 metrics-server 镜像，在网上也没有找到可用的代理的服务器。请自行解决此问题。 ","date":"2021-03-09","objectID":"/posts/kubernetes-metricsserver/:1:0","tags":["kubernetes","metrics-server"],"title":"使用 metrics-server 采集 Kubernetes 集群性能指标","uri":"/posts/kubernetes-metricsserver/"},{"categories":["kubernetes"],"content":"Dashboard 页面效果展示 metrics-server 安装完成后可以在 dashboard 页面中看到效果 dashboard metrics ","date":"2021-03-09","objectID":"/posts/kubernetes-metricsserver/:1:1","tags":["kubernetes","metrics-server"],"title":"使用 metrics-server 采集 Kubernetes 集群性能指标","uri":"/posts/kubernetes-metricsserver/"},{"categories":["kubernetes"],"content":"Dashboard 介绍 Kubernetes Dashboard 是 Kubernetes 集群的基于 Web 的通用 UI。 它允许用户管理群集中运行的应用程序并对其进行故障排除，以及管理群集本身。 Dashboard 的 Github 主页：https://github.com/kubernetes/dashboard ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:1:0","tags":["kubernetes","dashboard"],"title":"使用 Dashborad 管理 Kubernetes 集群","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"部署 Dashboard Dashboard 默认的 YAML 配置文件中是采用 HTTPS 方式访问，本实验中前端 SLB 采用 HTTP 与后端集群中 Ingress 通信，因此需要对该 YAML 文件进行改造，否则会出现以客户端 HTTP 方式请求 HTTPS 接口的报错。 由于 Dashboard 使用自签发的证书, 在连接及配置反向代理多有不便，建议将其改为 HTTP 方式，然后在 nginx 反代处加证书。 从官网下载 Dashboard YAML 配置文件，进行修改，配置如下: # Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion:v1kind:Namespacemetadata:name:kubernetes-dashboard---apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:ServiceapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:ports:# 配置关联 HTTP 服务端口- port:80targetPort:8080#- port: 443# targetPort: 8443selector:k8s-app:kubernetes-dashboard---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-certsnamespace:kubernetes-dashboardtype:Opaque---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-csrfnamespace:kubernetes-dashboardtype:Opaquedata:csrf:\"a36ff54b93460ed9c987ca71a618659e\"---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-key-holdernamespace:kubernetes-dashboardtype:Opaque---kind:ConfigMapapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-settingsnamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardrules:# Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups:[\"\"]resources:[\"secrets\"]resourceNames:[\"kubernetes-dashboard-key-holder\",\"kubernetes-dashboard-certs\",\"kubernetes-dashboard-csrf\"]verbs:[\"get\",\"update\",\"delete\"]# Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups:[\"\"]resources:[\"configmaps\"]resourceNames:[\"kubernetes-dashboard-settings\"]verbs:[\"get\",\"update\"]# Allow Dashboard to get metrics.- apiGroups:[\"\"]resources:[\"services\"]resourceNames:[\"heapster\",\"dashboard-metrics-scraper\"]verbs:[\"proxy\"]- apiGroups:[\"\"]resources:[\"services/proxy\"]resourceNames:[\"heapster\",\"http:heapster:\",\"https:heapster:\",\"dashboard-metrics-scraper\",\"http:dashboard-metrics-scraper\"]verbs:[\"get\"]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\"metrics.k8s.io\"]resources:[\"pods\",\"nodes\"]verbs:[\"get\",\"list\",\"watch\"]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:DeploymentapiVersion:apps/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:k8s-app:kubernetes-dashboardtemplate:metadata:labels:k8s-app:kubernetes-dashboardspec:containers:- name:kubernetes-dashboardimage:kubernetesui/dashboard:v2.0.3","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:2:0","tags":["kubernetes","dashboard"],"title":"使用 Dashborad 管理 Kubernetes 集群","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"使用 Ingress 暴露 Dashboard 服务 使用 traefik 2.x 版本实现 Ingress 功能，准备 ingressroute.yaml 资源配置清单 apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:dashboard-ingressnamespace:kubernetes-dashboardspec:entryPoints:- webroutes:- match:Host(`dashboard.host.com`)kind:Ruleservices:- name:kubernetes-dashboardport:80 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:3:0","tags":["kubernetes","dashboard"],"title":"使用 Dashborad 管理 Kubernetes 集群","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"配置 nginx 反代 由于 dashboard 不允许使用 http 登录，所以需要在 nginx 反向代理配置文件中配置 tls 证书 upstream www_pools { server 10.7.50.17; server 10.7.149.184; } server { listen 443 ssl; server_name dashboard.host.com; ssl_certificate ssl/wglee.cn.crt; ssl_certificate_key ssl/wglee.cn.key; ssl_ciphers \"TLS13-AES-256-GCM-SHA384:TLS13-CHACHA20-POLY1305-SHA256:TLS13-AES-128-GCM-SHA256:TLS13-AES-128-CCM-8-SHA256:TLS13-AES-128-CCM-SHA256:EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5\"; ssl_session_cache builtin:1000 shared:SSL:10m; location / { proxy_pass http://www_pools; include proxy.conf; } } server { listen 80; server_name dashboard.host.com; access_log off; return 301 https://$host$request_uri; } 此时我们可以通过域名访问 kubernetes-dashboard 页面 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:4:0","tags":["kubernetes","dashboard"],"title":"使用 Dashborad 管理 Kubernetes 集群","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes"],"content":"创建集群管理员账号 由于 kubernetes-dashboard 需要使用集群用户验证，我们先准备集群管理员账号配置清单 admin-user.yaml ---apiVersion:v1kind:ServiceAccountmetadata:name:adminnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:adminroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:adminnamespace:kubernetes-dashboard 应用 admin-user.yaml 配置，在 kubernetes-dashboard 名称空间里创建集群管理员账号。 获取 admin 管理员用户的 token kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" 使用 admin 管理员 token 登录 dashboard 管理页面 ","date":"2021-03-09","objectID":"/posts/kubernetes-dashboard/:5:0","tags":["kubernetes","dashboard"],"title":"使用 Dashborad 管理 Kubernetes 集群","uri":"/posts/kubernetes-dashboard/"},{"categories":["kubernetes","traefik"],"content":"Traefik 简介 Traefik 是一个开源的可以使服务发布变得轻松有趣的边缘路由器。它负责接收你系统的请求，然后使用合适的组件来对这些请求进行处理。 除了众多的功能之外，Traefik 的与众不同之处还在于它会自动发现适合你服务的配置。当 Traefik 在检查你的服务时，会找到服务的相关信息并找到合适的服务来满足对应的请求。 Traefik 兼容所有主流的集群技术，比如 Kubernetes，Docker，Docker Swarm，AWS，Mesos，Marathon，等等；并且可以同时处理多种方式。（甚至可以用于在裸机上运行的比较旧的软件。） 使用 Traefik，不需要维护或者同步一个独立的配置文件：因为一切都会自动配置，实时操作的（无需重新启动，不会中断连接）。使用 Traefik，你可以花更多的时间在系统的开发和新功能上面，而不是在配置和维护工作状态上面花费大量时间。 官方文档 – https://doc.traefik.io/traefik/ ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:1:0","tags":["kubernetes","traefik"],"title":"Kubernetes 部署 Traefik 2.4","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes","traefik"],"content":"核心概念 Traefik 是一个边缘路由器，是你整个平台的大门，拦截并路由每个传入的请求：它知道所有的逻辑和规则，这些规则确定哪些服务处理哪些请求；传统的反向代理需要一个配置文件，其中包含路由到你服务的所有可能路由，而 Traefik 会实时检测服务并自动更新路由规则，可以自动服务发现。 首先，当启动 Traefik 时，需要定义 entrypoints（入口点），然后，根据连接到这些 entrypoints 的路由来分析传入的请求，来查看他们是否与一组规则相匹配，如果匹配，则路由可能会将请求通过一系列中间件转换过后再转发到你的服务上去。在了解 Traefik 之前有几个核心概念我们必须要了解： Providers 用来自动发现平台上的服务，可以是编排工具、容器引擎或者 key-value 存储等，比如 Docker、Kubernetes、File Entrypoints 监听传入的流量（端口等…），是网络入口点，它们定义了接收请求的端口（HTTP 或者 TCP）。 Routers 分析请求（host, path, headers, SSL, …），负责将传入请求连接到可以处理这些请求的服务上去。 Services 将请求转发给你的应用（load balancing, …），负责配置如何获取最终将处理传入请求的实际服务。 Middlewares 中间件，用来修改请求或者根据请求来做出一些判断（authentication, rate limiting, headers, …），中间件被附件到路由上，是一种在请求发送到你的服务之前（或者在服务的响应发送到客户端之前）调整请求的一种方法。 ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:2:0","tags":["kubernetes","traefik"],"title":"Kubernetes 部署 Traefik 2.4","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes","traefik"],"content":"安装 Traefik 2.4 由于 Traefik 2.x 版本和之前的 1.x 版本不兼容，我们这里选择功能更加强大的 2.x 版本来和大家进行讲解，我们这里使用的镜像是 traefik:2.4。 在 Traefik 中的配置可以使用两种不同的方式： 动态配置：完全动态的路由配置 静态配置：启动配置 静态配置中的元素（这些元素不会经常更改）连接到 providers 并定义 Treafik 将要监听的 entrypoints。 在 Traefik 中有三种方式定义静态配置：在配置文件中、在命令行参数中、通过环境变量传递 动态配置包含定义系统如何处理请求的所有配置内容，这些配置是可以改变的，而且是无缝热更新的，没有任何请求中断或连接损耗。 安装 Traefik 到 Kubernetes 集群中的资源清单文件可以到官方文档中找到，链接: https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/ 将配置资源清单保存到本地，资源清单文件我这里准备好了（做了些修改，traefik 容器网络模式改为与共用宿主机网络，详细查看配置文件），通过以下命令应用。 请根据自己的需求修改资源配置清单 kubectl apply -f https://liwanggui.com/files/k8s/traefik2/crd.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2/rbac.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2/traefik.yaml kubectl apply -f https://liwanggui.com/files/k8s/traefik2/traefik-ui.yaml 其中 traefik.yaml 使用的是 DaemonSet 部署方式，如果你需要修改可以下载下来做相应的修改即可。我们这里是通过命令行参数来做的静态配置： args: - --log.level=INFO - --accesslog - --api=true # 开启 api/dashboard 会创建一个名为 api@internal 的特殊 service，在 dashboard 中可以直接使用这个 service 来访问 - --api.insecure - --metrics.prometheus=true - --metrics.prometheus.addentrypointslabels - --metrics.prometheus.addserviceslabels - --entrypoints.web.address=:80 # 定义名为 web 的入口 - --entryPoints.websecure.address=:443 # 定义名为 websecure 的入口 - --entrypoints.tcpep.address=:8000 # 定义名为 tcpep 的入口 - --entrypoints.web.forwardedheaders.insecure # 信任 web 入口 所有转发的 header 头信息，可以用于获取客户端真实 IP 地址 - --entrypoints.websecure.forwardedheaders.insecure # 信任 websecure 入口 所有转发的 header 头信息，可以用于获取客户端真实 IP 地址 - --providers.kubernetescrd - --providers.kubernetesingress traefik-ui.yaml 中定义的是访问 traefik WEB UI 的资源配置清单，可以根据自己的实际情况修改。 $ kubectl get pods -n kube-system -l app=traefik NAME READY STATUS RESTARTS AGE traefik-7s6wk 1/1 Running 0 7m27s traefik-bx6m8 1/1 Running 0 7m27s 部署完成后我们可以通过域名 traefik.host.cn 访问 Traefik 的 Dashboard 页面了 资源参考 https://www.qikqiak.com/post/traefik-2.1-101/ ","date":"2021-03-09","objectID":"/posts/kubernetes-traefik/:3:0","tags":["kubernetes","traefik"],"title":"Kubernetes 部署 Traefik 2.4","uri":"/posts/kubernetes-traefik/"},{"categories":["kubernetes"],"content":"主机环境 本示例中的 Kubernetes 集群部署将基于以下环境进行。 OS: Ubuntu 18.04.5 Kubernetes：v1.18.1 Container Runtime: Docker CE 19.03.15 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:1:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"环境说明 测试使用的 kubernetes 集群可由一个 master 主机及一个以上（建议至少两个）node 主机组成，这些主机可以是物理服务器，也可以运行于 vmware、virtualbox 或 kvm 等虚拟化平台上的虚拟机，甚至是公有云上的 VPS 主机。 本测试环境将由 master1.host.com、node1.host.com、node2.host.com 3个独立的主机组成，它们分别拥有 4 核心的 CPU 及 8G 的内存资源，操作系统环境均为最小化部署的 Ubuntu Server 18.04.5 LTS，启用了 SSH 服务，域名为 host.com。此外，各主机需要预设的系统环境如下： 借助于 chronyd 服务（程序包名称 chrony）设定各节点时间精确同步； 通过 DNS 完成各节点的主机名称解析； 各节点禁用所有的 Swap 设备； 各节点禁用默认配置的 iptables 防火墙服务； 注意：为了便于操作，后面将在各节点直接以系统管理员 root 用户进行操作。若用户使用了普通用户，建议将如下各命令以 sudo 方式运行。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:2:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"网络规划 Kubernetes 网络分为三类: Pod 网络：172.16.0.0/16 Service 网络: 192.168.0.0/16 Node 网络: 10.7.0.0/16 注意: 请按照实际情况进行网络规划。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:3:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"设定时钟同步 若节点可直接访问互联网，安装 chrony 程序包后，可直接启动 chronyd 系统服务，并设定其随系统引导而启动。随后，chronyd 服务即能够从默认的时间服务器同步时间。 apt install chrony systemctl start chronyd.service 不过，建议用户配置使用本地的的时间服务器，在节点数量众多时尤其如此。存在可用的本地时间服务器时，修改节点的 /etc/chrony/chrony.conf 配置文件，并将时间服务器指向相应的主机即可，配置格式如下： server CHRONY-SERVER-NAME-OR-IP iburst ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:4:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"主机名称解析（可选） 使用 bind9 提供 dns 服务, 本环境直接在主节点安装 bind9 apt install bind9 更新多详细配置参考 Ubuntu Server 安装配置 bind9 host.com zone 配置文件示例 ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA host.com. root.host.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS ns.host.com. @ IN A 10.7.79.148 ns IN A 10.7.79.148 master1 IN A 10.7.79.148 node1 IN A 10.7.50.17 node2 IN A 10.7.149.184 apiserver IN A 10.7.79.148 由于本实验中使用的机器较小，使用 hosts 文件实现本地域名解析 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:5:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"禁用 Swap 设备 部署集群时，kubeadm 默认会预先检查当前主机是否禁用了 Swap 设备，并在未禁用时强制终止部署过程。因此，在主机内存资源充裕的条件下，需要禁用所有的 Swap 设备，否则，就需要在后文的 kubeadm init 及 kubeadm join 命令执行时额外使用相关的选项忽略检查错误。 关闭 Swap 设备，需要分两步完成。首先是关闭当前已启用的所有 Swap 设备： swapoff -a 而后编辑 /etc/fstab 配置文件，注释用于挂载 Swap 设备的所有行。 另外，若确需在节点上使用 Swap 设备，也可选择让 kubeam 忽略 Swap 设备的相关设定。我们编辑 kubelet 的配置文件 /etc/default/kubelet，设置其忽略 Swap 启用的状态错误即可，文件内容如下： KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\" ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:6:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"禁用默认的防火墙服务 Ubuntu 和 Debian 等 Linux 发行版默认使用 ufw（Uncomplicated FireWall）作为前端来简化 iptables 的使用，处于启用状态时，它默认会生成一些规则以加强系统安全。出于降低配置复杂度之目的，本文选择直接将其禁用。 ufw disable ufw status ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:7:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装程序包 提示：以下操作需要在本示例中的所有三台主机上分别进行 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装 docker 此时我们需要安装指定版本的 docker，docker 安装请参考 Docker 快速安装 kubelet 需要让 docker 容器引擎使用 systemd 作为 CGroup 的驱动，其默认值为 cgroupfs，因而，我们还需要编辑 docker 的配置文件 /etc/docker/daemon.json，添加如下内容 { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"], # 镜像加速器 \"insecure-registries\":[\"harbor.host.com\"], # 第三方仓库或自建仓库地址，可以配置为 http \"data-root\": \"/data/docker\", # docker 数据存储目录 \"exec-opts\": [\"native.cgroupdriver=systemd\"], # 额外参数,部署 k8s 时需要指定此选项 \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\"}, \"live-restore\": true } 配置完成后即可启动 docker 服务，并将其设置为随系统启动而自动引导： systemctl daemon-reload systemctl start docker.service systemctl enable docker.service ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:1","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"安装配置 kubelet 和 kubeadm 首先，在各主机上生成 kubelet 和 kubeadm 等相关程序包的仓库，这里以阿里云的镜像服务为例： apt update \u0026\u0026 apt install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003e/etc/apt/sources.list.d/kubernetes.list\u003c\u003cEOF deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update 接着，在各主机安装 kubelet、kubeadm 和 kubectl 程序包，并将其设置为随系统启动而自动引导： apt install -y kubelet=1.18.1-00 kubeadm=1.18.1-00 kubectl=1.18.1-00 systemctl enable kubelet 安装完成后，要确保 kubeadm 等程序文件的版本，这将也是后面初始化 Kubernetes 集群时需要明确指定的版本号 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:8:2","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化第一个主节点 该步骤开始尝试构建 Kubernetes 集群的 master 节点，配置完成后，各 worker 节点直接加入到集群中的即可。需要特别说明的是，由 kubeadm 部署的 Kubernetes 集群上，集群核心组件 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd 等均会以静态 Pod 的形式运行，它们所依赖的镜像文件默认来自于 gcr.io 这一 Registry 服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种，本示例将选择使用更易于使用的后一种方式。 使用能够到达该服务的代理服务；使用国内的镜像服务器上的服务，例如 gcr.azk8s.cn/google_containers 和 registry.aliyuncs.com/google_containers 等。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:9:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化 master 节点 在 master1.host.com 上完成如下操作 在运行初始化命令之前先运行如下命令单独获取相关的镜像文件，而后再运行后面的 kubeadm init 命令，以便于观察到镜像文件的下载过程。 # 查看镜像列表 kubeadm config images list --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.1 # 获取镜像文件至本地 kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.1 而后即可进行 master 节点初始化。kubeadm init 命令支持两种初始化方式，一是通过命令行选项传递关键的部署设定，另一个是基于 yaml 格式的专用配置文件，后一种允许用户自定义各个部署参数。下面分别给出了两种实现方式的配置步骤，建议采用第二种方式进行。 初始化方式一 运行如下命令完成 master 节点的初始化： kubeadm init \\ --image-repository registry.aliyuncs.com/google_containers \\ --control-plane-endpoint apiserver.host.com \\ --kubernetes-version v1.18.1 \\ --apiserver-advertise-address 10.7.79.148 \\ --service-cidr 192.168.0.0/16 \\ --pod-network-cidr 172.16.0.0/16 \\ --token-ttl 0 \\ --upload-certs 命令中的各选项简单说明如下： --image-repository： 指定要使用的镜像仓库，默认为 gcr.io； --kubernetes-version：kubernetes 程序组件的版本号，它必须要与安装的 kubelet 程序包的版本号相同； --control-plane-endpoint：控制平面的固定访问端点，可以是 IP 地址或 DNS 名称，会被用于集群管理员及集群组件的 kubeconfig 配置文件的 API Server 的访问地址；单控制平面部署时可以不使用该选项； --pod-network-cidr：Pod 网络的地址范围，其值为 CIDR 格式的网络地址，通常，Flannel 网络插件的默认为 10.244.0.0/16, Calico 插件的默认值为 192.168.0.0/16； --service-cidr：Service 的网络地址范围，其值为 CIDR 格式的网络地址，默认为 10.96.0.0/12；通常，仅 Flannel一类的网络插件需要手动指定该地址； --apiserver-advertise-address：apiserver 通告给其他组件的IP地址，一般应该为 Master 节点的用于集群内部通信的IP地址，0.0.0.0 表示节点上所有可用地址； --upload-certs: 将控制平面证书上传到 kubeadm-certs secret。 --token-ttl：共享令牌（token）的过期时长，默认为 24小时，0 表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在 token 过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建 token，并生成节点加入命令。 kubeadm token create --print-join-command 需要注意的是，若各节点未禁用Swap设备，还需要附加选项 “–ignore-preflight-errors=Swap”，从而让 kubeadm 忽略该错误设定。 初始化方式二 kubeadm 也可通过配置文件加载配置，以定制更丰富的部署选项。以下是个符合前述命令设定方式的使用示例，不过，它明确定义了 kubeProxy 的模式为 ipvs，并支持通过修改 imageRepository 的值修改获取系统镜像时使用的镜像仓库。 默认配置可以通过 kubeadm config print init-defaults 获取 ---apiVersion:kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:- system:bootstrappers:kubeadm:default-node-tokenttl:24h0m0susages:- signing- authenticationkind:InitConfigurationlocalAPIEndpoint:# 这里的地址即为初始化的控制平面第一个节点的IP地址；advertiseAddress:192.168.31.11bindPort:6443nodeRegistration:criSocket:/var/run/dockershim.sock# 第一个控制平面节点的主机名称；name:master1.host.comtaints:- effect:NoSchedulekey:node-role.kubernetes.io/master---apiServer:timeoutForControlPlane:4m0sapiVersion:kubeadm.k8s.io/v1beta2# 控制平面的接入端点，我们这里选择适配到 apiserver.host.com 这一域名上controlPlaneEndpoint:\"apiserver.host.com:6443\"certificatesDir:/etc/kubernetes/pkiclusterName:kubernetescontrollerManager:{}dns:type:CoreDNSetcd:local:# 配置 etcd 数据存储路径dataDir:/data/etcd# 配置镜像拉取站点imageRepository:registry.aliyuncs.com/google_containerskind:ClusterConfiguration# 版本号要与部署的目标版本保持一致kubernetesVersion:v1.18.1networking:# 要使用的集群域名，默认为 cluster.localdnsDomain:cluster.local# Pod 的网络地址段podSubnet:172.16.0.0/16# Service 的网络地址段serviceSubnet:192.168.0.0/16scheduler:{}---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfiguration# 用于配置 kube-proxy 上为 Service 指定的代理模式，默认为 iptablesmode:\"ipvs\"ipvs:scheduler:\"nq\" 将上面的内容保存于配置文件中，例如 kubeadm-config.yaml，而后执行如下命令即能实现类似前一种初始化方式中的集群初始配置，但这里将 Service 的代理模式设定为了 ipvs。 初始化命令 kubeadm init --config kubeadm-config.yaml kubeadm init 命令完整参考指南请移步官方文档，地址为 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:9:1","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"初始化完成后的操作步骤 注意：对于 Kubernetes 系统的新用户来说，无论使用上述哪种方法，命令运行结束后，请记录最后的 kubeadm join 命令输出的最后提示的操作步骤。下面的内容是需要用户记录的一个命令输出示例，它提示了后续需要的操作步骤： [init] Using Kubernetes version: v1.18.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [master1.host.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local apiserver.host.com] and IPs [10.10.0.1 192.168.31.11] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [master1.host.com localhost] and IPs [192.168.31.11 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [master1.host.com localhost] and IPs [192.168.31.11 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" W0309 06:48:52.172442 48909 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" W0309 06:48:52.173496 48909 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 26.002622 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.18\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master1.host.com as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node master1.host.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: x3oo6y.ytmywnftdx6khuh5 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [boot","date":"2021-03-09","objectID":"/posts/kubernetes-install/:9:2","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"向集群添加额外的控制平面节点 在添加额外主节点之前我们需要将集群证书上传到集群中以便向其它主节点共享证书并生成证书密钥，使用此密钥可以解密由 init 上载的证书。 使用如下命令命令完成。 kubeadm init phase upload-certs --upload-certs I0309 07:16:31.594295 62834 version.go:252] remote version is much newer: v1.20.4; falling back to: stable-1.18 W0309 07:16:35.449971 62834 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace [upload-certs] Using certificate key: 7c3c34ac69d980bdaf28eb42e38186c71245cef7c5926d0fb252b7200aad05bf 也可以直接拷贝 /etc/kubernetes/pki 至另一台主节点上的 /etc/kubernetes 目录下 集群添加额外的主节点，使用如下命令完成。 kubeadm join apiserver.host.com:6443 --token x3oo6y.ytmywnftdx6khuh5 \\ --discovery-token-ca-cert-hash sha256:7708b5166572b6f33094b27d3c457d080213ec3bd701161d4d648367c53f1013 \\ --control-plane --certificate-key 7c3c34ac69d980bdaf28eb42e38186c71245cef7c5926d0fb252b7200aad05bf 本次实验由于资源有限只部署一台主节点，实际生产环境中建议部署多台，避免单点故障。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:10:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"添加节点到集群中 下面的两个步骤，需要分别在 node1.host.com 和 node2.host.com 上完成。 1、若未禁用 Swap 设备，编辑 kubelet 的配置文件 /etc/default/kubelet，设置其忽略 Swap 启用的状态错误，内容如下： KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\" 2、将节点加入 master 的集群中，要使用主节点初始化过程中记录的 kubeadm join 命令，并且在未禁用 Swap 设备的情况下，额外附加 “--ignore-preflight-errors=Swap” 选项； kubeadm join apiserver.host.com:6443 --token x3oo6y.ytmywnftdx6khuh5 \\ --discovery-token-ca-cert-hash sha256:7708b5166572b6f33094b27d3c457d080213ec3bd701161d4d648367c53f1013 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:11:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"验证节点添加结果 在每个节点添加完成后，即可通过 kubectl 验正添加结果。下面的命令及其输出是在 node1 和 node2 均添加完成后运行的，其输出结果表明两个 Node 已经准备就绪。 kubectl get nodes NAME STATUS ROLES AGE VERSION master1.host.com Ready master 41m v1.18.1 node1.host.com Ready \u003cnone\u003e 51s v1.18.1 node2.host.com Ready \u003cnone\u003e 43s v1.18.1 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:11:1","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["kubernetes"],"content":"测试应用编排及服务访问 到此为止，一个 master，并附带有二个 node 的 kubernetes 集群基础设施已经部署完成，用户随后即可测试其核心功能。例如，下面的命令可将 demoapp 以 Pod 的形式编排运行于集群之上，并通过在集群外部进行访问： kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 kubectl scale deployment/demoapp --replicas=2 kubectl create service nodeport demoapp --tcp=80:80 而后，使用如下命令了解 Service 对象 demoapp 使用的 NodePort，以便于在集群外部进行访问 kubectl get svc -l app=demoapp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demoapp NodePort 10.10.149.76 \u003cnone\u003e 80:32002/TCP 10s demoapp 是一个 web 应用，因此，用户可以于集群外部通过 http://NodeIP:32002 这个 URL 访问 demoapp 上的应用，例如于集群外通过浏览器访问 http://10.7.50.17:32002。 ","date":"2021-03-09","objectID":"/posts/kubernetes-install/:12:0","tags":["kubernetes","kubeadm"],"title":"使用 Kubeadm 快速部署 Kubernetes 集群","uri":"/posts/kubernetes-install/"},{"categories":["docker"],"content":" docker 安装请参考: – Docker 快速安装 ","date":"2021-03-06","objectID":"/posts/docker-cli/:0:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"镜像管理 ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"1. 获取镜像 # 默认从 dockerhub 拉取最新版本镜像 [root@localhost ~]# docker pull busybox Using default tag: latest latest: Pulling from library/busybox add3ddb21ede: Pull complete Digest: sha256:b82b5740006c1ab823596d2c07f081084ecdb32fd258072707b99f52a3cb8692 Status: Downloaded newer image for busybox:latest # 拉取指定版本的镜像 [root@localhost ~]# docker pull ubuntu:14.04 14.04: Pulling from library/ubuntu 48f0413f904d: Downloading [======\u003e ] 8.925MB/67.12MB 2bd2b2e92c5f: Download complete 06ed1e3efabb: Download complete a220dbf88993: Waiting 57c164185602: Waiting ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:1","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"2. 列出镜像 [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB ubuntu 14.04 c69811d4e993 4 weeks ago 188MB ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:2","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"3. 删除镜像 [root@localhost ~]# docker rmi ubuntu:14.04 Untagged: ubuntu:14.04 Untagged: ubuntu@sha256:6a3e01207b899a347115f3859cf8a6031fdbebb6ffedea6c2097be40a298c85d Deleted: sha256:c69811d4e9931740c0a490f74fafb566bb520b945f6e62cab96f6faecd750b95 Deleted: sha256:5294610fabc319f443fc036f7bf5c02299f2614d4b0f79c87529bb9aef46ce4e Deleted: sha256:a783f54895fb2d76726d8b4fbbb263bcffc0cbb7fe858450a39d21b7f4de1df6 Deleted: sha256:e11129e7baf41455394f83970e3e232fa7c33a87948b76ca1c121942c4f0403f Deleted: sha256:38c3fb0ca70b3e0444085376821508b758e4f30b290a0016c4b044b9f46bddf8 Deleted: sha256:826fc2344fbbc40cf9f2714c831a0d3ff88596e471f71c33b1055f3913d829d4 ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:3","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"4. 保存镜像 [root@localhost ~]# docker save ubuntu:latest -o ubuntu-latest.tar [root@localhost ~]# ls -lh total 44M -rw-r--r-- 1 root root 72.9M Sep 9 19:20 ubuntu-latest.tar 保存并压缩 [root@localhost ~]# docker save ubuntu:latest | gzip \u003e ubuntu-latest.tar.gz [root@localhost ~]# ls -lh total 44M -rw-r--r-- 1 root root 44M Sep 9 19:20 ubuntu-latest.tar.gz ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:4","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"5. 载入镜像 [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB [root@localhost ~]# docker load -i ubuntu-latest.tar.gz 8aa4fcad5eeb: Loading layer [==================================================\u003e] 124.1MB/124.1MB 25e0901a71b8: Loading layer [==================================================\u003e] 15.87kB/15.87kB 625c7a2a783b: Loading layer [==================================================\u003e] 11.78kB/11.78kB 9c42c2077cde: Loading layer [==================================================\u003e] 5.632kB/5.632kB a09947e71dc0: Loading layer [==================================================\u003e] 3.072kB/3.072kB Loaded image: ubuntu:latest [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB PS: 结合以上命令可以使用 shell 命令完成镜像迁移工作 docker save \u003c镜像名\u003e | bzip2 | pv | ssh \u003c用户名\u003e@\u003c主机名\u003e 'cat | docker load' ","date":"2021-03-06","objectID":"/posts/docker-cli/:1:5","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"使用容器 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:0","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"1. 启动/停止/重启容器 # -d 参数表示守护进程方式运行 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 727dbeacc1f5 ubuntu:latest \"/bin/bash\" 3 seconds ago Up 2 seconds inspiring_goldberg # 停止容器，重启，启动指令为 restart, start [root@localhost ~]# docker container stop inspiring_goldberg inspiring_goldberg [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 727dbeacc1f5 ubuntu:latest \"/bin/bash\" About a minute ago Exited (0) 1 second ago inspiring_goldberg ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:1","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"2. 删除容器 [root@localhost ~]# docker container rm 727dbeacc1f5 727dbeacc1f5 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 容器删除前需要先停止 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:2","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"3. 导出容器 [root@localhost ~]# docker container export 84bc66973544 \u003e ubuntu-latest.tar ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:3","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"4. 导入容器为镜像 [root@localhost ~]# cat ubuntu-latest.tar | docker import - test/ubuntu:latest sha256:389b10ce91abd80cc9b306cbc02cfc74b5089ba60766d8fd66af48691ab9d6fc [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE test/ubuntu latest 389b10ce91ab 5 seconds ago 97.9MB busybox latest d20ae45477cb 2 weeks ago 1.13MB ubuntu latest ccc7a11d65b1 4 weeks ago 120MB 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以 使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容 器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状 态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入 时可以重新指定标签等元数据信息。 ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:4","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"5. 进入容器 [root@localhost ~]# docker container ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 84bc66973544 ubuntu:latest \"/bin/bash\" 12 minutes ago Up 12 minutes hungry_bartik [root@localhost ~]# docker exec -ti 84bc66973544 bash root@84bc66973544:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"16.04.3 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.3 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial ","date":"2021-03-06","objectID":"/posts/docker-cli/:2:5","tags":["docker"],"title":"Docker 基本操作","uri":"/posts/docker-cli/"},{"categories":["docker"],"content":"harbor 使用 docker 容器的方式部署，所以在部署 harbor 前需要安装好 docker 及单机编排工具 docker-compose ","date":"2021-03-06","objectID":"/posts/docker-harbor/:0:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"下载 harbor 离线安装包 harbor 托管于 Github，在 Github 上有提供完整的离线安装直接下载即可。 Github 地址 root@ops:/opt# wget https://github.com/goharbor/harbor/releases/download/v2.1.0/harbor-offline-installer-v2.1.0.tgz ","date":"2021-03-06","objectID":"/posts/docker-harbor/:1:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"安装 docker-compose 由于 harbor 依赖于 docker-compose 完成单机编排工作，需要先安装好 root@ops:/opt# curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose root@ops:/opt# chmod +x /usr/local/bin/docker-compose ","date":"2021-03-06","objectID":"/posts/docker-harbor/:2:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"部署 harbor root@ops:/opt# tar xzf harbor-offline-installer-v2.1.0.tgz root@ops:/opt# cd harbor # 复制一份harbor配置文件 root@ops:/opt/harbor# cp harbor.yml.tmpl harbor.yml root@ops:/opt/harbor# egrep -v '^$|#' harbor.yml hostname: harbor.host.com # harbor 站点域名 http: port: 801 # 修改端口 harbor_admin_password: Harbor12345 # harbor 管理员默认密码 database: password: root123 max_idle_conns: 50 max_open_conns: 100 data_volume: /data/harbor # harbor 数据存储路径 clair: updaters_interval: 12 trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /data/harbor/logs # harbor 日志文件路径 _version: 2.0.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - clair - trivy root@ops:/opt/harbor# ./install # 开始部署 harbor ","date":"2021-03-06","objectID":"/posts/docker-harbor/:3:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":"配置 nginx 反代 部署完成后可以配置 nginx 反代对外提供服务，nginx 配置注意加大 client_max_body_size 参数值. server { listen 80; server_name harbor.wfugui.com; # 注意此项配置 client_max_body_size 2000m; location / { proxy_pass https://localhost:801; include proxy.conf; } } 注意: 最新版本的 harbor 如果不启用证书验证连接(https) 在 push 镜像时会不成功 ","date":"2021-03-06","objectID":"/posts/docker-harbor/:4:0","tags":["docker","harbor"],"title":"Docker 私有仓库部署 (harbor)","uri":"/posts/docker-harbor/"},{"categories":["docker"],"content":" Dockerfile 是一个文本格式的配置文件，用户可以使用 Dockerfile 快速创建自定义的镜像。 Dockerfile 由一行行命令语句组成，并且支持以 # 开头注释行。 Dockerfile 一般分为四部分： 基础镜像信息，维护者信息、镜像操作指令和容器启动时执行指令。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:0:0","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"指令 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:0","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"FROM 格式: FROM \u003cimage\u003e 或者 FROM \u003cimage\u003e:\u003ctag\u003e Dockerfile 的第一条指令必须是 FROM ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:1","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"MAINTAINER - 弃用 格式: MAINTAINER \u003cname\u003e，指定维护者信息 推荐使用 LABEL maintainer=\"SvenDowideit@home.org.au\" ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:2","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"LABLE 格式: LABEL \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e ... 该指令将元数据添加到 docker 镜像中 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:3","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"USER 格式: USER \u003cusername\u003e 指定容器内进程使用的用户名或 UID, 当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:4","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"WORKDIR 格式: WORKDIR /path/to/workdir 指定容器的当前工作路径，为后续的 RUN、CMD、ENTRYPINT 指令配置工作路径 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:5","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ENV 格式: ENV \u003ckey\u003e \u003cvalue\u003e 指定一个环境变量，可以被 RUN 指令使用，并在容器运行时保持存在 ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 可以同时指定多个变量，推荐使用这种方式 ENV PG_MAJOR=9.3 PG_VERSION=9.3.4 RUN curl -SL http://example.com/postgres-${PG_VERSION}.tar.gz | tar -xzC /usr/src \u0026\u0026 ... ENV PATH=/usr/local/postgres-$PG_MAJOR/bin:$PATH ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:6","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ARG 格式: ARG \u003ckey\u003e=\u003cvalue\u003e ... 该指令定义了一个变量，只在构建镜像时生效。可以只定义变量名(或者默认值)，然后使用命令 docker build --build-arg 参数, 传递变量值的给构建者。 如果用户指定了Dockerfile中未定义的构建参数，则该构建会输出警告。ARGdocker build--build-arg \u003cvarname\u003e=\u003cvalue\u003e ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:7","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ADD 格式: ADD \u003csrc\u003e \u003cdest\u003e 该指令将复制指定的 \u003csrc\u003e 到容器中的 \u003cdest\u003e. 其中 \u003csrc\u003e 可以是 Dockerfile 所在目录的一个相对路径(文件和目录)； 也可以是一个 url；还可以是一个 tar 文件（自动解压为目录） ADD hom* /mydir/ ADD hom?.txt /mydir/ ADD --chown=55:mygroup files* /somedir/ ADD --chown=bin files* /somedir/ ADD --chown=1 files* /somedir/ ADD --chown=10:11 files* /somedir/ ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:8","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"COPY 格式: COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] \u003csrc\u003e... \u003cdest\u003e COPY [--chown=\u003cuser\u003e:\u003cgroup\u003e] [\"\u003csrc\u003e\",... \"\u003cdest\u003e\"] 该指令复制文件或目录，并将它们添加到容器的文件系统路径中; 可以指定多个资源，但文件和目录的路径将基于构建路径。 COPY hom* /mydir/ COPY hom?.txt /mydir/ COPY --chown=55:mygroup files* /somedir/ COPY --chown=bin files* /somedir/ COPY --chown=1 files* /somedir/ COPY --chown=10:11 files* /somedir/ ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:9","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"RUN 格式: RUN \u003ccommand\u003e 或者 RUN [\"executable\", \"param1\", \"param2\"] 每条 RUN 指令将在当前镜像的基础上执行指定的命令，并提交为新镜像。当命令过长时可以使用 \\ 换行。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:10","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"EXPOSE 格式: EXPOSE \u003cport\u003e [\u003cport\u003e ...] 示例: EXPOSE 22 80 443 EXPOSE 指令用于暴露容器端口。在启动容器时需要通过 -P, Docker 服务会随机分配一个端口转发到指定的端口； 使用 -p, 可以手动指定具体本地的端口与容器端口映射。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:11","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"VOLUME 格式: VOLUME [\"/data\"] 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放需要持久化的数据等。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:12","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"CMD 格式: CMD [\"executable\", \"param1\", \"param2\"] 使用 exec 执行，推荐方式 CMD command param1 param2 ,在 /bin/sh 中执行 CMD [\"param1\", \"param2\"] 提供给 ENTRYPOINT 的默认参数 指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 指令。如果指定多条，只有最后一条生效被执行。 如果在启动容器时指定了运行的命令，会覆盖掉 CMD 指定的命令。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:13","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"ENTRYPOINT 格式: ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖. 每个 Dockerfile 中只能有一个 ENTRYPOINT，指定多个 ENTRYPOINT 时，只有最一个有效。 ","date":"2021-03-06","objectID":"/posts/docker-dockerfile/:1:14","tags":["docker","dockerfile"],"title":"Dockerfile 相关指令","uri":"/posts/docker-dockerfile/"},{"categories":["docker"],"content":"完整的配置项参考: https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file docker 默认配置文件路径为: /etc/docker/daemon.json ","date":"2021-03-06","objectID":"/posts/docker-config/:0:0","tags":["docker"],"title":"Docker 基本配置项","uri":"/posts/docker-config/"},{"categories":["docker"],"content":"docker 常用的配置项 [root@localhost ~]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"], # 镜像加速器 \"insecure-registries\":[\"harbor.host.com\"], # 第三方仓库或自建仓库地址，可以配置为 http \"data-root\": \"/data/docker\", # docker 数据存储目录 \"exec-opts\": [\"native.cgroupdriver=systemd\"], # 额外参数,部署 k8s 时需要指定此选项 \"bip\": \"10.10.0.1/16\", # 配置 docker 网桥 ip \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\"}, \"live-restore\": true } ","date":"2021-03-06","objectID":"/posts/docker-config/:1:0","tags":["docker"],"title":"Docker 基本配置项","uri":"/posts/docker-config/"},{"categories":["docker"],"content":"一键安装 docker Docker 官方提供了一键安装 docker 脚本工具: https://github.com/docker/docker-install ","date":"2021-03-06","objectID":"/posts/docker-install/:1:0","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"安装 docker curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh 默认下载源是 docker 官方境外的源，在国内下载很慢 ","date":"2021-03-06","objectID":"/posts/docker-install/:1:1","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"使用阿里云安装 docker curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh --mirror=Aliyun ","date":"2021-03-06","objectID":"/posts/docker-install/:1:2","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"配置阿里源安装 docker # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs)stable\" # Step 4: 更新并安装Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce ","date":"2021-03-06","objectID":"/posts/docker-install/:2:0","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["docker"],"content":"安装指定版本的Docker-CE 1: 查找 Docker-CE的版本: root@ops:~# apt-cache madison docker-ce docker-ce | 5:20.10.5~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.4~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.3~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.2~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.1~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:20.10.0~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.15~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.14~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.13~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.12~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.11~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.10~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.9~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.8~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.7~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages ... 2: 安装指定版本的 Docker-CE: (VERSION 例如上面的 5:19.03.15~3-0~ubuntu-bionic) root@ops:~# apt-get -y install docker-ce=5:19.03.15~3-0~ubuntu-bionic 注意: 如果部署 kubernetes 集群，就是需要安装经过 kubernetes 验证过的 docker 版本 ","date":"2021-03-06","objectID":"/posts/docker-install/:2:1","tags":["docker"],"title":"Docker 快速安装","uri":"/posts/docker-install/"},{"categories":["ubuntu"],"content":"如果你没有在 Linux 下安装和运行 Systemd-Resolved、DNSMasq、Nscd 缓存服务，那就没有操作系统级的 DNS 缓存，不同的 Linux 发行版在刷新 DNS 缓存上方法是不同的。 以下操作在 Ubuntu 18.04 操作系统下进行 ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:0:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 Systemd Resolved 缓存 Ubuntu 18.04 系统是使用 Systemd Resolved 服务来缓存 DNS 的，所以可以运行以下命令确定该服务是否运行： sudo systemctl is-active systemd-resolved.service 如果服务运行，则会看到返回的活动状态信息，否则只会看到非活动状态。 删除 Systemd Resolved DNS 缓存的方法，运行以下命令： sudo systemd-resolve --flush-caches ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:1:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 DNSMasq 缓存 如果你在 Ubuntu 18.04 下使用 DNSMasq 作为缓存服务器，要删除 DNS 缓存，请运行以下命令： sudo systemctl restart dnsmasq.service ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:2:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"刷新 Nscd 缓存 如果使用了 Nscd，删除 DNS 缓存只需要运行以下命令： sudo systemctl restart nscd.service 或者运行： sudo service nscd restart ","date":"2021-03-04","objectID":"/posts/ubuntu-flushdns/:3:0","tags":["ubuntu","dns"],"title":"Ubuntu 刷新/删除 DNS 缓存","uri":"/posts/ubuntu-flushdns/"},{"categories":["ubuntu"],"content":"域名服务（DNS）是一种Internet服务，可将IP地址和标准域名（FQDN）相互映射。这样，DNS减轻了记住IP地址的需要。运行DNS的计算机称为名称服务器。Ubuntu附带了BIND (Berkley Internet Naming Daemon)，BIND是用于在Linux上维护名称服务器的最常用程序。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:0:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"安装 在终端提示符下，输入以下命令安装 dns: sudo apt install bind9 dnsutils 软件包是测试和解决 DNS 问题非常有用的。 这些工具通常已经安装，但是要检查或安装 dnsutils，请输入以下内容： sudo apt install dnsutils ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:1:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"配置角色 有许多方法可以配置BIND9。一些最常见的配置是缓存名称服务器，主服务器和辅助服务器。 当配置为缓存名称服务器时，BIND9将找到名称查询的答案，并在再次查询域时记住答案。 作为主要服务器，BIND9从其主机上的文件中读取区域的数据，并且对该区域具有权威性。 作为辅助服务器，BIND9从另一个对该区域具有权威性的名称服务器获取区域数据。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:2:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"配置文件概览 DNS配置文件存储在 /etc/bind 目录中。主要配置文件是 /etc/bind/named.conf ，在软件包提供的布局中仅包括这些文件。 /etc/bind/named.conf.options：DNS 全局选项配置文件 /etc/bind/named.conf.local：自定义区域配置文件 /etc/bind/named.conf.default-zones：默认区域，例如localhost，其反向和根提示 根名称服务器曾经在文件中描述过 /etc/bind/db.root 。 现在由软件包 /usr/share/dns/root.hints 附带的文件提供了此功能 dns-root-data，并且在 named.conf.default-zones 上面的配置文件中对此进行了引用。 可以将同一服务器配置为缓存名称服务器，主要和辅助名称服务器：这都取决于它所服务的区域。服务器可以是一个区域的授权开始（SOA），同时为另一区域提供辅助服务。同时为本地LAN上的主机提供缓存服务。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:3:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"缓存名称服务器 默认配置充当缓存服务器。只需取消注释并编辑 /etc/bind/named.conf.options 即可设置ISP的DNS服务器的IP地址： forwarders { 1.2.3.4; 5.6.7.8; }; 注意: 用实际 DNS 服务器的IP地址替换 1.2.3.4 和 5.6.7.8。 要启用新配置，请重新启动DNS服务器。在终端提示下： sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:4:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"主服务器 在本节中，将BIND9配置为域的主服务器 example.com。只需 example.com 用您的FQDN（完全合格的域名）替换即可。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"转发区域文件 要将DNS区域添加到BIND9，将BIND9变成主服务器，请首先编辑 /etc/bind/named.conf.local： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; }; 注意 如果bind将像使用DDNS一样接收文件的自动更新，请在此处以及下面的复制命令中使用 /var/lib/bind/db.example.com 而不是 /etc/bind/db.example.com。 现在，使用现有的区域文件作为模板来创建 /etc/bind/db.example.com 文件： sudo cp /etc/bind/db.local /etc/bind/db.example.com 编辑新的区域文件，/etc/bind/db.example.com 然后更改 localhost.为服务器的FQDN，.在末尾保留其他文件。更改 127.0.0.1 为名称服务器的IP地址和 root.localhost 有效的电子邮件地址，但用.代替通常的@符号，并再次.在末尾保留。更改注释以指示此文件所针对的域。 为基本域创建A记录example.com。此外，创建一个A记录的ns.example.com，在这个例子中，域名服务器： ; ; BIND data file for example.com ; $TTL 604800 @ IN SOA example.com. root.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL @ IN NS ns.example.com. @ IN A 192.168.1.10 @ IN AAAA ::1 ns IN A 192.168.1.10 每次更改区域文件时，都必须增加序列号(Serial)。如果在重新启动BIND9之前进行了多次更改，只需增加一次串行。 现在，您可以将DNS记录添加到区域文件的底部。有关详细信息，请参阅公共记录类型。 注意，许多管理员喜欢使用最后编辑的日期作为区域的序列号(Serial)，例如2020012100，它是yyyymmddss(其中ss是序列号) 对区域文件进行了更改之后，需要重新启动BIND9以使更改生效 sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:1","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"反向区域文件 现在已经设置了区域并将名称解析为IP地址，现在需要添加反向区域以允许DNS将地址解析为名称。 编辑 /etc/bind/named.conf.local 并添加以下内容： zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; }; 注意: 将 1.168.192 替换为所用网络的前三个八位位组。 另外，适当命名区域文件 /etc/bind/db.192。 它应与网络的第一个八位位组匹配。 现在创建 /etc/bind/db.192 文件: sudo cp /etc/bind/db.127 /etc/bind/db.192 接下来编辑 /etc/bind/db.192，更改与/etc/bind/db.example.com相同的选项： ; ; BIND reverse data file for local 192.168.1.XXX net ; $TTL 604800 @ IN SOA ns.example.com. root.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS ns. 10 IN PTR ns.example.com. 每次更改时，“反向”区域中的序列号也需要增加。 对于您在/etc/bind/db.example.com中配置的每个A记录（即针对另一个地址），您需要在/etc/bind/db.192中创建一个PTR记录。 创建反向区域文件后，重新启动BIND9 sudo systemctl restart bind9.service ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:5:2","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"辅助服务器 一旦配置了主服务器，强烈建议使用辅助服务器，以在主服务器不可用时维持域的可用性。 首先，在主服务器上，需要允许区域传输。将 allow-transfer 选项添加到示例正向和反向区域定义中 /etc/bind/named.conf.local： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; allow-transfer { 192.168.1.11; }; }; zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; allow-transfer { 192.168.1.11; }; }; 注意 替换192.168.1.11为辅助名称服务器的IP地址。 在主服务器上重新启动BIND9： sudo systemctl restart bind9.service 接下来，在辅助服务器上，以与主服务器相同的方式安装bind9软件包。然后编辑，/etc/bind/named.conf.local 并为正向和反向区域添加以下声明： zone \"example.com\" { type slave; file \"db.example.com\"; masters { 192.168.1.10; }; }; zone \"1.168.192.in-addr.arpa\" { type slave; file \"db.192\"; masters { 192.168.1.10; }; }; 注意 替换192.168.1.10为您的主要名称服务器的IP地址。 在辅助服务器上重新启动BIND9： sudo systemctl restart bind9.service 在其中，/var/log/syslog 您应该看到类似以下内容的内容（为了适应本文档的格式，对某些行进行了拆分）： client 192.168.1.10#39448: received notify for zone '1.168.192.in-addr.arpa' zone 1.168.192.in-addr.arpa/IN: Transfer started. transfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53: connected using 192.168.1.11#37531 zone 1.168.192.in-addr.arpa/IN: transferred serial 5 transfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53: Transfer completed: 1 messages, 6 records, 212 bytes, 0.002 secs (106000 bytes/sec) zone 1.168.192.in-addr.arpa/IN: sending notifies (serial 5) client 192.168.1.10#20329: received notify for zone 'example.com' zone example.com/IN: Transfer started. transfer of 'example.com/IN' from 192.168.1.10#53: connected using 192.168.1.11#38577 zone example.com/IN: transferred serial 5 transfer of 'example.com/IN' from 192.168.1.10#53: Transfer completed: 1 messages, 8 records, 225 bytes, 0.002 secs (112500 bytes/sec) 注意：仅当主服务器上的序列号大于辅助服务器上的序列号时，才会传输区域。如果要让您的主DNS通知其他辅助DNS服务器区域更改，则可以将其添加also-notify { ipaddress; };到/etc/bind/named.conf.local以下示例中： zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; allow-transfer { 192.168.1.11; }; also-notify { 192.168.1.11; }; }; zone \"1.168.192.in-addr.arpa\" { type master; file \"/etc/bind/db.192\"; allow-transfer { 192.168.1.11; }; also-notify { 192.168.1.11; }; }; 注意 非权威区域文件的默认目录为/var/cache/bind/。该目录还在AppArmor中配置为允许命名守护程序向其写入。有关AppArmor的更多信息，请参见Security-AppArmor。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:6:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"测试 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"resolv.conf 测试BIND9的第一步是将名称服务器的IP地址添加到主机解析器。应该配置主要名称服务器以及另一个主机，以仔细检查。有关将名称服务器地址添加到网络客户端的详细信息，请参阅DNS客户端配置。最后，您的nameserver一行/etc/resolv.conf应指向，127.0.0.53并且您应该search为您的域指定一个参数。像这样： nameserver 127.0.0.53 search example.com 要检查您的本地解析器正在使用哪个DNS服务器，请运行： systemd-resolve --status 注意 如果主要服务器不可用，您还应该将辅助名称服务器的IP地址添加到客户端配置中。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:1","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"dig 如果安装了dnsutils软件包，则可以使用DNS查找实用程序dig测试设置： 安装完BIND9之后，请对环回接口使用dig来确保它正在侦听端口53。从终端提示符下： dig -x 127.0.0.1 您应该在命令输出中看到类似于以下内容的行： ;; Query time: 1 msec ;; SERVER: 192.168.1.10#53(192.168.1.10) 如果您已将BIND9配置为缓存名称服务器，则“挖掘”外部域以检查查询时间： dig ubuntu.com 注意查询时间接近命令输出的末尾： ;; Query time: 49 msec 经过第二次挖掘后，应该有所改进： ;; Query time: 1 msec ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:2","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"ping 现在演示应用程序如何使用DNS解析主机名，使用ping实用程序发送ICMP回显请求： ping example.com 这测试名称服务器是否可以将名称解析为ns.example.com IP 地址。 命令输出应类似于： PING ns.example.com (192.168.1.10) 56(84) bytes of data. 64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.800 ms 64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.813 ms ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:3","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"named-checkzone 测试区域文件的一种好方法是使用 named-checkzone 与bind9软件包一起安装的实用程序。使用此实用程序，可以在重新启动BIND9并使更改生效之前确保配置正确。 要测试我们的示例正向区域文件，请从命令提示符处输入以下内容： named-checkzone example.com /etc/bind/db.example.com 如果一切配置正确，您应该会看到类似以下的输出： zone example.com/IN: loaded serial 6 OK 同样，要测试反向区域文件，请输入以下内容： named-checkzone 1.168.192.in-addr.arpa /etc/bind/db.192 输出应类似于： zone 1.168.192.in-addr.arpa/IN: loaded serial 3 OK ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:7:4","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"日志 BIND9有多种可用的日志记录配置选项，但是两个主要的选项是channel和category，它们分别配置日志的去向和要记录的信息。 如果未配置任何日志记录选项，则默认配置为： logging { category default { default_syslog; default_debug; }; category unmatched { null; }; }; 让我们将BIND9配置为将与DNS查询相关的调试消息发送到单独的文件。 我们需要配置一个通道以指定要将消息发送到的文件，以及一个category。在此示例中，类别将记录所有查询。编辑/etc/bind/named.conf.local并添加以下内容： logging { channel query.log { file \"/var/log/named/query.log\"; severity debug 3; }; category queries { query.log; }; }; 注意 该调试选项可以从1设置为3。如果没有指定级别，1级是默认的。 由于命名守护程序以绑定用户身份运行，因此/var/log/named必须创建目录并更改所有权： sudo mkdir /var/log/named sudo chown bind:bind /var/log/named 现在重新启动BIND9，以使更改生效： sudo systemctl restart bind9.service 您应该看到文件中/var/log/named/query.log填充了查询信息。这是BIND9日志记录选项的简单示例。 注意 您的区域文件的序列号可能会有所不同。 ","date":"2021-03-04","objectID":"/posts/ubuntu-bind/:8:0","tags":["bind","dns"],"title":"Ubuntu Server 安装配置 bind9","uri":"/posts/ubuntu-bind/"},{"categories":["ubuntu"],"content":"临时IP地址分配 对于临时网络配置，可以使用在大多数其他 GNU/Linux 操作系统上也可以找到的ip命令。ip 命令允许您配置立即生效的设置，但是这些设置不是永久性的，并且在重新启动后会丢失。 要临时配置IP地址，可以按以下方式使用ip命令。修改IP地址和子网掩码以符合您的网络要求。 sudo ip addr add 10.102.66.200/24 dev enp0s25 然后可以使用ip来设置链接的打开或关闭。 ip link set dev enp0s25 up ip link set dev enp0s25 down 要验证 enp0s25 的 IP 地址配置，可以按以下方式使用 ip 命令。 ip address show dev enp0s25 10: enp0s25: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:16:3e:e2:52:42 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.102.66.200/24 brd 10.102.66.255 scope global dynamic eth0 valid_lft 2857sec preferred_lft 2857sec inet6 fe80::216:3eff:fee2:5242/64 scope link valid_lft forever preferred_lft forever6 要配置默认网关，可以按以下方式使用ip命令。修改默认网关地址以符合您的网络要求。 sudo ip route add default via 10.102.66.1 要验证默认网关配置，可以按以下方式使用ip命令。 ip route show default via 10.102.66.1 dev eth0 proto dhcp src 10.102.66.200 metric 100 10.102.66.0/24 dev eth0 proto kernel scope link src 10.102.66.200 10.102.66.1 dev eth0 proto dhcp scope link src 10.102.66.200 metric 100 如果您需要DNS进行临时网络配置，则可以在文件中添加DNS服务器IP地址 /etc/resolv.conf。通常，/etc/resolv.conf不建议直接进行编辑，但这是一个临时且非持久的配置。下面的示例显示如何在中输入两个DNS服务器/etc/resolv.conf，应将其更改为适合您的网络的服务器。下一节将更详细地说明进行DNS客户端配置的正确的持久方法。 nameserver 8.8.8.8 nameserver 8.8.4.4 如果您不再需要此配置，并且希望从接口清除所有IP配置，则可以将ip命令与flush选项一起使用，如下所示。 ip addr flush eth0 使用ip命令刷新IP配置不会清除的内容 /etc/resolv.conf。您必须手动删除或修改这些条目，或者重新引导，这也将导致重新写入/etc/resolv.conf，这是到的符号链接/run/systemd/resolve/stub-resolv.conf ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:1:0","tags":["netplan"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["ubuntu"],"content":"静态IP地址分配 要将系统配置为使用静态地址分配，请在文件中创建一个netplan配置 /etc/netplan/99_config.yaml。下面的示例假定您正在配置标识为eth0的第一个以太网接口。更改地址，gateway4和名称服务器值，以满足您的网络要求。 network:version:2renderer:networkdethernets:eth0:addresses:- 10.10.10.2/24gateway4:10.10.10.1nameservers:search:- mydomain- otherdomainaddresses:- 10.10.10.1- 1.1.1.1 然后可以使用netplan命令应用该配置。 sudo netplan apply ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:2:0","tags":["netplan"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["ubuntu"],"content":"名称解析 与IP网络相关的名称解析是将IP地址映射到主机名的过程，从而更容易识别网络上的资源。下一节将说明如何使用DNS和静态主机名记录正确配置系统以进行名称解析。 DNS客户端配置 传统上，该文件/etc/resolv.conf是静态配置文件，很少需要通过DCHP客户端挂接进行更改或自动更改。Systemd 解析处理名称服务器配置，并且应该通过systemd-resolve命令与之交互。Netplan配置systemd-resolved以生成要放入的名称服务器和域的列表/etc/resolv.conf，这是一个符号链接： /etc/resolv.conf -\u003e ../run/systemd/resolve/stub-resolv.conf 要配置解析器，请将适合您的网络的名称服务器的IP地址添加到netplan配置文件中。您还可以添加可选的DNS后缀搜索列表以匹配您的网络域名。生成的文件可能如下所示： network:version:2renderer:networkdethernets:enp0s25:addresses:- 192.168.0.100/24gateway4:192.168.0.1nameservers:search:[mydomain, otherdomain]addresses:[1.1.1.1,8.8.8.8,4.4.4.4] 该搜索选项也可以用多个域名使用，使得DNS查询将按照它们的输入顺序追加。例如，您的网络可能有多个子域可供搜索；的父域example.com和两个子域，sales.example.com以及dev.example.com。 如果您要搜索多个域，则配置可能如下所示： network:version:2renderer:networkdethernets:enp0s25:addresses:- 192.168.0.100/24gateway4:192.168.0.1nameservers:search:[example.com, sales.example.com, dev.example.com]addresses:[1.1.1.1,8.8.8.8,4.4.4.4] 如果您尝试对名称为server1的主机执行ping操作，系统将按以下顺序自动查询DNS的完全合格域名（FQDN）： server1.example.com server1.sales.example.com server1.dev.example.com 如果找不到匹配项，则DNS服务器将提供notfound的结果，并且DNS查询将失败。 ","date":"2021-03-04","objectID":"/posts/ubuntu-network/:3:0","tags":["netplan"],"title":"Ubuntu Server 网络配置","uri":"/posts/ubuntu-network/"},{"categories":["cli"],"content":"awk 文本处理 awk 是一种很棒的语言，它适合文本处理和报表生成，其语法较为常见，借鉴了某些语言的一些精华，如 C 语言等。在 linux 系统日常处理工作中，发挥很重要的作用，掌握了 awk 将会使你的工作变的高大上。awk 是三剑客的老大，利剑出鞘，必会不同凡响。 ","date":"2021-03-03","objectID":"/posts/awk/:1:0","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["cli"],"content":"awk 内置变量 $0 匹配当前记录整行数据 $1-$n 匹配当前记录的第n个字段（列） FS 输入字段分隔符，默认是空格或制表符(tab) RS 输入记录分隔符，默认为换行符 NF 当前记录的字段个数，就是有多个列 NR 记录所有行数，就是行号，从1开始 OFS 输出字段分隔符，默认也是空格 ORS 输入的记录分隔符，默认为换行符 内置变量很多，更多参数自行查询 ","date":"2021-03-03","objectID":"/posts/awk/:1:1","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["cli"],"content":"awk 使用示例 1. 统计 linux 系统下 tcp 协议所有网络状态条数 root@ops:~# ss -ant | awk '{++d[$1]} END {for (k in d){print k \": \" d[k]}}' | grep -v State LISTEN: 10 ESTAB: 1 语法解析 { # 定义一个数组 d, 键为 $1 (状态名), ++ 表示数组中键名相同时值自动加 1 ++d[$1] } END { # awk 在处理了输入文件中的所有行之后执行这个块 # 输入数组中所有数据，k 是键名，通过键名取数据 for (k in d){ print k \": \" d[k] } } 2. 求和并排序 文本有两列字段，用户名，充值金额。 在数据中用户可能会重复出现，现在需要统计出所有用户的金额总值并按降序排列 root@ops:~# cat b.txt 1234 100 1344 13 1242 783 1234 234 4563 21 4562 145 root@ops:~# awk '{d[$1]+=$2} END { for (n in d){print n \": \" d[n]}}' b.txt | sort -nr -k 2 1242: 783 1234: 334 4562: 145 4563: 21 1344: 13 awk 语法解析 { # 定义一个数据，key 为用户名，value 为 充值金额， # 由于用户名会重复出现金额需要累加，所以使用 += 表达式进行赋值 d[$1]+=$2 } END { # 输入数据中所有值 for (n in d){ print n \": \" d[n] } } 3. 列出占用 80 端口进程的 pid 并结束它 root@ops:~# lsof -i:80 | awk '{ if ($2 ~ /[0-9]/) {print $2}}' | xargs kill 30213 30232 30233 也可以使用: ss -antp | grep ':80 ' | egrep -o 'pid=[0-9]{,5}' | tr -d 'pid=' | xargs kill ","date":"2021-03-03","objectID":"/posts/awk/:1:2","tags":["awk"],"title":"Linux 文本三剑客: awk","uri":"/posts/awk/"},{"categories":["haproxy"],"content":"HAPrxoy 介绍 HAProxy 是一个使用 C 语言编写的自由及开放源代码软件，其提供高可用性、负载均衡，以及基于 tcp 和 http 的应用程序代理。 mode http：七层反向代理，受端口数量限制 mode tcp：四层反向代理，不受套接字文件数量限制 HAProxy 特别适用于那些负载特大的 web 站点，这些站点通常又需要会话保持或七层处理。HAProxy 运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中，同时可以保护你的 web 服务器不被暴露到网络上。 更多内容请查看 官方网站 – 官方文档 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:1:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"安装 HAProxy 以 Ubuntu Server 18.04 操作系统为例, 官方教程 您需要使用以下命令启用专用的PPA： root@lb-01:~# apt-get install --no-install-recommends software-properties-common root@lb-01:~# add-apt-repository ppa:vbernat/haproxy-2.2 然后，使用以下命令： root@lb-01:~# apt-get install haproxy=2.2.\\* 您将获得最新版本的 HAProxy 2.2（并坚持使用此分支）。 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:2:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"HAProxy 服务配置 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"程序环境 主程序：/usr/sbin/haproxy 主配置文件：/etc/haproxy/haproxy.cfg systemd 服务配置文件：/lib/systemd/system/haproxy.service ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:1","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"配置文件说明 root@lb-01:/etc/haproxy# cat haproxy.cfg # 全局配置段 global log /dev/log local0 log /dev/log local1 notice # 指定最大连接数 maxconn 200000 # 限制 haproxy 根路径 chroot /var/lib/haproxy # 指定 haproxy 管理 socket 文件并与指定的进程绑定(process) stats socket /run/haproxy/admin1.sock mode 660 level admin expose-fd listeners process 1 stats socket /run/haproxy/admin2.sock mode 660 level admin expose-fd listeners process 2 stats timeout 30s # 指定 haproxy 运行的用户和组 user haproxy group haproxy # 开启守护进程 daemon # 避免对于后端检测同时并发造成的问题，设置错开时间比，范围0到50，一般设置2-5较好 spread-checks 5 # 开启多进程 nbproc 2 # 设定进程与 CPU 绑定 cpu-map 1 0 cpu-map 2 1 # 默认参数配置段， 为 frontend, listen, backend 提供默认配置 defaults log global # 使用 http 模式 mode http # 日志类别 http 日志格式 option httplog # 对应的服务器挂掉后,强制定向到其他健康的服务器 option redispatch # 不记录健康检查的日志信息 option dontlognull # 开启 ip 透传，向后端服务发送真实客户端地址 option forwardfor # 开启 http 长连接 option http-keep-alive # 长连接超时时间设置 timeout http-keep-alive 120s # 连接超时时间设置 timeout connect 60s # 与后端服务器连接超时时间设置 timeout server 600s # 与客户端连接超时时间设置 timeout client 600s # check 检测超时时间设置 timeout check 5s # 配置默认 http 错误响应码对应的页面文件 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # 开启状态页 listen stauts mode http bind :9999 stats enable # 指定状态面的 uri 路径 stats uri /haproxy-status # 配置状态页的用户名和密码 stats auth haproxy:liwg # 配置 http 代理 listen http # 代理监控端口 bind *:80 # 使用的负载均衡调度算法 balance roundrobin # 在后端启用基于 cookie 的持久性 cookie SRVID insert indirect nocache # 后端服务器配置列表, cookie: 指定 cookie值， check: 启用健康检测 # inter 检测间隔， fall 检测失败次数， rise 重试次数 server server1 192.168.31.31:80 cookie 31 check inter 3s fall 3 rise 5 server server2 192.168.31.32:80 cookie 32 check inter 3s fall 3 rise 5 当使用 haproxy 负载均衡集群时，监听的地址为 vip 时，服务会无法启动， 这是因为 Linux 绑定了一个网卡上没有配置的 ip 地址导致的， 此时可以通过修改 Linux 内核 net.ipv4.ip_nonlocal_bind = 1 参数解决 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:3:2","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"启动 HAProxy 服务并测试 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:0","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"启动 HAProxy 服务 root@lb-01:/etc/haproxy# systemctl start haproxy.service root@lb-01:/etc/haproxy# ss -anptl | grep haproxy LISTEN 0 128 0.0.0.0:9999 0.0.0.0:* users:((\"haproxy\",pid=26699,fd=12),(\"haproxy\",pid=26697,fd=12)) LISTEN 0 128 0.0.0.0:80 0.0.0.0:* users:((\"haproxy\",pid=26699,fd=14),(\"haproxy\",pid=26697,fd=14)) root@lb-01:/etc/haproxy# ps -ef | grep haproxy root 26690 1 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock haproxy 26697 26690 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock haproxy 26699 26690 0 14:32 ? 00:00:00 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock 端口 9999 是 haproxy 状态页，端口 80 用于后端服务器负载均衡， 通过 ps 命令， 可以看到 haproxy 开启了多进程 ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:1","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["haproxy"],"content":"动态下线/上线后端服务器 动态下线后端服务器 root@lb-01:/etc/haproxy# echo 'disable server http/server2' | socat stdio /run/haproxy/admin1.sock root@lb-01:/etc/haproxy# echo 'disable server http/server2' | socat stdio /run/haproxy/admin2.sock 注意： 当输入软下线的命令时 haproxy 依旧可以将用户的请求调度到后端已经下线的服务器上，这是因为 haproxy 的 socket 文件的关系，一个 socket 文件对应一个进程，当 haproxy 处于多进程的模式下时，就需要有多个 socket 文件，并将其和进程进行绑定，对后端服务器进行软下线时需要对所有的 socket 文件下达软下线的指令。 可以通过 http://haproxy_ipaddress:9999/haproxy-status 查看后端服务器状态 动态上线后端服务器 root@lb-01:/etc/haproxy# echo 'enable server http/server2' | socat stdio /run/haproxy/admin1.sock root@lb-01:/etc/haproxy# echo 'enable server http/server2' | socat stdio /run/haproxy/admin2.sock ","date":"2021-02-22","objectID":"/posts/haproxy-install/:4:2","tags":["haproxy"],"title":"HAPrxoy 的简单使用","uri":"/posts/haproxy-install/"},{"categories":["tomcat"],"content":"配置优化 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:0","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"修改 Server 节点 shutdown 属性值为长随机数 \u003cServer port=\"8005\" shutdown=\"d41d8cd98f00b204e9800998ecf8427e\"\u003e ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:1","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"启用 tomcat 线程池 使用线程池，用较少的线程处理较多的访问，可以提高tomcat处理请求的能力。使用方式： 打开 conf/server.xml，增加 \u003cExecutorname=\"tomcatThreadPool\" namePrefix=\"catalina-exec-\" maxThreads=\"500\" minSpareThreads=\"20\" maxIdleTime=\"60000\" prestartminSpareThreads=\"true\" maxQueueSize=\"100\"/\u003e 属性说明 name: 线程名称 namePrefix: 线程前缀 maxThreads : 最大并发连接数，不配置时默认200，一般建议设置500~ 800 ，要根据自己的硬件设施条件和实际业务需求而定。 minSpareThreads：Tomcat 启动初始化的线程数，默认值25 prestartminSpareThreads：在 Tomcat 初始化的时候就初始化 minSpareThreads 的值 maxQueueSize: 最大的等待队列数，超过则拒绝请求 maxIdleTime：线程最大空闲时间60秒 然后，修改 Connector 节点，增加 executor 属性，如: \u003cConnector port=\"8080\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" executor=\"tomcatThreadPool\" connectionTimeout=\"20000\" enableLookups=\"false\" redirectPort=\"8443\" maxPostSize=\"20971520\" acceptCount=\"2000\" acceptorThreadCount=\"2\" disableUploadTimeout=\"true\" URIEncoding=\"utf-8\"/\u003e 属性说明 port ：连接端口。 protocol：连接器使用的传输方式。 executor： 连接器使用的线程池名称 enableLookups：禁用DNS 查询 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理，默认设置 100 。 maxPostSize：限制以 FORM URL 参数方式的POST请求的内容大小，单位字节，默认是 2097152(2M)，10485760 为 10M。如果要禁用限制，则可以设置为 -1。 acceptorThreadCount： 用于接收连接的线程的数量，默认值是1。一般这个指需要改动的时候是因为该服务器是一个多核CPU，如果是多核 CPU 一般配置为 2。 disableUploadTimeout：上传时是否使用超时机制，以是 servlet 有较长时间来完成它的执行，默认值为 false； URIEncoding: 指定 Url 字符编码，防止出现乱码 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:1:2","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"运行权限优化 默认情况下 Tomcat 服务是以 root 用户运行的，为了减少安全隐患需要更改为普通用户运行 Tomcat 服务 [root@10-7-171-239 apache-tomcat-9.0.43]# cd bin/ [root@10-7-171-239 bin]# tar xzf commons-daemon-native.tar.gz [root@10-7-171-239 bin]# cd commons-daemon-1.2.4-native-src/unix/ [root@10-7-171-239 unix]# ./configure [root@10-7-171-239 unix]# make [root@10-7-171-239 unix]# mv jsvc /usr/local/apache-tomcat-9.0.43/bin [root@10-7-171-239 unix]# cd /usr/local/apache-tomcat-9.0.43/bin [root@10-7-171-239 bin]# cat \u003e setenv.sh \u003c\u003cEOF #!/bin/bash JAVA_HOME=/usr/local/jdk TOMCAT_USER=tomcat JSVC_OPTS='-jvm server' JAVA_OPTS=\"-server -Xms3072m -Xmx3072m -Djava.security.egd=file:/dev/./urandom\" # 开启监控配置 #CATALINA_OPTS=\"${CATALINA_OPTS} -Dcom.sun.management.jmxremote.port=\u003c监听端口\u003e -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=\u003c本机ip地址\u003e -Dcom.sun.management.jmxremote\" EOF [root@10-7-171-239 bin]# useradd -r tomcat [root@10-7-171-239 apache-tomcat-9.0.43]# chown -R tomcat.tomcat /usr/local/apache-tomcat-9.0.43/ 注意 setenv.sh 配置文件中的 jvm 内存大小，请根据实际情况进行配置。 此时就可以使用 daemon.sh 脚本对 Tomcat 服务进行启停操作了 [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/daemon.sh start [root@10-7-171-239 apache-tomcat-9.0.43]# ps -ef | grep tomcat root 31610 1 0 11:54 ? 00:00:00 jsvc.exec -jvm server -java-home /usr/local/jdk -user tomcat -pidfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.pid -wait 10 -umask 0027 -outfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.out -errfile \u00261 -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/commons-daemon.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -server -Xms512m -Xmx512m -Djava.security.egd=file:/dev/./urandom -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Dignore.endorsed.dirs= -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap tomcat 31611 31610 30 11:54 ? 00:00:03 jsvc.exec -jvm server -java-home /usr/local/jdk -user tomcat -pidfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.pid -wait 10 -umask 0027 -outfile /usr/local/apache-tomcat-9.0.43/logs/catalina-daemon.out -errfile \u00261 -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/commons-daemon.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -server -Xms512m -Xmx512m -Djava.security.egd=file:/dev/./urandom -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Dignore.endorsed.dirs= -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap root 31645 29718 0 11:54 pts/0 00:00:00 grep --color=auto tomcat 从上面的进程信息可以看到 Tomcat 服务的进程，现在已经是使用的 tomcat 用户运行了。 ","date":"2021-02-22","objectID":"/posts/tomcat-optimization/:2:0","tags":["tomcat"],"title":"Tomcat 配置及运行权限优化","uri":"/posts/tomcat-optimization/"},{"categories":["tomcat"],"content":"Tomcat 介绍 Tomcat 是 Apache 软件基金会（Apache Software Foundation）项目中的一个核心项目，由 Apache、Sun 和其他一些公司及个人共同开发而成。 Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试 JSP 程序的首选。 实际生产环境中建议和 nginx 配合一起使用，nginx 处理静态，tomcat 处理动态程序 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:1:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"开始安装 安装 tomcat 前需先安装 JDK 工具包。 JDK 是 java 语言的软件开发工具包，它包含了 java 的运行环境（jvm + java 系统类库）和 java 工具。 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"安装 JDK JDK 下载地址: https://www.oracle.com/java/technologies/javase-downloads.html 这里我们选择安装 JDK 8 [root@10-7-171-239 src]# wget https://download.oracle.com/otn/java/jdk/8u281-b09/89d678f2be164786b292527658ca1605/jdk-8u281-linux-x64.tar.gz?AuthParam=1614219040_36465185941d2c06fb1457b5fc724aee -O jdk-8u281-linux-x64.tar.gz [root@10-7-171-239 src]# tar xzf jdk-8u281-linux-x64.tar.gz -C /usr/local [root@10-7-171-239 src]# cd /usr/local/ [root@10-7-171-239 local]# ln -s jdk1.8.0_281/ jdk 配置 JDK 环境变量 [root@10-7-171-239 local]# cat \u003e /etc/profile.d/jdk.sh \u003c\u003c EOF \u003e export JAVA_HOME=/usr/local/jdk \u003e export PATH=\\$JAVA_HOME/bin:\\$PATH \u003e EOF [root@10-7-171-239 local]# source /etc/profile [root@10-7-171-239 local]# java -version java version \"1.8.0_281\" Java(TM) SE Runtime Environment (build 1.8.0_281-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode) ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"安装 Tomcat Tomcat 下载地址: https://tomcat.apache.org/download-90.cgi [root@10-7-171-239 src]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gz [root@10-7-171-239 src]# tar xzf apache-tomcat-9.0.43.tar.gz -C /usr/local/ Tomcat 的安装很简单，只要下载解压即可开始使用 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:2","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat 服务启停介绍 默认 Tomcat 启动可以直接执行 bin 目录的 startup.sh 脚本，停止使用 shutdown.sh 脚本，如果你查看这两个脚本文件中的内容会发现它们都是通过调用 catalina.sh 脚本并传递相应的参数进行启动的。 所以我们可以直接使用 .catalina.sh 脚本进行 Tomcat 服务的启动与停止. catalina.sh 脚本帮助信息 [root@10-7-171-239 bin]# ./catalina.sh Using CATALINA_BASE: /usr/local/apache-tomcat-9.0.43 Using CATALINA_HOME: /usr/local/apache-tomcat-9.0.43 Using CATALINA_TMPDIR: /usr/local/apache-tomcat-9.0.43/temp Using JRE_HOME: /usr/local/jdk Using CLASSPATH: /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar Using CATALINA_OPTS: Usage: catalina.sh ( commands ... ) commands: debug Start Catalina in a debugger debug -security Debug Catalina with a security manager jpda start Start Catalina under JPDA debugger run Start Catalina in the current window run -security Start in the current window with security manager start Start Catalina in a separate window start -security Start in a separate window with security manager stop Stop Catalina, waiting up to 5 seconds for the process to end stop n Stop Catalina, waiting up to n seconds for the process to end stop -force Stop Catalina, wait up to 5 seconds and then use kill -KILL if still running stop n -force Stop Catalina, wait up to n seconds and then use kill -KILL if still running configtest Run a basic syntax check on server.xml - check exit code for result version What version of tomcat are you running? Note: Waiting for the process to end and use of the -force option require that $CATALINA_PID is defined ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:3","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"启动 Tomcat 服务 [root@10-7-171-239 apache-tomcat-9.0.43]# /usr/local/apache-tomcat-9.0.43/bin/startup.sh Using CATALINA_BASE: /usr/local/apache-tomcat-9.0.43 Using CATALINA_HOME: /usr/local/apache-tomcat-9.0.43 Using CATALINA_TMPDIR: /usr/local/apache-tomcat-9.0.43/temp Using JRE_HOME: /usr/local/jdk Using CLASSPATH: /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar Using CATALINA_OPTS: Tomcat started. [root@10-7-171-239 apache-tomcat-9.0.43]# ss -anptl | grep java LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:((\"java\",pid=30004,fd=68)) LISTEN 0 100 :::8080 :::* users:((\"java\",pid=30004,fd=57)) Tomcat 默认监听于 8080 端口，直接在浏览器上访问 http://\u003cyour_server_ipaddress\u003e:8080 [root@10-7-171-239 apache-tomcat-9.0.43]# ps -ef | grep tomcat root 30004 1 8 10:23 pts/0 00:00:02 /usr/local/jdk/bin/java -Djava.util.logging.config.file=/usr/local/apache-tomcat-9.0.43/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/apache-tomcat-9.0.43/bin/bootstrap.jar:/usr/local/apache-tomcat-9.0.43/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/apache-tomcat-9.0.43 -Dcatalina.home=/usr/local/apache-tomcat-9.0.43 -Djava.io.tmpdir=/usr/local/apache-tomcat-9.0.43/temp org.apache.catalina.startup.Bootstrap start root 30038 29718 0 10:24 pts/0 00:00:00 grep --color=auto tomcat 通过查看 tomcat 进程可以看到 tomcat 默认使用 root 用户启动，这会存在安全风险，那该如何使用普通用户启动呢？ ","date":"2021-02-22","objectID":"/posts/tomcat-install/:2:4","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat 目录结构 [root@10-7-171-239 apache-tomcat-9.0.43]# tree -L 1 -d . ├── bin # 管理脚本存放路径 ├── conf # 配置文件目录 ├── lib # 公共程序库目录 ├── logs # 日志目录 ├── temp ├── webapps # 默认 web 应用程序目录 └── work ","date":"2021-02-22","objectID":"/posts/tomcat-install/:3:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"webapps 目录 webapps 目录存放都是 web 应用，每个目录都是单独的应用。其中 ROOT 比较特殊，ROOT 目录中的应用是打开网页可以直接访问到的，例如 http://localhost:8080 访问的是 ROOT 目录中的应用，如果需要访问 docs 应用需要在 url 上加上 http://localhost:8080/docs/ 路径。 [root@10-7-171-239 apache-tomcat-9.0.43]# tree webapps -L 1 -d webapps ├── docs ├── examples ├── host-manager ├── manager └── ROOT 默认 webapps 中有 Tomcat 自带管理应用，不用可以移除。 ","date":"2021-02-22","objectID":"/posts/tomcat-install/:3:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"配置 Tomcat ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:0","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"配置介绍 Tomcat 的配置文件存放在 conf 目录中，其中 server.xml 为主配置文件。默认配置信息如下 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!-- port: tomcat 服务管理端口， shutdown: 服务停止字符，如果从服务管理端口接收到此字符 tomcat 服务将会停止， 有安全风险，建议更改为更复杂的字符串。 可以使用 cat /dev/urandom | head -n 1 | md5sum 生成 --\u003e \u003cServer port=\"8005\" shutdown=\"SHUTDOWN\"\u003e \u003cListener className=\"org.apache.catalina.startup.VersionLoggerListener\" /\u003e \u003cListener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /\u003e \u003cListener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /\u003e \u003cListener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /\u003e \u003cListener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /\u003e \u003cGlobalNamingResources\u003e \u003cResource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /\u003e \u003c/GlobalNamingResources\u003e \u003cService name=\"Catalina\"\u003e \u003c!-- HTTP 服务监听的端口 --\u003e \u003cConnector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /\u003e \u003c!-- defaultHost: 定义缺省处理请求的虚拟主机域名 --\u003e \u003cEngine name=\"Catalina\" defaultHost=\"localhost\"\u003e \u003cRealm className=\"org.apache.catalina.realm.LockOutRealm\"\u003e \u003cRealm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/\u003e \u003c/Realm\u003e \u003c!-- 虚拟主机定义, name: 域名， appBase: WEB 应用程序路径 --\u003e \u003cHost name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e \u003c/Host\u003e \u003c/Engine\u003e \u003c/Service\u003e \u003c/Server\u003e ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:1","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["tomcat"],"content":"Tomcat url 路径配置 如果想让 Tomcat 的根指向为 webapps 中的 test 应用该如何配置？？ 准备测试数据 [root@10-7-171-239 webapps]# mkdir test [root@10-7-171-239 webapps]# echo 'hello test file' \u003e test/index.html 修改 server.xml 配置文件, 在 Host 节点加入 Context 配置项，具体内容如下 \u003cContext path=\"/\" docBase=\"test/\" reloadable=\"false\" debug=\"0\" /\u003e path: 指定 url 路径，如果是 / 可以忽略不写 docBase: 用于指定 WEB 应用路径，可以是相当路径（相对于 Host 的 appBase 属性的值），也可以是绝对路径 reloadable: 是否自动重载，建议\u0008值设置为 false，此属性会影响服务性能 重启服务，测试 [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/shutdown.sh [root@10-7-171-239 apache-tomcat-9.0.43]# ./bin/startup.sh [root@10-7-171-239 apache-tomcat-9.0.43]# curl localhost:8080 hello test file ","date":"2021-02-22","objectID":"/posts/tomcat-install/:4:2","tags":["tomcat"],"title":"Tomcat 的简单使用","uri":"/posts/tomcat-install/"},{"categories":["keepalived","lvs"],"content":"环境准备 两台调度服务器: 安装 keepalived + ipvsadm 两台真实服务器: 安装 nginx 提供 Web 服务 web-01: 192.168.31.31/24 web-02: 192.168.31.32/24 lb-01: 192.168.31.33/24 lb-02: 192.168.31.34/24 vip: 192.168.31.30 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:1:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 Real Server 服务器 安装 nginx 来提供一个简单 Web 页面用来测试 web-01 root@web-01:~# apt install nginx root@web-01:~# echo '\u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-01:~# systemctl start nginx root@web-01:~# curl localhost \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e web-02 root@web-02:~# apt install nginx root@web-02:~# echo '\u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-02:~# systemctl start nginx root@web-02:~# curl localhost \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:2:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 lvs 调用可用集群 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"安装 keepalived 与 ipvs 管理工具 root@lb-01:~# apt install keepalived ipvsadm ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:1","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 keepalived 配置主节点 /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface ens32 virtual_router_id 50 priority 100 advert_int 1 nopreempt authentication { auth_type PASS auth_pass 11111 } virtual_ipaddress { 192.168.31.30 } } virtual_server 192.168.31.31 80 { delay_loop 6 ! 使用 rr 调度算法 lb_algo rr ! 使用 DR 直接路由工作方式 lb_kind DR ! 会话保持时间，单位是秒。这个选项对动态网页是非常有用的，为集群系统中的 SEEION 共享 ! 提供了一个很好的解决方案。有了这个会话保持功能，用户的请求会一直分发到同一个服务节点， ! 直到超过这个会话的保持时间。 注意: 这个会话保持时间是最大无响应超时时间。如果用户一直 ! 在操作动态页面，是不受这个时间限制的。 persistence_timeout 50 ! 使用转发的协议类型， 也可以是 UDP protocol TCP ! 真实提供服务器的机器 real_server 192.168.31.31 80 { ! 定义权重 weight 3 ! real_server 状态检测，单位为秒 TCP_CHECK { ! 表示3秒无响应超时 connect_timeout 3 ! 表示重试次数 nb_get_retry 3 ! 重试间隔 delay_before_retry 3 } } real_server 192.168.31.32 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } } 配置备用节点 /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state BACKUP interface ens32 virtual_router_id 50 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 11111 } virtual_ipaddress { 192.168.31.30 } } virtual_server 192.168.31.31 80 { delay_loop 6 lb_algo rr lb_kind DR persistence_timeout 50 protocol TCP real_server 192.168.31.31 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } real_server 192.168.31.32 80 { weight 3 TCP_CHECK { connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } } ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:2","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"启动 keepalivd 服务 启动 keepalived 服务并查看 ipvs 规则， vip 配置情况 root@lb-01:/etc/keepalived# systemctl start keepalived.service root@lb-01:/etc/keepalived# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.31.30:80 rr persistent 50 -\u003e 192.168.31.31:80 Route 3 0 0 -\u003e 192.168.31.32:80 Route 3 0 0 root@lb-01:/etc/keepalived# ip addr show ens32 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 备用节点的 keepalived 服务也启动起来 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:3","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"配置 RealServer 由于 lvs 的 DR 方式，需要在两台 Real Server 上也配置 vip 地址，并且还需要抑制 arp 广播。 使用以下脚本完成。 #!/bin/bash # # filename: lvs-vip.sh # VIP=192.168.31.30 INTERFACE=lo case $1 in start) echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The RS Server is Ready!\" ;; stop) ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce echo \"The RS Server is Canceled!\" ;; *) echo $\"Usage: $0{start|stop|restart}\" exit 1 ;; esac 启用 vip root@web-01:~# mv lvs-vip.sh /usr/local/bin/ root@web-01:~# chmod +x /usr/local/bin/lvs-vip.sh root@web-01:~# lvs-vip.sh start The RS Server is Ready! ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:3:4","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived","lvs"],"content":"测试 lvs 高可用集群 测试过程略，请自行测试… 可以将主 lvs 服务器宕机，然后查看服务是否会中断， 备用 lvs 服务器是否能接手，并正确配置 vip 提供调度服务。 ","date":"2021-02-21","objectID":"/posts/keepalived-lvs-dr/:4:0","tags":["keepalived","lvs","ipvsadm"],"title":"Keepalived 实现 LVS 高可用负载均衡群集","uri":"/posts/keepalived-lvs-dr/"},{"categories":["keepalived"],"content":"Keepalived 介绍 Keepalived 是一个基于 VRRP 协议来实现的 WEB 服务高可用方案，可以利用其来避免单点故障。使用多台节点安装keepalived。其他的节点用来提供真实的服务（RealServer），同样的，他们对外表现一个虚拟的IP（vip）。主服务器宕机的时候，备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。 ","date":"2021-02-21","objectID":"/posts/keepalived/:1:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"Keepalived 简单应用 拓扑图 ","date":"2021-02-21","objectID":"/posts/keepalived/:2:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"环境准备 本次实验需要准备2台后端 Web 服务器，用于提供真实服务，最前端挂2台 nginx 反向代理并安装 keepalived 实现高可用。 总共4台机器。 实现的目的 当用户请求 web 服务器时，请求先到 keeplived 主服务器，然后 keepalived 主服务器根据 nginx 的负载均衡配置规则 选择 1台后端的 web 服务器将用户的请求转发给它，keepalived 主服务器拿到相应的数据后在响应给用户。如果keepalived 主服务器的 nginx 服务挂了，将由备份 keepalived 服务器接手为用户提供服务。 nginx：反向代理、负载均衡配置参考 nginx 文档 ","date":"2021-02-21","objectID":"/posts/keepalived/:2:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装配置 keepalived ","date":"2021-02-21","objectID":"/posts/keepalived/:3:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装 keepalived root@lb-01:~# apt install keepalived # 复制一份示例配置文件 root@lb-01:~# cp /usr/share/doc/keepalived/samples/keepalived.conf.sample /etc/keepalived/keepalived.conf ","date":"2021-02-21","objectID":"/posts/keepalived/:3:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"keepalived 高可用配置 keepalived 的配置文件默认位于：/etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { ! 运行Keepalived服务器的一个标识，发邮件时显示在邮件主题中。 router_id LVS_DEVEL } ! 定义 nginx 服务检测脚本 vrrp_script check_nginx { script \"/bin/bash /etc/keepalived/chk_ngx.sh\" ! 检查时间间隔，2秒 interval 2 } ! 定义高可用实例 vrrp_instance VI_1 { ! 指明当前服务的角色，备用为 BACKUP state MASTER ! 指定监测的网络接口 interface ens32 ! 虚拟路由标识，同一个实例使用唯一的标识，master与backup必须是一致的 virtual_router_id 50 ! 定义节点的优先级，数字越大，优先级越高。 priority 100 ! 主从主机之间同步检查的时间间隔 advert_int 1 ! 不抢占，恢复后不进行切换，主挂了再起来不会抢回身份。防止 vip 飘来飘去影响稳定性 nopreempt ! 用于设置节点间通信验证类型和密码 authentication { auth_type PASS auth_pass 11111 } ! 设置虚拟 ip 地址 virtual_ipaddress { 192.168.31.30 } ! 检查条件 track_script { check_nginx } } /etc/keepalived/chk_ngx.sh #!/bin/bash /bin/pidof nginx \u0026\u003e /dev/null ## 也可使用 killall -0 nginx 探测服务状态 if [[ $? -ne 0 ]]; then echo 'nginx is stop' exit 1 fi 将 keepalived.conf 和 chk_ngx.sh 复制到备份主机上，修改 keepalived.conf 文件 state BACKUP, priority 90, 删除 nopreempt 即可。 ","date":"2021-02-21","objectID":"/posts/keepalived/:3:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装反代和Web服务 ","date":"2021-02-21","objectID":"/posts/keepalived/:4:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装 Web 服务 这里使用 nginx 来提供一个简单 Web 页面用来测试 web-01 root@web-01:~# apt install nginx root@web-01:~# echo '\u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-01:~# systemctl start nginx root@web-01:~# curl localhost \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e web-02 root@web-02:~# apt install nginx root@web-02:~# echo '\u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e' \u003e /var/www/html/index.nginx-debian.html root@web-02:~# systemctl start nginx root@web-02:~# curl localhost \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e ","date":"2021-02-21","objectID":"/posts/keepalived/:4:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"安装配置 nginx 反向代理 在两台 keepalived 机器上安装 nginx 并配置反向代理到后端两台 Web 服务器, 两台机的配置是一样的。 root@lb-01:~# apt install nginx root@lb-01:~# cat /etc/nginx/sites-available/default upstream web { server 192.168.31.31; server 192.168.31.32; } server { listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / { proxy_pass http://web; } } root@lb-01:~# systemctl restart nginx ","date":"2021-02-21","objectID":"/posts/keepalived/:4:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"测试 keepalived 高可用 ","date":"2021-02-21","objectID":"/posts/keepalived/:5:0","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"启用 keepalived 服务 root@lb-01:~# systemctl start keepalived.service root@lb-01:~# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 日志信息 Feb 27 08:10:46 lb-01 systemd[1]: Starting Keepalive Daemon (LVS and VRRP)... Feb 27 08:10:46 lb-01 Keepalived[8259]: Starting Keepalived v1.3.9 (10/21,2017) Feb 27 08:10:46 lb-01 Keepalived[8259]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 systemd[1]: Started Keepalive Daemon (LVS and VRRP). Feb 27 08:10:46 lb-01 Keepalived[8275]: Starting Healthcheck child process, pid=8276 Feb 27 08:10:46 lb-01 Keepalived[8275]: Starting VRRP child process, pid=8277 Feb 27 08:10:46 lb-01 Keepalived_healthcheckers[8276]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering Kernel netlink reflector Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering Kernel netlink command channel Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Registering gratuitous ARP shared channel Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: WARNING - default user 'keepalived_script' for script execution does not exist - please create. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: (VI_1): Warning - nopreempt will not work with initial state MASTER Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: Using LinkWatch kernel netlink reflector... Feb 27 08:10:46 lb-01 Keepalived_vrrp[8277]: VRRP_Script(check_nginx) succeeded Feb 27 08:10:47 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Transition to MASTER STATE Feb 27 08:10:48 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Entering MASTER STATE 从上面的信息可以看到， keepalived 自动为 ens32 网卡添加一个 vip，服务角色为 master 开启备份节点服务 root@lb-02:/etc/keepalived# systemctl start keepalived.service ","date":"2021-02-21","objectID":"/posts/keepalived/:5:1","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["keepalived"],"content":"测试 测试使用 vip 访问 Web 服务 root@web-01:~# while :; do curl 192.168.31.30; sleep .5; done \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e 停止 keepalived 主服务的 nginx 服务，查看 vip 是否转移。Web 服务是否可用。 root@lb-01:~# systemctl stop nginx 主节点日志信息 Feb 27 08:17:15 lb-01 systemd[1]: Stopping A high performance web server and a reverse proxy server... Feb 27 08:17:15 lb-01 systemd[1]: Stopped A high performance web server and a reverse proxy server. Feb 27 08:17:17 lb-01 Keepalived_vrrp[8277]: /bin/bash /etc/keepalived/chk_ngx.sh exited with status 1 Feb 27 08:17:17 lb-01 Keepalived_vrrp[8277]: VRRP_Script(check_nginx) failed Feb 27 08:17:18 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Entering FAULT STATE Feb 27 08:17:18 lb-01 Keepalived_vrrp[8277]: VRRP_Instance(VI_1) Now in FAULT state Feb 27 08:17:19 lb-01 Keepalived_vrrp[8277]: /bin/bash /etc/keepalived/chk_ngx.sh exited with status 1 日志信息表明，检测到 nginx 服务停止，检测失败，keepalived 进入 FAULT 状态。 查看备用节点的日志和网卡信息 root@lb-02:/etc/keepalived# systemctl start keepalived.service root@lb-02:/etc/keepalived# tail /var/log/syslog Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Registering gratuitous ARP shared channel Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Opening file '/etc/keepalived/keepalived.conf'. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: WARNING - default user 'keepalived_script' for script execution does not exist - please create. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: Using LinkWatch kernel netlink reflector... Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Entering BACKUP STATE Feb 27 08:14:08 ubuntu Keepalived_vrrp[10287]: VRRP_Script(check_nginx) succeeded Feb 27 08:17:01 ubuntu CRON[10641]: (root) CMD ( cd / \u0026\u0026 run-parts --report /etc/cron.hourly) Feb 27 08:17:18 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Transition to MASTER STATE Feb 27 08:17:19 ubuntu Keepalived_vrrp[10287]: VRRP_Instance(VI_1) Entering MASTER STATE root@lb-02:/etc/keepalived# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:0c:ee:46 brd ff:ff:ff:ff:ff:ff inet 192.168.31.34/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe0c:ee46/64 scope link valid_lft forever preferred_lft forever 从日志和网卡信息可以看出，备用节点接替了主节点的 vip ，并将自己提升为主节点 客户端请求测试信息 \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused curl: (7) Failed to connect to 192.168.31.30 port 80: Connection refused \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 11111111111!\u003c/h1\u003e \u003ch1\u003eWelcome to nginx 22222222!\u003c/h1\u003e 可以看到在 vip 切换时，有一些无法响应的请求，之后又立即恢复了 ","date":"2021-02-21","objectID":"/posts/keepalived/:5:2","tags":["keepalived"],"title":"Keepalived 高可用简单入门","uri":"/posts/keepalived/"},{"categories":["lvs"],"content":"环境准备 Director Server: 192.168.31.33/24 Real Server 1: 192.168.31.31/24 Real Server 2: 192.168.31.32/24 VIP: 192.168.31.30/32 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:1:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"安装配置 ipvs ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"安装 ipvs 管理软件 在 Director Server 上安装 ipvsadm root@lb-01:~# apt install ipvsadm ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"配置 ipvs 配置 vip root@lb-01:~# ip addr add 192.168.31.30/32 brd 192.168.31.30 dev ens32 root@lb-01:~# ip addr show ens32 2: ens32: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:af:9c:1f brd ff:ff:ff:ff:ff:ff inet 192.168.31.33/24 brd 192.168.31.255 scope global ens32 valid_lft forever preferred_lft forever inet 192.168.31.30/32 brd 192.168.31.30 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feaf:9c1f/64 scope link valid_lft forever preferred_lft forever 配置 ipvs 规则 # 清空 ipvsadm 配置表 root@lb-01:~# ipvsadm -C # 设置连接超时值 root@lb-01:~# ipvsadm --set 30 5 60 # 添加虚拟服务器 root@lb-01:~# ipvsadm -A -t 192.168.31.30:80 -s rr -p 20 # 向虚拟服务器添加 RealServer root@lb-01:~# ipvsadm -a -t 192.168.31.30:80 -r 192.168.31.31:80 -g -w 1 root@lb-01:~# ipvsadm -a -t 192.168.31.30:80 -r 192.168.31.32:80 -g -w 1 # 查看 lvs 状态 root@lb-01:~# ipvsadm -L -n --stats IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes -\u003e RemoteAddress:Port TCP 192.168.31.30:80 0 0 0 0 0 -\u003e 192.168.31.31:80 0 0 0 0 0 -\u003e 192.168.31.32:80 0 0 0 0 0 参数说明 -C: 清空 ipvs 所有规则 -A: 添加虚拟服务器 -t: 使用 tcp 协议, 后接虚拟服务器的 ip 地址和端口 -s: 使用的调度算法 -p: 定义持久时间，能够实现将来自同一个地址的请求始终发往同一个 RealServer -a: 添加 RealServer -r: 指定 RealServer 地址和端口 -g: 使用的 DR 实现方式 -w: RealServer 服务器的权重 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:2:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"RealServer 服务器配置 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"向回环接口（lo）添加 vip root@web-01:~# ip addr add 192.168.31.30/32 brd 192.168.31.30 dev lo ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"抑制 arp 广播 禁止发送 arp 广播，由于 vip 在多台机器上配置了，如果不抑制广播会造成 ip 地址冲突。 root@web-01:~# echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore root@web-01:~# echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore root@web-01:~# echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce root@web-01:~# echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ","date":"2021-02-20","objectID":"/posts/lvs-configure/:3:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"在客户机测试 LVS 负载群集 curl 192.168.31.30 注意: 至此 lvs 负载均衡集群配置完成，由于 lvs 服务器本身不支持高可用，存在单点故障， 可以配合 keepalived 一起使用 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:4:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"服务脚本 lvs 的配置是终端上配置的，机器重启后会丢失，那么该如何管理配置呢？ 有两种方法: ipvsadm 包自带配置信息管理工具，ipvsadm-save, ipvsadm-restore 开发脚本管理 ipvsadm-save: 用于导出 lvs 配置 ipvsadm-restore: 用于从文件中恢复 lvs 配置 也可以使用 ipvsadm 包中的脚本管理 /etc/init.d/ipvsadm ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:0","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"lvs 配置管理 root@lb-01:~# ipvsadm-save \u003e /etc/ipvsadm.rules # 等价于 /etc/init.d/ipvsadm save root@lb-01:~# ipvsadm-restore \u003c /etc/ipvsadm.rules # 等价于 /etc/init.d/ipvsadm load root@lb-01:~# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.31.30:80 rr persistent 20 -\u003e 192.168.31.31:80 Route 1 0 0 -\u003e 192.168.31.32:80 Route 1 0 0 ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:1","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"lvs 服务器配置脚本 #!/bin/bash INTERFACE=ens32 PORT=80 VIP=192.168.31.30 RIP=( 192.168.31.31 192.168.31.32 ) IPVSADM=/usr/sbin/ipvsadm start() { ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE $IPVSADM -C $IPVSADM --set 30 5 60 $IPVSADM -A -t ${VIP}:${PORT} -s rr -p 20 for (( i = 0; i \u003c echo ${#RIP[*]}; i++ )); do $IPVSADM -a -t ${VIP}:${PORT} -r ${RIP[$i]}:${PORT} -g -w 1 done echo \"The lvs Server is start!\" } stop() { $IPVSADM -C ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The lvs Server is stop!\" } status() { $IPVSADM -L -n --stats } case $1 in start) start ;; stop) stop ;; restart) stop sleep 1 start ;; status) status ;; *) echo $\"Usage: $0{start|stop|status|restart}\" exit 2 ;; esac ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:2","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"RealServer 服务器配置脚本 #!/bin/bash VIP=192.168.31.30 INTERFACE=lo case $1 in start) echo 1 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 2 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce ip addr add ${VIP}/32 brd ${VIP} dev $INTERFACE echo \"The RS Server is Ready!\" ;; stop) ip addr del ${VIP}/32 brd ${VIP} dev $INTERFACE echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 \u003e /proc/sys/net/ipv4/conf/all/arp_announce echo 0 \u003e /proc/sys/net/ipv4/conf/lo/arp_announce echo \"The RS Server is Canceled!\" ;; *) echo $\"Usage: $0{start|stop|restart}\" exit 1 ;; esac ","date":"2021-02-20","objectID":"/posts/lvs-configure/:5:3","tags":["lvs","ipvsadm"],"title":"配置 LVS DR 负载均衡","uri":"/posts/lvs-configure/"},{"categories":["lvs"],"content":"LVS 介绍 Linux 虚拟服务器（Linux Virtual Server, LVS），是一个由章文嵩开发的一款自由软件。利用 LVS 可以实现高可用、可伸缩的 Web、Mail、Cache 等网络服务。 LVS 具有很好的可伸缩性、可靠性和可管性，通过 LVS 要实现的最终目标是：利用 Linux 操作系统和 LVS 集群软件实现一个高可用、高性能、低成本的服务器应用集群。 常用的实现负载均衡集群的开源软件有: LVS、haproxy、Nginx 等 LVS 工作于 OSI模型的传输层, 也可以称为4层负载。Nginx 与 Haproxy 即可以工作在 4 层（传输层）也可以工作于 7 层 （应用层）。 LVM 集群架构由2部分组成：最前端是负载均衡层（Load Balancer），后端是服务器群组 (一般称其为 Real Server) 负载均衡层: 由于一台或多台负载调度器(Director Server)组成。 LVS 核心模板 IPVS 就安装在 Director Server 上，而 Director 的主要作用类似于一个路由器，它含有为完成 LVS 功能所设定的路由表，通过这些路由表把用户的请求分发给服务器群组的应用服务器（Real Server）。 同时，在 Director Server 上还要安装对 Real Server 的监控模块 Ldirectord, 此模块用于监测各个 Real Server 服务健康状况。 在 Real Server 不可用时可以将其从 LVS 路由表中删除，在恢复时加入。 服务器群组层：由一组实际提供服务的机器组成，Real Server 可以在同一个网络中，也可以在不同网络中或者不同物理位置。 Linux 内核原生内转了 LVS 的各个模块，不用任何设置就可以支持 LVS 功能。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:1:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"IPVS 负载均衡实现方式 IPVS 实现负载均衡的方式有3种，分别是 NAT, TUN 和 DR。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/NAT (Virtual Server via Network Address Translation) VS/NAT 方式使用网络地址翻译技术实现虚拟服务器。当用户请求到达调度器时，调度器将请求报文的目标地址(即虚拟IP地址)改写成选定的 RealServer 地址，同时将报文的目标端口也改成选定的 RealServer 的相应端口，最后将报文发送到选定的RealServer。 在服务器得到数据后， RealServer 将数据返回给用户时，需要再次经过负载均衡调度器将报文的源地址和源端口改成虚拟IP地址和相应的端口，然后把数据发送给用户，完成整个负载调度过程。 可以看出，在 NAT 方式下，用户请求和响应的报文都需要经过负载均衡调度器重写，当请求越来越多时，调度器的处理能力将成为瓶颈。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:1","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/TUN (Virtual Server via IP Tunneling) VS/TUN 方式是通过 IP 隧道技术实现虚拟服务器。这种方式的连接调度和管理与 VS/NAT 方式一样，只是报文转发方法不同。 在 VS/TUN 方式中，调度器采用 IP 隧道技术将用户的请求转发到某个 RealServer，而这个 RealServer 将直接响应用户的请求，不再经过前端调度器。 此外，对 RealServer 的地域位置没有要求。 因为在 TUN 方式中，调度器将只处理用户请求的报文，从而使集群系统的吞吐量大大提高。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:2","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"VS/DR (Virtual Server via Direct Routing) VS/DR 就是直接路由技术实现虚拟服务器，这种方式的连接调度和管理与前两种一样，但它的报文转发方法又有所不同，VS/DR 通过改写请求报文的 MAC 地址，将请求发送到 RealServer， 而 RealServer 将响应直接返回给用户，免去了 VS/TUN的 IP 隧道开销。 这种方式是3咱负载调度方式中性能最好的，但是要求 Director Server 与 RealServer 必须由一块网卡连在同一物理网段上。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:2:3","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"负载调度算法 根据前面的介绍，我们了解了LVS的三种工作模式，但不管实际环境中采用的是哪种模式，调度算法进行调度的策略与算法都是LVS的核心技术，LVS在内核中主要实现了一下十种调度算法。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:0","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"1.轮询调度 轮询调度（Round Robin 简称’RR'）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:1","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"2.加权轮询调度 加权轮询（Weight Round Robin 简称’WRR'）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:2","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"3.最小连接调度 最小连接调度（Least Connections 简称’LC'）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:3","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"4.加权最小连接调度 加权最少连接（Weight Least Connections 简称’WLC'）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:4","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"5.基于局部的最少连接 基于局部的最少连接调度（Locality-Based Least Connections 简称’LBLC'）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用’最少连接’的原则选出一个可用的服务器，将请求发送到服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:5","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"6.带复制的基于局部性的最少连接 带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称’LBLCR'）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按’最小连接’原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按’最小连接’原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:6","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"7.目标地址散列调度 目标地址散列调度（Destination Hashing 简称’DH'）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:7","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"8.源地址散列调度 源地址散列调度（Source Hashing 简称’SH'）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:8","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"9.最短的期望的延迟 最短的期望的延迟调度（Shortest Expected Delay 简称’SED'）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3 就把请求交给得出运算结果最小的服务器。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:9","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["lvs"],"content":"10.最少队列调度 最少队列调度（Never Queue 简称’NQ'）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。 ","date":"2021-02-20","objectID":"/posts/lvs-introduction/:3:10","tags":["lvs"],"title":"LVS 负载均衡，介绍","uri":"/posts/lvs-introduction/"},{"categories":["redis"],"content":"使用场景介绍 Memcached：多核的缓存服务，更加适合于多用户并发访问次数较少的应用场景 Redis：单核的缓存服务，单节点情况下，更加适合于少量用户，多次访问的应用场景。 redis 一般是单机多实例架构，配合 redis 集群出现。 ","date":"2021-02-12","objectID":"/posts/redis-install/:1:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"安装 Reis ","date":"2021-02-12","objectID":"/posts/redis-install/:2:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"源码编译安装 Redis cd /usr/local/src wget http://download.redis.io/releases/redis-3.2.12.tar.gz tar xzf redis-3.2.12.tar.gz yum -y install gcc automake autoconf libtool make cd /usr/local/src/redis-3.2.12 make make install ","date":"2021-02-12","objectID":"/posts/redis-install/:2:1","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"安装 Redis 服务 使用源码包中的 install_server.sh 脚本工具，安装 redis 服务 [root@10-7-171-239 redis-3.2.12]# cd utils [root@10-7-171-239 utils]# bash install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] Selecting default: 6379 Please select the redis config file name [/etc/redis/6379.conf] Selected default - /etc/redis/6379.conf Please select the redis log file name [/var/log/redis_6379.log] Selected default - /var/log/redis_6379.log Please select the data directory for this instance [/var/lib/redis/6379] Selected default - /var/lib/redis/6379 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6379 Config file : /etc/redis/6379.conf Log file : /var/log/redis_6379.log Data dir : /var/lib/redis/6379 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6379.conf =\u003e /etc/init.d/redis_6379 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! ","date":"2021-02-12","objectID":"/posts/redis-install/:2:2","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"配置 Redis ","date":"2021-02-12","objectID":"/posts/redis-install/:3:0","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"连接测试 Redis redis 默认配置文件中监听的地址和端口是 127.0.0.1:6379，无密码验证(requirepass) 可以直接使用 redis-cli 命令连接 [root@localhost ~]# /etc/init.d/redis_6379 status Redis is running (1446) [root@localhost ~]# redis-cli 127.0.0.1:6379\u003e ping PONG 输入 ping 指令，redis 回复 PONG w代表连接成功，可以正常与 redis-server 通信 ","date":"2021-02-12","objectID":"/posts/redis-install/:3:1","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"配置 redis 配置文件基础项说明 $ egrep -v '^#|^$' /etc/redis/6379.conf # 绑定的 ip 地址， 只绑定 127.0.0.1 地址是无法对外提供服务的 # 生产环境中建议配置此项 bind 127.0.0.1 10.7.171.239 # 是否启用保护模式 # 在未指定绑定地址，未向客户端请求认证密码。 在此模式下，仅从环回接口接受连接。 # 如果要从外部计算机连接到 Redis 可以使用以下方法 # 1. 关闭保护模式, 通过在线修改 (CONFIG SET protected-mode no) 或者 修改配置文件 # 2. 启动服务时加入 '--protected-mode no' 参数 # 3. 配置 bind 地址或身份验证密码 # 注意：您只需要执行上述操作之一，服务器就可以开始接受来自外部的连接。 protected-mode yes # 连接时验证的密码 requirepass S4Ea0lFLwJjehB91 # 服务监听端口 port 6379 # 是否后台运行 daemonize yes # pidfile 存放路径 pidfile /var/run/redis_6379.pid # 日志 存放路径 logfile /var/log/redis_6379.log # 日志级别 loglevel notice ## RDB 持久化配置 ## # RDB 持久化数据文件, 存储在 dir 选项配置的目录下 dbfilename dump.rdb # 数据持久化存储路径 dir /var/lib/redis/6379 # 900 秒内如果累积 1 个变更就持久化一次 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes # 最大使用内存大小 maxmemory 512m slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 ## AOF 持久化配置，如果用于缓存用途可以不开启 ## appendonly no appendfilename \"appendonly.aof\" appendfsync everysec ","date":"2021-02-12","objectID":"/posts/redis-install/:3:2","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["redis"],"content":"在线查看和修改 redis 配置 在线查看配置项 # 查看所有配置项 127.0.0.1:6379\u003e config get * 1) \"dbfilename\" 2) \"dump.rdb\" 3) \"requirepass\" 4) \"123\" 5) \"masterauth\" 6) \"\" 7) \"unixsocket\" 8) \"\" 9) \"logfile\" .... # 查看验证密码 127.0.0.1:6379\u003e config get maxmemory 1) \"maxmemory\" 2) \"0\" # 模糊查看配置项 127.0.0.1:6379\u003e config get maxm* 1) \"maxmemory\" 2) \"128000000\" 3) \"maxmemory-samples\" 4) \"5\" 5) \"maxmemory-policy\" 6) \"noeviction\" 在线调整配置项 # 配置最大使用内存量 127.0.0.1:6379\u003e config set maxmemory 128m OK # 配置连接验证密码 127.0.0.1:6379\u003e config set requirepass 123 OK # 连接验证 127.0.0.1:6379\u003e auth 123 OK # 查看配置 127.0.0.1:6379\u003e config get maxmemory 1) \"maxmemory\" 2) \"128000000\" 127.0.0.1:6379\u003e config get requirepass 1) \"requirepass\" 2) \"123\" # 将修改的配置修改写入配置文件中 127.0.0.1:6379\u003e config rewrite OK ","date":"2021-02-12","objectID":"/posts/redis-install/:3:3","tags":["redis"],"title":"Redis 的简单安装与使用","uri":"/posts/redis-install/"},{"categories":["mongodb"],"content":"备份工具介绍 MongoDB 自带两种备份工具, 以备份出的文件区分为文本备份工具与二进制备份工具，各有不同的适用场景。 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:0","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"文本备份工具 使用此工具备份出的文件是可读的，备份格式可选为 json 或 csv。 适用场景 异构平台: 当我们需要迁移 mysql 数据至 mongodb 时就可以选用此工具了(相反亦可)。 同平台，跨大版本升级：mongodb2 –\u003e mongodb3 mongoexport: 以 CSV 或 JSON 格式从 MongoDB 导出数据 mongoimport: 将 CSV，TSV 或 JSON 数据导入 MongoDB。 如果未提供文件，则 mongoimport 从 stdin 中读取。 在 test 库中生成测试数据 ues test for (var i=1; i\u003c=10000; i++){ db.rands.insert({id: i, date: new Date()}) } mongoexport json 格式 mongoexport 默认导入数据为 json 格式 $ mongoexport -u root -p root123 --authenticationDatabase admin -d test -c rands -o rands.json 2021-02-24T10:38:15.398+0800 connected to: localhost 2021-02-24T10:38:15.557+0800 exported 10000 records csv 格式 导出 csv 格式需要加上 --type 选项并指定要导出的键名使用 -f 选项 $ mongoexport -u root -p root123 --authenticationDatabase admin -d test -c rands --type=csv -f id,date -o rands.csv 2021-02-24T10:44:01.075+0800 connected to: localhost 2021-02-24T10:44:01.144+0800 exported 10000 records mongoimport 导入 json 数据 $ mongoimport -u root -p root123 --authenticationDatabase admin -d test -c rands --file rands.json 2021-02-24T10:52:55.845+0800 connected to: localhost 2021-02-24T10:52:55.935+0800 imported 10000 documents 导入 csv 数据 如果 csv 文件首行是为列名，需要加入 --headerline 选项，如果不是需要使用 -f 选项指定列名. --headerline: 指明第一行是列名，不需要导入 mongoimport -u root -p root123 --authenticationDatabase admin -d test -c rands --type=csv --headerline --file rands.csv 2021-02-24T10:55:23.714+0800 connected to: localhost 2021-02-24T10:55:23.776+0800 imported 10000 documents 注意: 数据导入是追加导入，所以不重复导入以免数据重复 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:1","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"二进制备份工具 日常备份恢复推荐使用此工具 mongodump 能够在 Mongodb 运行时进行备份，它的工作原理是对运行的 Mongodb 做查询，然后将所有查到的文档写入磁盘。 但是存在的问题时使用 mongodump 产生的备份不一定是数据库的实时快照，如果我们在备份时对数据库进行了写入操作， 则备份出来的文件可能不完全和 Mongodb 实时数据相等。另外在备份时可能会对其它客户端性能产生不利的影响。 mongodump 参数说明 -h: 指明数据库宿主机的 IP -u: 指明数据库的用户名 -p: 指明数据库的密码 --authenticationDatabase: 指明验证库名 -d: 指明数据库的名字 -c: 指明 collection 的名字 -o: 指明到要导出到的路径名 -q: 指明导出数据的过滤条件 -j, --numParallelCollections: 并行转储的集合数（默认为4个） --oplog: 备份的同时备份 oplog 全库备份 不指定 -d 和 -c 选项时备份全库 $ mongodump -u root -p root123 --authenticationDatabase admin -o ./full 2021-02-24T11:13:12.997+0800 writing admin.system.users to 2021-02-24T11:13:12.997+0800 done dumping admin.system.users (1 document) 2021-02-24T11:13:12.997+0800 writing admin.system.version to 2021-02-24T11:13:12.998+0800 done dumping admin.system.version (2 documents) 2021-02-24T11:13:12.998+0800 writing test.rands to 2021-02-24T11:13:13.035+0800 done dumping test.rands (20000 documents) mongodump 备份的是 bson 格式的二进制文件, 备份目录不存在自动创建，目录结构按库名分 备份 test 库 $ mongodump -u root -p root123 --authenticationDatabase admin -d test -o /backup 2021-02-24T11:28:11.957+0800 writing test.rands to 2021-02-24T11:28:12.045+0800 done dumping test.rands (20000 documents) mongorestore 恢复 test 库 $ mongorestore -u root -p root123 --authenticationDatabase admin -d test /backup/test 2021-02-24T11:34:47.061+0800 the --db and --collection args should only be used when restoring from a BSON file. Other uses are deprecated and will not exist in the future; use --nsInclude instead 2021-02-24T11:34:47.061+0800 building a list of collections to restore from test dir 2021-02-24T11:34:47.062+0800 reading metadata for test.rands from test/rands.metadata.json.gz 2021-02-24T11:34:47.067+0800 restoring test.rands from test/rands.bson.gz 2021-02-24T11:34:47.153+0800 no indexes to restore 2021-02-24T11:34:47.153+0800 finished restoring test.rands (20000 documents) 2021-02-24T11:34:47.153+0800 done 当我们恢复数据库出现这样的错误信息 - E11000 duplicate key error collection: test.rands index: _id_ dup key: { : ObjectId('6035c17ea0af461fd150f74c') } 时，是因为数据重复无法写入此可以加入 --drop 选项解决, 但不建议使用 --drop 选项，此操作危险，可能会有数据丢失的风险。 ","date":"2021-02-09","objectID":"/posts/mongodb-backup/:1:2","tags":["mongodb","mongoexport","mongoimport","mongodump","mongorestore"],"title":"MongoDB 备份恢复","uri":"/posts/mongodb-backup/"},{"categories":["mongodb"],"content":"Replication Set 基本原理 MongoDB 复制集的基本构成是一主两从的结构，自带互相监控投标机制，使用 Raft 协议保证数据一致性，（MySQL MGR 用的是 Paxos 变种） 如果发生主库宕机，复制集内部会进行投票选举，选择一个新的主库替代原有主库对外提供服务。同时复制集会自动通知 客户端程序，主库已经发生切换了。应用就会连接到新的主库。 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:1:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"Replication Set 配置过程 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"多实例复制集环境规划 三个以上的 mongodb 节点或多实例, 这里使用多实例。 多实例端口: 28017、28018、28019 多实例配置目录: /data/mongodb/{28017,28018,28019}/etc 多实例配置目录: /data/mongodb/{28017,28018,28019}/logs 多实例数据目录: /data/mongodb/{28017,28018,28019}/data ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"创建多实例环境 使用以下脚本创建 MongoDB 多实例 #!/bin/bash # # filename: mongodb-instances.sh # for port in {28017..28019}; do # 创建多实例目录 mkdir -p /data/mongodb/$port/etc mkdir -p /data/mongodb/$port/logs mkdir -p /data/mongodb/$port/data chown -R mongod.mongod /data/mongodb/$port # 生成配置文件 cat \u003e /data/mongodb/$port/etc/mongod.conf \u003c\u003cEOF systemLog: destination: file path: /data/mongodb/$port/logs/mongodb.log logAppend: true storage: journal: enabled: true dbPath: /data/mongodb/$port/data directoryPerDB: true #engine: wiredTiger wiredTiger: engineConfig: cacheSizeGB: 1 directoryForIndexes: true collectionConfig: blockCompressor: zlib indexConfig: prefixCompression: true processManagement: fork: true net: bindIp: 127.0.0.1,10.7.171.239 port: $port replication: oplogSizeMB: 2048 replSetName: my_repl EOF echo \"start mongodb $portinstance\" echo \"/usr/local/mongodb/bin/mongod -f /data/mongodb/$port/etc/mongod.conf\" done 执行脚本 $ bash mongodb-instances.sh start mongodb 28017 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28017/etc/mongod.conf start mongodb 28018 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28018/etc/mongod.conf start mongodb 28019 instance /usr/local/mongodb/bin/mongod -f /data/mongodb/28019/etc/mongod.conf 启用 mongodb 多实例服务 ## 使用 mongod 用户启动 mongodb 多实例服务 # su - mongod $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28017/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15702 child process started successfully, parent exiting $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28018/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15731 child process started successfully, parent exiting $ /usr/local/mongodb/bin/mongod -f /data/mongodb/28019/etc/mongod.conf about to fork child process, waiting until server is ready for connections. forked process: 15760 child process started successfully, parent exiting ## 查看服务启动状态 $ ss -anptl | grep mongo LISTEN 0 128 10.7.171.239:28017 *:* users:((\"mongod\",pid=15702,fd=12)) LISTEN 0 128 127.0.0.1:28017 *:* users:((\"mongod\",pid=15702,fd=11)) LISTEN 0 128 10.7.171.239:28018 *:* users:((\"mongod\",pid=15731,fd=12)) LISTEN 0 128 127.0.0.1:28018 *:* users:((\"mongod\",pid=15731,fd=11)) LISTEN 0 128 10.7.171.239:28019 *:* users:((\"mongod\",pid=15760,fd=12)) LISTEN 0 128 127.0.0.1:28019 *:* users:((\"mongod\",pid=15760,fd=11)) ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:2:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"配置普通复制集 配置一主两从，从库为两普通从库 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"初始化复制集 $ mongo --port 28017 admin ## 定义初始化信息 \u003e config = {_id: 'my_repl', members: [ {_id: 0, host: '10.7.171.239:28017'}, {_id: 1, host: '10.7.171.239:28018'}, {_id: 2, host: '10.7.171.239:28019'}] } ## 初始化复制集 \u003e rs.initiate(config) { \"ok\" : 1, \"operationTime\" : Timestamp(1614066863, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614066863, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查询复制集状态 my_repl:PRIMARY\u003e rs.status() { \"set\" : \"my_repl\", \"date\" : ISODate(\"2021-02-23T07:56:10.466Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) } }, \"members\" : [ ## 这里记录集群中所有实例的信息及其状态 { \"_id\" : 0, \"name\" : \"10.7.171.239:28017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 421, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1614066874, 1), \"electionDate\" : ISODate(\"2021-02-23T07:54:34Z\"), \"configVersion\" : 1, \"self\" : true, \"lastHeartbeatMessage\" : \"\" }, { \"_id\" : 1, \"name\" : \"10.7.171.239:28018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 106, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T07:56:10.130Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T07:56:08.582Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"10.7.171.239:28019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 106, \"optime\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614066965, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T07:56:05Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T07:56:10.130Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T07:56:08.581Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1614066965, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614066965, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:3:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"特殊从节点 (arbiter) arbiter 节点，翻译过来就是仲裁节点的意义，arbiter 主要负责选主过程中的投票，但是不存储任何数据，也不提供任何服务。 当我们想搭建一主一从的 MongoDB 复制集时就需要配置 arbiter 节点了。 搭建过程和搭建普通复制集基本是一样的，就是初始化配置多加一个配置。 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"arbiter 复制集 $ mongo --port 28017 admin ## 定义初始化信息 \u003e config = {_id: 'my_repl', members: [ {_id: 0, host: '10.7.171.239:28017'}, {_id: 1, host: '10.7.171.239:28018'}, {_id: 2, host: '10.7.171.239:28019', \"arbiterOnly\": true }] } ## 初始化复制集 \u003e rs.initiate(config) ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"将普通复制集更改为含有 arbiter 节点复制集 要想将普通节点改为 arbiter 节点，需要先移除，再添加为 arbiter 节点。 此例我们将 10.7.171.239:28019 节点更改为 arbiter 节点 ## 移除 10.7.171.239:28019 节点 my_repl:PRIMARY\u003e rs.remove('10.7.171.239:28019') { \"ok\" : 1, \"operationTime\" : Timestamp(1614069229, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069229, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } ## 添加为 arbiter 节点 my_repl:PRIMARY\u003e rs.addArb('10.7.171.239:28019') { \"ok\" : 1, \"operationTime\" : Timestamp(1614069241, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069241, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 复制集中的节点被移除时服务会自动停止，需要手动开启服务 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查看 arbiter 复制集 my_repl:PRIMARY\u003e rs.status() { \"set\" : \"my_repl\", \"date\" : ISODate(\"2021-02-23T08:38:56.291Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) } }, \"members\" : [ { \"_id\" : 0, \"name\" : \"10.7.171.239:28017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 2987, \"optime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"electionTime\" : Timestamp(1614066874, 1), \"electionDate\" : ISODate(\"2021-02-23T07:54:34Z\"), \"configVersion\" : 3, \"self\" : true, \"lastHeartbeatMessage\" : \"\" }, { \"_id\" : 1, \"name\" : \"10.7.171.239:28018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 2672, \"optime\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1614069535, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"optimeDurableDate\" : ISODate(\"2021-02-23T08:38:55Z\"), \"lastHeartbeat\" : ISODate(\"2021-02-23T08:38:56.010Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T08:38:56.015Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"10.7.171.239:28017\", \"syncSourceHost\" : \"10.7.171.239:28017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 3 }, { \"_id\" : 2, \"name\" : \"10.7.171.239:28019\", \"health\" : 1, \"state\" : 7, \"stateStr\" : \"ARBITER\", \"uptime\" : 174, \"lastHeartbeat\" : ISODate(\"2021-02-23T08:38:56.047Z\"), \"lastHeartbeatRecv\" : ISODate(\"2021-02-23T08:38:55.204Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"configVersion\" : 3 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1614069535, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1614069535, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 注意 “10.7.171.239:28019” 节点 “stateStr” 字段的值，此为 “ARBITER”， 说明 arbiter 节点配置成功 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:4:3","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"复制集管理操作 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:0","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"查看复制集状态信息 \u003e rs.status(); //查看整体复制集状态 \u003e rs.isMaster(); // 查看当前是否是主节点 \u003e rs.conf()； //查看复制集配置信息 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:1","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"添加删除节点 \u003e rs.remove(\"ip:port\"); // 删除一个节点 \u003e rs.add(\"ip:port\"); // 新增从节点 \u003e rs.addArb(\"ip:port\"); // 新增仲裁节点 注意: 以下操作需要在主节点上进行 ","date":"2021-02-08","objectID":"/posts/mongodb-replication/:5:2","tags":["mongodb","mongodb-replication"],"title":"搭建 MongoDB 复制集(Replication Set)","uri":"/posts/mongodb-replication/"},{"categories":["mongodb"],"content":"什么是验证库？ 验证库是建立用户时 use 到的库，在使用用户时，要加上验证库才能登陆。 对于管理员用户, 必须在 admin 下创建(先 use admin，再创建管理员用户)。 需要注意点 建用户时, use 到的库,就是此用户的验证库 登录时,必须明确指定验证库才能登录 通常,管理员用的验证库是 admin, 普通用户的验证库一般是所管理的库设置为验证库 如果直接登录到数据库,不进行use, 默认的验证库是 test, 不是我们生产建议的. 从 3.6 版本开始，不添加 bindIp 参数，默认不让远程登录，只能本地管理员登录。 ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:1:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"创建用户并赋于权限 创建管理员用户 \u003e use admin \u003e db.createUser( { user: \"root\", pwd: \"root123\", roles: [ { role: \"root\", db: \"admin\" } ] }) 基本语法说明 user: 用户名 pwd: 用户密码 roles: role: 角色名，常用角色名(root, readWrite,read) db: 作用的库对象 查看所有 roles 指令 use admin show roles ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:2:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"启用 mongodb 用户验证 在 /etc/mongod.conf 配置文件中加入以下配置以启用用户验证功能, 然后重启 MongoDB 服务 security:authorization:enabled 测试连接 mongo -u root -p root123 127.0.0.1/admin 查看用户信息 \u003e use admin # 先 use 到验证库 switched to db admin \u003e db.system.users.find().pretty() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:3:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"删除用户 ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:0","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"创建测试用户 \u003e use test switched to db test \u003e db.createUser({user: \"test\",pwd: \"test123\",roles: [ { role: \"readWrite\" , db: \"test\" }]}) Successfully added user: { \"user\" : \"test\", \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:1","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"删除用户 查看所有用户 \u003e use admin switched to db admin \u003e db.system.users.find().pretty() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } { \"_id\" : \"test.test\", \"userId\" : UUID(\"505db884-397e-4eee-a050-2dab9a6dc500\"), \"user\" : \"test\", \"db\" : \"test\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"P0mVJ7NqhnXkzzXyWEfQFw==\", \"storedKey\" : \"JOjf0Xya+cOKTKuGCki7J7f7GNI=\", \"serverKey\" : \"TQLjC7FQabHjrZOtWuxIFv8kfZg=\" } }, \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } 删除用户 删除用户时需要先 use 到此用户的验证库，再执行删除命令 # 切换到 test 用户的验证库 test 库，删除 test 用户 \u003e use test; switched to db test \u003e db.dropUser('test') true # 切换到 admin 库查看所有用户 \u003e use admin; switched to db admin \u003e db.system.users.find() { \"_id\" : \"admin.root\", \"userId\" : UUID(\"6bf7b26e-e41b-46a3-8d28-fb6b793ba1b7\"), \"user\" : \"root\", \"db\" : \"admin\", \"credentials\" : { \"SCRAM-SHA-1\" : { \"iterationCount\" : 10000, \"salt\" : \"aViob5trN+4saa+6/5Uiow==\", \"storedKey\" : \"6tAnFjGMtn5hamEbrioIS3eTydY=\", \"serverKey\" : \"iORLUz6Ay2alLzz6Z7YevOJzdIs=\" } }, \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } ","date":"2021-02-05","objectID":"/posts/mongodb-user-auth/:4:2","tags":["mongodb"],"title":"MongoDB 用户和权限管理","uri":"/posts/mongodb-user-auth/"},{"categories":["mongodb"],"content":"下载 MongoDB 3.6 MongoDB 社区版下载地址 [root@localhost src]# wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.6.20.tgz ","date":"2021-01-21","objectID":"/posts/mongodb-install/:1:0","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"配置 MongoDB ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:0","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"解压]安装 MongoDB [root@localhost src]# tar xzf mongodb-linux-x86_64-rhel62-3.6.20.tgz -C /usr/local/ [root@localhost src]# cd /usr/local/ [root@localhost local]# ln -s /usr/local/mongodb-linux-x86_64-rhel62-3.6.20 /usr/local/mongodb # 加入环境变量 [root@localhost mongodb]# echo 'export PATH=/usr/local/mongodb/bin:$PATH' \u003e /etc/profile.d/mongo.sh [root@localhost mongodb]# source /etc/profile ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:1","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"创建配置文件 [root@localhost mongodb]# cat \u003e /etc/mongod.conf \u003c\u003cEOF # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /data/mongo journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo # network interfaces net: port: 27017 bindIp: 127.0.0.1 # Listen to local interface only, comment to listen on all interfaces. #security: # authorization: enabled # 是否打开用户名和密码验证 #operationProfiling: #replication: #sharding: EOF # 创建数据目录并授权 [root@localhost mongodb]# useradd -r -s /bin/false mongod [root@localhost mongodb]# mkdir -p /data/mongo [root@localhost mongodb]# chown mongod:mongod /data/mongo ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:2","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"使用 systemd 管理 MongoDB 服务 [root@localhost mongodb]# cat \u003e /usr/lib/systemd/system/mongod.service \u003c\u003cEOF [Unit] Description=MongoDB Database Server Documentation=https://docs.mongodb.org/manual After=network.target [Service] User=mongod Group=mongod Environment=\"OPTIONS=-f /etc/mongod.conf\" ExecStart=/usr/local/mongodb/bin/mongod $OPTIONS ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb PermissionsStartOnly=true PIDFile=/var/run/mongodb/mongod.pid Type=forking # file size LimitFSIZE=infinity # cpu time LimitCPU=infinity # virtual memory size LimitAS=infinity # open files LimitNOFILE=64000 # processes/threads LimitNPROC=64000 # locked memory LimitMEMLOCK=infinity # total threads (user+kernel) TasksMax=infinity TasksAccounting=false # Recommended limits for for mongod as specified in # http://docs.mongodb.org/manual/reference/ulimit/#recommended-settings [Install] WantedBy=multi-user.target EOF [root@localhost mongodb]# systemctl enable mongod.service ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:3","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mongodb"],"content":"启动 MongoDB 服务 [root@localhost ~]# systemctl start mongod.service ","date":"2021-01-21","objectID":"/posts/mongodb-install/:2:4","tags":["mongodb"],"title":"安装 MongoDB 3.6","uri":"/posts/mongodb-install/"},{"categories":["mysql"],"content":" 参考资料: DBAplus 社区 ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:0:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"搭建主从集群 参考 MySQL GTID 主从复制配置 ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:1:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"安装 MaxScale MaxScale Github 地址 MaxScale 下载地址 yum install https://downloads.mariadb.com/MaxScale/2.5.6/centos/7/x86_64/maxscale-2.5.6-1.rhel.7.x86_64.rpm ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:2:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"配置 MaxScale 在主库创建监控用户，路由用户 # 监控账号 create user scalemon@'%' identified by \"123456\"; grant replication slave, replication client on *.* to scalemon@'%'; # 路由用户 create user maxscale@'%' identified by \"123456\"; grant select on mysql.* to maxscale@'%'; grant show databases on *.* to maxscale@'%'; 从库会自动同步账号 开始配置 由于我们只使用 Read-Write-Service，不需要 Read-Only-Service，将其注释即可。 Read-Only-Listener 也需要同时注释 [root@db-proxy ~]# cat /etc/maxscale.cnf # MaxScale documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24/ # Global parameters # # Complete list of configuration options: # https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-maxscale-configuration-guide/ [maxscale] threads=auto log_info=1 logdir=/tmp/ admin_host=0.0.0.0 admin_secure_gui=false # Server definitions # # Set the address of the server to the network # address of a MariaDB server. # [server1] type=server address=10.10.1.11 port=3306 protocol=MariaDBBackend [server2] type=server address=10.10.1.12 port=3306 protocol=MariaDBBackend [server3] type=server address=10.10.1.13 port=3306 protocol=MariaDBBackend # Monitor for the servers # # This will keep MaxScale aware of the state of the servers. # MariaDB Monitor documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-monitor/ [MariaDB-Monitor] type=monitor module=mariadbmon servers=server1,server2,server3 user=scalemon password=123456 monitor_interval=2000 # Service definitions # # Service Definition for a read-only service and # a read/write splitting service. # # ReadConnRoute documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-readconnroute/ #[Read-Only-Service] #type=service #router=readconnroute #servers=server1,server2,server3 #user=maxscale #password=123456 #router_options=slave # ReadWriteSplit documentation: # https://mariadb.com/kb/en/mariadb-maxscale-24-readwritesplit/ [Read-Write-Service] type=service router=readwritesplit servers=server1,server2,server3 user=maxscale password=123456 # Listener definitions for the services # # These listeners represent the ports the # services will listen on. # #[Read-Only-Listener] #type=listener #service=Read-Only-Service #protocol=MariaDBClient #port=4008 [Read-Write-Listener] type=listener service=Read-Write-Service protocol=MariaDBClient port=4006 启动检查状态 [root@db-proxy ~]# systemctl start maxscale.service [root@MHA_Maxscale ~]# netstat -anptl | grep maxscale [root@db-proxy ~]# ss -anptl | grep maxscale LISTEN 0 128 *:8989 *:* users:((\"maxscale\",pid=1498,fd=23)) LISTEN 0 128 :::4006 :::* users:((\"maxscale\",pid=1498,fd=28)) 4006: 是 MaxScale 实现 MySQL 读写分离时连接使用的端口 8989: 是 MaxScale web 管理页面端口 使用 maxctrl 命令查看数据库连接状态 [root@db-proxy ~]# maxctrl list services ┌────────────────────┬────────────────┬─────────────┬───────────────────┬───────────────────────────┐ │ Service │ Router │ Connections │ Total Connections │ Servers │ ├────────────────────┼────────────────┼─────────────┼───────────────────┼───────────────────────────┤ │ Read-Write-Service │ readwritesplit │ 0 │ 0 │ server1, server3, server2 │ └────────────────────┴────────────────┴─────────────┴───────────────────┴───────────────────────────┘ [root@db-proxy ~]# maxctrl list servers ┌─────────┬────────────┬──────┬─────────────┬─────────────────┬──────┐ │ Server │ Address │ Port │ Connections │ State │ GTID │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server2 │ 10.10.1.12 │ 3306 │ 0 │ Slave, Running │ │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server1 │ 10.10.1.11 │ 3306 │ 0 │ Master, Running │ │ ├─────────┼────────────┼──────┼─────────────┼─────────────────┼──────┤ │ server3 │ 10.10.1.13 │ 3306 │ 0 │ Slave, Running │ │ └─────────┴────────────┴──────┴─────────────┴─────────────────┴──────┘ 也可以登录 Web 页面查看，地址: http://maxscale_server_ip:8989, 默认的用户名和密码是 admin/mariadb ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:3:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"测试读写分离 使用 mysql 命令连接 maxscale 4006 端口进行测试，应用端也是使用此地址和端口进行连接数据库 [root@db-proxy ~]# mysql -h 10.10.1.10 -P 4006 -u lwg -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: 5.7.28-log MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [(none)]\u003e select @@hostname; # 默认读操作会发送至从库，重复多次执行可以看到两台从库轮询的效果 +------------+ | @@hostname | +------------+ | db2 | +------------+ 1 row in set (0.01 sec) MySQL [(none)]\u003e begin; select @@hostname; rollback; # 使用开启事务方式，模拟写操作，可以看到写操作被发送到主库 Query OK, 0 rows affected (0.01 sec) +------------+ | @@hostname | +------------+ | db1 | +------------+ 1 row in set (0.00 sec) Query OK, 0 rows affected (0.00 sec) ","date":"2021-01-14","objectID":"/posts/mysql-maxscale-readwrite-separation/:4:0","tags":["maxscale"],"title":"MaxScale：实现MySQL读写分离与负载均衡的中间件利器","uri":"/posts/mysql-maxscale-readwrite-separation/"},{"categories":["mysql"],"content":"MySQL MHA 架构介绍 官方文档: https://github.com/yoshinorim/mha4mysql-manager/wiki MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本 DeNA 公司 youshimaton（现就职于Facebook公司）开发，是一套优秀的作为 MySQL 高可用性环境下故障切换和主从提升的高可用软件。 在MySQL故障切换过程中，MHA能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 该软件由两部分组成：MHA Manager（管理节点）和MHA Node（数据节点）。 MHA Manager 可以单独部署在一台独立的机器上管理多个 master-slave 集群，也可以部署在一台 slave 节点上。 MHA Node 运行在每台 MySQL 服务器上，MHA Manager 会定时探测集群中的 master 节点，当 master 出现故障时，它可以自动将最新数据的 slave 提升为新的 master，然后将所有其他的 slave 重新指向新的 master。整个故障转移过程对应用程序完全透明（配合 vip）。 在 MHA 自动故障切换过程中，MHA 试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过 ssh 访问，MHA 没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用 binlog-server 可以最大程度减少日志的缺失，大大降低数据丢失的风险。MHA 可以 binlog-server 结合起来。如果只有一个 slave已经收到了最新的二进制日志，MHA 可以将最新的二进制日志应用于其他所有的 slave 服务器上，因此可以保证所有节点的数据一致性。 目前 MHA 主要支持一主多从的架构，要搭建 MHA,要求一个复制集群中必须最少有三台数据库服务器，一主二从，即一台充当 master，一台充当备用 master，另外一台充当从库，因为至少需要三台服务器，出于机器成本的考虑，淘宝也在该基础上进行了改造，目前淘宝TMHA已经支持一主一从。 注: MHA 是一次性高可用，Failover 后, Manager 会自动退出 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:1:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"MHA 工作原理 可以将 MHA 工作原理总结如下 从宕机崩溃的 master 保存二进制日志事件（binlog events）; 识别含有最新更新的 slave； 应用差异的中继日志（relay log）到其他的 slave； 应用从 master 保存的二进制日志事件（binlog events）； 提升一个 slave 为新的 master； 使其他的 slave 连接新的 master 进行复制； ","date":"2021-01-14","objectID":"/posts/mysql-mha/:2:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"MHA 软件说明 MHA 软件由两部分组成，Manager 工具包和 Node 工具包，具体的说明如下。 Manager 工具包主要包括以下几个工具： masterha_check_ssh: 检查MHA的SSH配置状况 masterha_check_repl: 检查MySQL复制状况 masterha_manger: 启动MHA masterha_check_status: 检测当前MHA运行状态 masterha_master_monitor: 检测master是否宕机 masterha_master_switch: 控制故障转移（自动或者手动） masterha_conf_host: 添加或删除配置的server信息 Node工具包（这些工具通常由MHA Manager的脚本触发，无需人为操作）主要包括以下几个工具： save_binary_logs: 保存和复制master的二进制日志 apply_diff_relay_logs: 识别差异的中继日志事件并将其差异的事件应用于其他的 slave filter_mysqlbinlog: 去除不必要的 ROLLBACK 事件（MHA 已不再使用这个工具） purge_relay_logs: 清除中继日志（不会阻塞 SQL 线程） ","date":"2021-01-14","objectID":"/posts/mysql-mha/:3:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"环境准备 环境说明: 服务器数量: 3台，一主两从（使用 GTID 模式搭建主从环境，搭建过程略） 操作系统: Ubuntu 18.04 server, MySQL 版本: 5.7.28 注意: 如果需要使用 MHA 的 VIP 功能，三台机的网卡名必须一致 Master: ip: 10.10.1.2/24 vip: 10.10.1.10/24 (应用连接主库使用的 ip 地址) server_id: 2 mha_role: node Slave1: ip: 10.10.1.3/24 server_id: 3 mha_role: node Slave2: ip: 10.10.1.4/24 server_id: 4 mha_role: node, manager 创建 mha 管理 mysql 用户， 在主库执行 create user mha@'10.10.1.%' identified by 'YMhHZawmFAFBEf6T'; grant all privileges on *.* to 'mha'@'10.10.1.%'; 配置 mysql, mysqlbinlog 软链接 ln -s /usr/local/mysql/bin/mysql /usr/bin/ ln -s /usr/local/mysql/bin/mysqlbinlog /usr/bin/ ","date":"2021-01-14","objectID":"/posts/mysql-mha/:4:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 SSH 互信 MHA Manager 在内部通过 SSH 连接到 MySQL 服务器。最新从站上的 MHA 节点还通过 SSH（scp）在内部将中继日志文件发送到其他从站。为了使这些过程自动化，通常建议在不使用口令的情况下启用SSH公钥身份验证。您可以使用 MHA Manager 中包含的 masterha_check_ssh 命令来检查SSH连接是否正常工作。 slave2 机器上操作 ssh-keygen -t rsa ...（略） ssh-copy -i /root/.ssh/id_rsa.pub 10.10.1.4 rsync -arvP /root/.ssh/ 10.10.1.2:/root./ssh rsync -arvP /root/.ssh/ 10.10.1.3:/root./ssh ","date":"2021-01-14","objectID":"/posts/mysql-mha/:4:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 MHA MHA 下载地址: mha4mysql-node: https://github.com/yoshinorim/mha4mysql-node/releases mha4mysql-manager: https://github.com/yoshinorim/mha4mysql-manager/releases ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 mha4mysql-node 在三台机器执行安装 dpkg -i mha4mysql-node_0.58-0_all.deb apt install -f 修复 mha4mysql-node bug mha4mysql-node-0.58 版本中 /usr/share/perl5/MHA/NodeUtil.pm 文件在执行 masterha_check_repl 命令时会提示错误，修复方法: 直接从 mha4mysql-node 存储库下载最新的 NodeUtil.pm 覆盖即可 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"安装 mha4mysql-manager 在 slave2 机器上安装 mha4mysql-manager dpkg -i mha4mysql-manager_0.58-0_all.deb ","date":"2021-01-14","objectID":"/posts/mysql-mha/:5:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 MHA ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"生成 MHA 配置文件 # 创建配置目录 mkdir /etc/mha # 配置 mha 配置文件 cat \u003e /etc/mha/app1.conf \u003c\u003cEOF [server default] # mha 的工作目录 manager_workdir=/var/log/masterha # mha-manager 的日志文件 manager_log=/var/log/masterha/app1.log # 主库的 BINLOG 日志存储路径 master_binlog_dir=/data/mysql/3306 # MHA管理器 ping 主库主机的频率 ping_interval=2 # mha 管理 mysql 的用户名和密码 user=mha password=123456 # mysql replication username and password repl_user=repl repl_password=123456 # ssh 连接其他服务器的用户名 ssh_user=root # 主库故障切换时 VIP 漂移脚本文件，需要自定义 master_ip_failover_script=/usr/local/bin/master_ip_failover # 故障切换时发送邮箱提示, 自定义脚本(可以调用通讯工具的 api 发送消息, 例如: 微信) report_script=/usr/local/bin/alarm.sh [server1] hostname=10.10.1.2 port=3306 [server2] hostname=10.10.1.3 port=3306 # 配置为备选主，但是如果日志量落后 master 太多话也可能不会选为新主 # 此时需要配合 check_repl_delay = 0 参数 candidate_master=1 # 不检查日志落后量 check_repl_delay=0 [server3] hostname=10.10.1.4 port=3306 [binlog1] # 不参与选主 no_master=1 hostname=10.10.1.4 # 注意此参数必须与 [server default] 下配置值不同 master_binlog_dir=/data/mysql/binlog EOF binlogserver 配置：找一台额外的机器，必须要有 MySQL 5.6 以上的版本，支持 gtid 并开启 注意: mha 配置文件名是可以自己随意指定，建议和业务有关。mha 可以管理多套主从高可用 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 vip vip 配置项 [server default] master_ip_failover_script=/usr/local/bin/master_ip_failover 注意: 需要先在主库手动配置上 vip 地址，本例是: 10.10.1.10/24 vip 切换脚本 注意: 使用 mha vip 功能需要保证所有机器的网卡名是一致的 脚本内容修改说明: 根据实际情况修改脚本 vip 变量: $vip, $ssh_start_vip, $ssh_stop_vip #!/usr/bin/env perl # Copyright (C) 2011 DeNA Co.,Ltd. # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 2 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., # 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA ## Note: This is a sample script and is not complete. Modify the script based on your environment. use strict; use warnings FATAL =\u003e 'all'; use Getopt::Long; use MHA::DBHelper; my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port, $new_master_user, $new_master_password ); # vip 变量配置处 my $vip = '10.10.1.10/24'; my $ssh_start_vip = \"/sbin/ip addr add $vip dev ens33\"; my $ssh_stop_vip = \"/sbin/ip addr del $vip dev ens33\"; GetOptions( 'command=s' =\u003e \\$command, 'ssh_user=s' =\u003e \\$ssh_user, 'orig_master_host=s' =\u003e \\$orig_master_host, 'orig_master_ip=s' =\u003e \\$orig_master_ip, 'orig_master_port=i' =\u003e \\$orig_master_port, 'new_master_host=s' =\u003e \\$new_master_host, 'new_master_ip=s' =\u003e \\$new_master_ip, 'new_master_port=i' =\u003e \\$new_master_port, 'new_master_user=s' =\u003e \\$new_master_user, 'new_master_password=s' =\u003e \\$new_master_password, ); exit \u0026main(); sub main { if ( $command eq \"stop\" || $command eq \"stopssh\" ) { # $orig_master_host, $orig_master_ip, $orig_master_port are passed. # If you manage master ip address at global catalog database, # invalidate orig_master_ip here. my $exit_code = 1; eval { # updating global catalog, etc $exit_code = 0; }; if ($@) { warn \"Got Error: $@\\n\"; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"start\" ) { # all arguments are passed. # If you manage master ip address at global catalog database, # activate new_master_ip here. # You can also grant write access (create user, set read_only=0, etc) here. my $exit_code = 10; eval { print \"Enabling the VIP - $vip on the new master - $new_master_host \\n\"; \u0026start_vip(); \u0026stop_vip(); $exit_code = 0; }; if ($@) { warn $@; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"status\" ) { print \"Checking the Status of the script.. OK \\n\"; `ssh $ssh_user\\@$orig_master_host \\\" $ssh_start_vip \\\"`; exit 0; } else { \u0026usage(); exit 1; } } sub start_vip() { `ssh $ssh_user\\@$new_master_host \\\" $ssh_start_vip \\\"`; } # A simple system call that disable the VIP on the old_master sub stop_vip() { `ssh $ssh_user\\@$orig_master_host \\\" $ssh_stop_vip \\\"`; } sub usage { print \"Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n\"; } ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置故障切换告警 故障切换告警脚本需自行开发，可以调用通讯应用的 api 接口发送信息，例如: 微信，叮叮等 [server default] report_script=/usr/local/bin/alarm.sh ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:3","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"配置 binlogserver mha 的 binlogserver 配置项 [binlog1] no_master=1 hostname=10.10.1.4 # 注意此参数须与 [server default] 下配置值不同 master_binlog_dir=/data/mysql/binlog 启动 binlogserver 服务 # 必须进入到自己创建好的目录 cd /data/mysql/binlog mysqlbinlog -R --host=10.10.1.10 --user=mha --password=mha --raw --stop-never mysql-bin.000001 \u0026 --host=10.10.1.10: 从主库拉取二进制日志 mysql-bin.000001: 拉取二进制日志的起始日志文件名，可以在从库 show slave status\\G 查看获取 注意： 拉取日志的起点, 需要按照目前从库的已经获取到的二进制日志点为起点 binlogserver 服务可以使用 supervisor 程序管理 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:4","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"环境检测 检查 ssh 连接 root@db3:~# masterha_check_ssh --conf=/etc/mha/app1.conf Tue Dec 29 20:09:18 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:18 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Starting SSH connection tests.. Tue Dec 29 20:09:23 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:22 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:18 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [info] All SSH connection tests passed successfully. 检查 mysql 连接 root@db3:~# masterha_check_ssh --conf=/etc/mha/app1.conf Tue Dec 29 20:09:18 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:18 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:18 2020 - [info] Starting SSH connection tests.. Tue Dec 29 20:09:23 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.4(10.10.1.4:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:19 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.2(10.10.1.2:22).. Tue Dec 29 20:09:22 2020 - [debug] ok. Tue Dec 29 20:09:22 2020 - [debug] Connecting via SSH from root@10.10.1.3(10.10.1.3:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [debug] Tue Dec 29 20:09:18 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.3(10.10.1.3:22).. Tue Dec 29 20:09:20 2020 - [debug] ok. Tue Dec 29 20:09:20 2020 - [debug] Connecting via SSH from root@10.10.1.2(10.10.1.2:22) to root@10.10.1.4(10.10.1.4:22).. Tue Dec 29 20:09:23 2020 - [debug] ok. Tue Dec 29 20:09:24 2020 - [info] All SSH connection tests passed successfully. root@db3:~# masterha_check_repl --conf=/etc/mha/app1.conf Tue Dec 29 20:09:59 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Tue Dec 29 20:09:59 2020 - [info] Reading application default configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:59 2020 - [info] Reading server configuration from /etc/mha/app1.conf.. Tue Dec 29 20:09:59 2020 - [info] MHA::MasterMonitor version 0.58. Tue Dec 29 20:10:01 2020 - [info] GTID failover mode = 1 Tue Dec 29 20:10:01 2020 - [info] Dead Servers: Tue Dec 29 20:10:01 2020 - [info] Alive Servers: Tue Dec 29 20:10:01 2020 - [info] 10.10.1.2(10.10.1.2:3306) Tue Dec 29 20:10:01 2020 - [info] 10.10.1.3(10.10.1.3:3306) Tue Dec 29 20:10:01 2020 - [info] 10.10.1.4(10.10.1.4:3306) Tue Dec 29 20:10:01 2020 - [info] Alive Slaves: Tue Dec 29 20:1","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:5","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"启动 MHA 由于 masterha_manager 需要手动将程序放入后台运行，这里使用 supervisor 作为进程管理工具 # 安装 supervisor root@db3:~# apt install supervisor # 启动 supervisor root@db3:~# systemctl start supervisor.service # 编写 supervisor mha 配置文件 root@db3:~# cd /etc/supervisor/conf.d/ root@db3:/etc/supervisor/conf.d# cat \u003e mha.conf \u003c\u003cEOF [program:mha] command=/usr/bin/masterha_manager --conf=/etc/mha/app1.conf --remove_dead_master_conf --ignore_last_failover process_name=%(program_name)s numprocs=1 directory=/var/log/masterha umask=022 autostart=true startsecs=1 startretries=3 autorestart=unexpected exitcodes=0,2 stopsignal=QUIT stopwaitsecs=10 stopasgroup=false killasgroup=false user=root redirect_stderr=true stdout_logfile=/var/log/masterha/app1.log stderr_logfile=/var/log/masterha/app1.log EOF # 更新 supervisor 配置 root@db3:/etc/supervisor/conf.d# supervisorctl update # 查看 supervisor 管理进程信息 root@db3:/etc/supervisor/conf.d# supervisorctl status ","date":"2021-01-14","objectID":"/posts/mysql-mha/:6:6","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"故障测试 ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"测试故障 停止主库进程，查看 masterha_manager 日志信息，检查主从复制状态, 请自行测试！ ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:1","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"恢复过程 主库宕机后，binlogserver 自动停掉，masterha_manager 也会自动停止 处理思路: 检查主从复制状态 检查 masterha_manager 配置文件，查看前主库信息是否已经删除 (–remove_dead_master_conf 选项会自动删除故障主库配置信息)，如果存在故障切换可能失败。 查看 masterha_manager 日志文件 清理 binlogserver 二进制日志，重新获取新主库的 binlog 到 binlogserver 中 (使用了 vip 连接不需更改，启动服务即可) 修复故障库，手工加入主从。 重新配置 masterha_manager 配置文件 最后再启动 MHA ","date":"2021-01-14","objectID":"/posts/mysql-mha/:7:2","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"最后 虽然 mha 高可用解决了主库故障问题，但真实使用的只有一台主库别两台从库处于空闲状态，资源得不到有效的利用。 此时为了更好地利用资源，提升效率，我们可以在 mha 高可用的基础上加入读写分离架构进行优化提升效率。 参考资源: https://www.jianshu.com/p/0f7b5a962ba7 MHA 官方文档: https://github.com/yoshinorim/mha4mysql-manager/wiki ","date":"2021-01-14","objectID":"/posts/mysql-mha/:8:0","tags":["mysqlmha","mha"],"title":"MySQL MHA 高可用配置","uri":"/posts/mysql-mha/"},{"categories":["mysql"],"content":"优化参数 [mysqld] # 从库配置优化 master_info_repository = TABLE relay_log_info_repository = TABLE relay_log_recovery = 1 relay-log-purge = 1 read_only = 1 super_read_only = 1 master.info: 存储连接主库的信息，已经接收的 binlog 位置点信息 (默认在从库数据目录中) 配置项: master_info_repository = FILE/TABLE master_info_repository 默认值为 FILE 存储文件名为 master.info，值为改为 TABLE 时 master.info 信息将存储在表中，可以提高性能 relay-log.info: 记录从库回放 relay-log 位置点 (默认在从库数据目录中) 配置项: relay_log_info_repository = FILE/TABLE relay_log_info_repository 默认值为 FILE 存储文件名为 relay-log.info，值为改为 TABLE 时 relay-log.info 信息将存储在表中，可以提高性能 relay_log_purge: 自动清理 relay-log 文件 read_only: 禁止写操作，从库配置可以防止误写操作 super_read_only: 禁止管理员写操作，从库配置可以防止误写操作 ","date":"2021-01-13","objectID":"/posts/mysql-replication-optimization/:1:0","tags":["mysql-replication"],"title":"MySQL 主从复制优化","uri":"/posts/mysql-replication-optimization/"},{"categories":["mysql"],"content":"延时从库 应用场景：普通主从正常情况可以应对物理损坏，但无法应用逻辑损坏。例如: drop 和 delete 等操作。 延时从库可以应对这种逻辑损坏场景： 主库做了某项操作后，等多少秒后从库再应用。 注: 延时从库延时的是 sql 线程回放 relay 日志的时间，不是与主库传输二进制日志的时间 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"配置延时从库 主要参数: MASTER_DELAY mysql\u003e stop slave; mysql\u003e CHANGE MASTER TO MASTER_DELAY=10800; mysql\u003e start slave; ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:1","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"恢复思路 1.先停业务，挂维护页 2.停从库 SQL 线程, stop slave sql_thread; 看 relay_log 位置点; stop slave; 这里只是停止 sql 线程，io 线程并没有停，也就是说主库与从库的二进制日志传输是一直存在的。 最后停止从库时注意观察主从复制二进制日志的情况是否一至。 3.追加后续缺失的日志到从库。（相当于手工替代 sql 线程工作） 日志文件: relay-log 日志文件起始位置确认: 查看命令: show slave status\\G 也可以通过查看relay-log.info 文件 cat /data/mysql/3306/relay-log.info 日志文件终点确认: 查看命令: show relaylog events in 'db2-relay-bin.000002' 查看 relaylog 日志事件，只需要看 Pos 列； End_log_pos 列是对应主库的 binlog 位置点。 4.恢复业务，直接将业务指向从库或者将数据导回到主库 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:1:2","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"过滤复制 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"主库配置 binlog_do_db: 需要记录二进制的库 binlog_ignore_db: 不需要记录二进制的库 [mysqld] binlog_do_db=test 二选一即可 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:1","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"从库配置 如果使用 replicate-ignore-db 参数设置不同步的库，需要注意: 使用 use 语句选库后执行的操作才会被忽略不同步，如果 sql 直接通过 库名.表名 执行的操作还是会被同步的。 如果希望不管选不选库的操作都会被忽略可以使用 replicate-wild-ignore-table 配置项 库级别 replicate_do_db: 需要复制的库名 replicate_ignore_db: 忽略复制的库名 表级别 replicate_do_table: 需要复制的库中的表 replicate_ignore_table: 忽略复制的库中的表 带有模糊匹配的配置项 replicate_wild_do_table replicate_wild_ignore_table ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:2:2","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"半同步复制 经典主从复制使用的异步复制工作模型，会导致主从数据不一致的情况 MySQL 5.5 版本为了保证主从数据的一致性问题，加入半同步复制的组件(插件) 在主从复制结构中都需要启用半同步复制插件。 半同步复制主要是控制从库io是否将 relay-log 写入磁盘，一旦落盘通过插件返回 ACK 给主库的 ACK_rec， 接收到 ACK 之后，主库的事务才能提交成功。 在默认情况下,如果超过 10s 没有返回 ACK，此次复制行为会切换为异步复制。 半同步复制会影响数据库性能，也并不能完全保证主从复制的数据一致性。并不推荐使用 ","date":"2021-01-13","objectID":"/posts/mysql-slave-extend/:3:0","tags":["mysql-replication"],"title":"MySQL 从库扩展","uri":"/posts/mysql-slave-extend/"},{"categories":["mysql"],"content":"备份主库 为了节省恢复的时间我们使用 xtrabackup 备份主库，然后拷贝到从库再将数据恢复到从库中 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:1:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"完整备份主库 # 备份 xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf -S /data/mysql/mysql.sock -u root -p --backup --target-dir=/data/backup ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:1:1","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"恢复主从复制 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"恢复从库数据 # 恢复准备，应用日志 xtrabackup --prepare --target-dir=/data/backup/mysql # 恢复备份 xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf --copy-back --target-dir=/data/backup/mysql # 修改数据目录的权限 chown -R mysql:mysql /data/mysql # 启动数据库 systemctl start mysqld ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:1","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"开始恢复主从复制 由于我们使用的是 xtrabackup 工具备份恢复的数据，gtid_purged 值可以从 xtrabackup_binlog_info 文件中获取。 如果是使用是 mysqldump 命令备份加上 --master-data=1 参数就可以在备份文件中看到 SET @@GLOBAL.GTID_PURGED='d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-4'; 语句，此时恢复从库数据时就会自动执行 SET @@GLOBAL.GTID_PURGED='d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-4'; 语句，在恢复主从复制时就不需要在手动执行 set global gtid_purged=... 命令\u0008啦~ stop slave; reset master; reset slave; set global gtid_purged='cdb92087-ac64-11e9-bb08-20040ff98044:1-395071'; change master to master_host='10.10.1.11', master_user='repl', master_password='123456',master_port=3306, master_auto_position=1; start slave; 注意: 设置 gtid_purged 是为了告诉从库与主库进行主从复制时的起始 GITD， 如果不加会默认从 gtid 的1号位置点执行，此时就会出现主从复制错误 ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:2:2","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"检查，测试 请自行检测，略… ","date":"2021-01-13","objectID":"/posts/mysql-restore-gtid-replication/:3:0","tags":["mysql-replication","xtrabackup"],"title":"快速恢复 GTID 从库","uri":"/posts/mysql-restore-gtid-replication/"},{"categories":["mysql"],"content":"环境准备 准备两台服务器安装 MySQL 5.7, 参考 安装 MySQL 5.7 服务器列表 master: 10.10.1.11/24 slave1: 10.10.1.12/24 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:1:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"配置 MySQL 配置基于 GTID 的主从复制需要启动 gtid 和 binlog 功能，具体配置如下 主库: my.cnf [client] port = 3306 socket = /data/mysql/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log # gtid 配置 server_id=11 gtid_mode=on enforce-gtid-consistency = true master-info-repository = TABLE relay-log-info-repository = TABLE relay_log_recovery = on sync-master-info = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 从库: my.cnf [client] port = 3306 socket = /data/mysql/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log # gtid 配置 server_id=12 gtid_mode=on enforce-gtid-consistency = true master-info-repository = TABLE relay-log-info-repository = TABLE relay_log_recovery = on sync-master-info = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 ## 禁止从库数据写入 read_only = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:2:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"配置主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"创建主从同步用户 在主库上操作 create user 'repl'@'10.10.1.%' identified by '123456'; grant replication slave on *.* to 'repl'@'10.10.1.%'; flush privileges; ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:1","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"主从数据同步 由于当前 MySQL 环境是全新搭建的，没有任何数据，此步可以忽略。 如果是在已经运行了很久的数据库或者数据库存在数据的情况下，需要对主库进行全备然后恢复到从库，在配置启动主从复制 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:2","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"启动主从同步 在从库上执行以下命令 change master to master_host='10.10.1.11', master_user='repl', master_password='123456', master_port=3306, master_auto_position=1; start slave; GTID 主从复制只需指定 master_auto_position=1 参数即可，相比经典主从复制更简单 ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:3","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"查看主从同步状态 mysql\u003e show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.10.1.11 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mybinlog.000003 Read_Master_Log_Pos: 763 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 974 Relay_Master_Log_File: mybinlog.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 763 Relay_Log_Space: 1179 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 10 Master_UUID: d6f25a03-5d80-11eb-9fe8-000c29738b1d Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-3 Executed_Gtid_Set: d6f25a03-5d80-11eb-9fe8-000c29738b1d:1-3 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:3:4","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"主从同步测试 请自行测试, 略… ","date":"2021-01-13","objectID":"/posts/mysql-gtid-replication/:4:0","tags":["mysql-replication","mysql-gtid-replication"],"title":"MySQL GTID 主从复制配置","uri":"/posts/mysql-gtid-replication/"},{"categories":["mysql"],"content":"安装 MySQL 5.7 服务器列表 master: 10.10.1.2/24 slave1: 10.10.1.3/24 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"下载 MySQL root@db2:/usr/local/src# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz root@db1:/usr/local/src# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ root@db1:/usr/local# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"环境准备 # 安装依赖 root@db1:/usr/local/mysql# apt-get install libaio1 # 创建程序用户 root@db1:/usr/local/mysql# useradd -r -s /sbin/nologin mysql # 创建数据目录 root@db1:/usr/local/mysql# mkdir -p /data/mysql/3306 # 更改数据目录权限 root@db1:/usr/local/mysql# chown -R mysql.mysql /data/mysql/3306/ # 配置环境变量 root@db1:/usr/local/mysql# echo 'export PATH=/usr/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh root@db1:/usr/local/mysql# source /etc/profile ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"初始化数据库 准备 my.cnf 配置文件 root@db1:/usr/local/mysql# mkdir etc root@db1:/usr/local/mysql# cat \u003e etc/my.cnf \u003c\u003c EOF [client] port = 3306 socket = /data/mysql/3306/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql/3306 socket = /data/mysql/3306/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 log-error = /data/mysql/3306/error.log skip_name_resolve = 1 # 不同实例设置不同数字，不能相同 server-id = 1 # BINGLO 配置，主从同步必须启用 BINLOG 日志 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 初始化数据库 root@db1:/usr/local/mysql# mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3306 另一台数据库同样的方法安装初始化，就是配置文件 server_id 的值需要修改 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:1:3","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"配置主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"主库操作 启用主库的 binlog root@db1:/usr/local/mysql# cat etc/my.cnf [mysqld] # 不同实例设置不同数字，不能相同 server-id = 1 # BINGLO 配置，主从同步必须启用 BINLOG 日志 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 创建同步账号 mysql\u003e create user 'repl'@'10.10.1.%' identified by '123456'; mysql\u003e grant replication slave on *.* to 'repl'@'10.10.1.%'; mysql\u003e flush privileges; 导出数据用于创建 slave root@db1:~# mysqldump -uroot -p -A -R -E -B -x --master-data=2 | gzip \u003e all.sql.gz -A, –all-databases: 备份所有数据库 -E, –events: 备份事件 -B, –databases: 备份的数据库 -x, –lock-all-tables：锁定所有数据库的所有表 –master-data=2: 等于2时会将 CHANGE MASTER 命令以注释的方式加入备份文件中 将备份文件拷贝到从库还原 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"从库操作 配置 my.cnf 从库(slave)如果用于备份可以启用 binlog, 如果用于读操作可以不启用, 只配置 server-id 即可. [root@slave1 ~]# cat /usr/local/mysql/etc/my.cnf [mysqld] server-id = 2 # binlog 配置 log-bin = /data/mysql/3306/mybinlog binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 sync_binlog = 1 从 master 恢复数据 root@db2:~# gzip -d all.sql.gz root@db2:~# mysql -uroot \u003c all.sql 设置主从同步 # 从备份文件找到 CHANGE MASTER 命令 root@db2:~# more all.sql -- CHANGE MASTER TO MASTER_LOG_FILE='mybinlog.000002', MASTER_LOG_POS=763; # 配置 slave mysql\u003e CHANGE MASTER TO -\u003e MASTER_HOST='10.10.1.2', -\u003e MASTER_PORT=3306, -\u003e MASTER_USER='repl', -\u003e MASTER_PASSWORD='123456', -\u003e MASTER_LOG_FILE='mybinlog.000002', -\u003e MASTER_LOG_POS=763; mysql\u003e start slave; mysql\u003e show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.10.1.2 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mybinlog.000002 Read_Master_Log_Pos: 763 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 319 Relay_Master_Log_File: mybinlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes ... 查看从库 Slave_IO_Running 和 Slave_SQL_Running 两IO线程状态是否为 YES，为 YES 表示主从复制成功 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:2:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"测试主从同步 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:0","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"在主库查看从库 mysql\u003e SHOW SLAVE HOSTS; +-----------+------+------+-----------+--------------------------------------+ | Server_id | Host | Port | Master_id | Slave_UUID | +-----------+------+------+-----------+--------------------------------------+ | 2 | | 3306 | 1 | 2cc18b4b-4658-11eb-adf4-000c2955408a | +-----------+------+------+-----------+--------------------------------------+ 1 row in set (0.00 sec) mysql\u003e show processlist; +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ | 5 | root | localhost | NULL | Query | 0 | starting | show processlist | | 6 | repl | 10.10.1.3:57800 | NULL | Binlog Dump | 215 | Master has sent all binlog to slave; waiting for more updates | NULL | +----+------+-----------------+------+-------------+------+---------------------------------------------------------------+------------------+ 2 rows in set (0.00 sec) SHOW SLAVE HOSTS: 查看所有从库信息 show processlist: 查看当前所有线程信息， Binlog Dump 是主库和从库主从复制专用线程，如果有多个从库会有多个 Binlog Dump 线程 ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:1","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"测试主从复制 在主从库创建 test 库和 test 表，插入一些数据，然后到从库查看数据是否存在 主库 mysql\u003e create database test charset utf8mb4; Query OK, 1 row affected (0.01 sec) mysql\u003e use test; Database changed mysql\u003e create table test (id int, username varchar(60)); Query OK, 0 rows affected (0.05 sec) mysql\u003e insert into test values(1,'lisi'), (2, 'zhangshan'); Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 从库 mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql\u003e use test; mysql\u003e select * from test; +------+-----------+ | id | username | +------+-----------+ | 1 | lisi | | 2 | zhangshan | +------+-----------+ 2 rows in set (0.00 sec) ","date":"2021-01-13","objectID":"/posts/mysql-classic-replication/:3:2","tags":["mysql-replication","mysql-classic-replication"],"title":"MySQL 经典主从复制配置","uri":"/posts/mysql-classic-replication/"},{"categories":["mysql"],"content":"GTID 的概述 是对一个已提交事务的编号，并且是全局唯一的编号 全局事物标识：global transaction identifieds。 GTID事物是全局唯一性的，且一个事务对应一个GTID。 一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。 GTID 用来代替 经典 (classic) 的复制方法，不在使用 binlog + pos 开启复制。而是使用master_auto_postion=1 的方式自动匹配 GTID 断点进行复制。 MySQL-5.6.5 开始支持的，MySQL-5.6.10 后开始完善。 在传统的 slave 端，binlog 是不用开启的，但是在 GTID 中，slave 端的 binlog 是必须开启的，目的是记录执行过的GTID（强制）. ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:1:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 的组成部分 GTID 由 server_uuid 和 sequence number 组成，通过 : 连接 例如：7800a22c-95ae-11e4-983d-080027de205a:10 server_uuid: 每个mysql实例的唯一ID，由于会传递到 slave，所以也可以理解为源 ID。 sequence number：在每台MySQL服务器上都是从1开始自增长的序列，一个数值对应一个事务。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:2:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 比传统复制的优势 更简单的实现 failover，不用以前那样在需要找 log_file 和 log_Pos。 更简单的搭建主从复制。 比传统复制更加安全。 GTID是连续没有空洞的，因此主从库出现数据冲突时，可以用添加空事物的方式进行跳过。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:3:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"GTID 的工作原理 master 更新数据时，会在事务前产生 GTID，一同记录到 binlog 日志中。 slave 端的 i/o 线程将变更的 binlog，写入到本地的 relay-log 中。 sql 线程从 relay-log 中获取 GTID，然后对比 slave 端的 binlog 是否有记录。 如果有记录，说明该 GTID 的事务已经执行，slave 会忽略(幂等性)。 如果没有记录，slave 就会从 relay-log 中执行该 GTID 的事务，并记录到 binlog。 在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。 要点： slave 在接受 master 的 binlog 时，会校验 master 的 GTID 是否已经执行过（一个服务器只能执行一次）。 为了保证主从数据的一致性，多线程只能同时执行一个 GTID。 由于幂等性特点，开启 GTID 后，MySQL 恢复 binlog 时，重复的 GTID 事务不会执行. 所以在导出 binlog 日志时需要加上 --skip-gtid 参数，从而让导出 binlog 语句中不保留全局事务标识符； 而是让服务器像执行新事务一样执行事务。 ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:4:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"配置 GITD 启用 GTID 主要配置参数 root@db1:~# cat /usr/local/mysql/etc/my.cnf [mysqld] gtid-mode = on enforce-gtid-consistency = true ","date":"2021-01-12","objectID":"/posts/mysql-gtid/:5:0","tags":["mysql","mysql-gtid"],"title":"MySQL GITD 模式","uri":"/posts/mysql-gtid/"},{"categories":["mysql"],"content":"slowlog 慢日志 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:0","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"作用 记录 MySQL 运行过程运行过慢的语句，通过一个文本的文件记录下来。 帮助我们进行语句优化工作。 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:1","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"配置慢日志 root@db1:~# cat /usr/local/mysql/etc/my.cnf [mysqld] # 慢语句开关 slow_query_log = 1 # 慢日志存储路径 slow_query_log_file = /data/mysql/3306/slow.log # 定义慢语句时间阈值单位为秒 long_query_time = 1 # 记录不走索引的语句 log_queries_not_using_indexes = 1 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:1:2","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"查看慢日志 ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:0","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"mysqldumpslow 参数说明 root@db1:~# mysqldumpslow -h -s ORDER 日志排序方式, 默认排序为 'at' al: 平均锁定时间 ar: 发送的平均行数 at: 平均查询时间 c: 执行次数 l: 锁定时间 r: 发送行数 t: 查询时间 -r 反向排序 -t NUM 只显示前n个查询 查看执行次数最多的前5条慢语句 root@db1:~# mysqldumpslow -s c -t 5 /data/mysql/3306/slow.log ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:1","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"可视化展示慢日志 slow-log 工具: pt-query-digest + Amemometer ","date":"2021-01-12","objectID":"/posts/mysql-slowlog/:2:2","tags":["mysql","slowlog"],"title":"MySQL 慢日志","uri":"/posts/mysql-slowlog/"},{"categories":["mysql"],"content":"mysqlbinlog 参数说明 -d, --database 指定截取日志的库名 --start-position 截取日志起始 position 号 --stop-position 截取日志最终 position 号 --start-datetime 截取日志开始时间 --stop-datetime 截取日志结束时间 --skip-gtids 不保留全局事务标识符； 而是让服务器像执行新事务一样执行事务。 --include-gtids=name 截取日志起始和结束 GTID 号，例如: 09833d3f-4656-11eb-9892-000c2913c78e:1-4 --exclude-gtids=name 截取日志排除的 GTID 号 注: –skip-gtids 在启用 GTID 时需要加上，否则数据无法通过导出的 sql 还原, 这是由于 GTID 的特性（幂等性）导致的。 截取标志可以混用，也可以只指定起始位置点，不指定结束位置点(默认到最新的点) ","date":"2021-01-11","objectID":"/posts/mysql-mysqlbinlog/:1:0","tags":["mysqlbinlog"],"title":"MySQL mysqlbinlog 命令使用说明","uri":"/posts/mysql-mysqlbinlog/"},{"categories":["mysql"],"content":"通过 BINLOG 恢复数据 BINLOG 一般配合备份一起使用，单独使用 BINLOG 恢复数据在数据量大的情况比较困难。 ","date":"2021-01-11","objectID":"/posts/mysql-mysqlbinlog/:2:0","tags":["mysqlbinlog"],"title":"MySQL mysqlbinlog 命令使用说明","uri":"/posts/mysql-mysqlbinlog/"},{"categories":["mysql"],"content":"binlog 作用 主要记录数据库变化(DDL,DML,DCL)性质的日志 用于数据恢复：如果你的数据库出问题了，而你之前有过备份，那么可以看日志文件，找出是哪个命令导致你的数据库出问题了，想办法挽回损失。 主从服务器之间同步数据：主服务器上所有的操作都在记录日志中，从服务器可以根据该日志来进行，以确保两个同步。 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:1:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"配置 MySQL 8.0 版本以前默认都没有开启，生产环境建议开启 配置方法 [mysqld] # 主机编号，主从中使用，5.7 版本中 开启 binlog 必须设置 server_id server_id = 1 # binlog 日志名前缀，『/data/mysql/binlog』 binlog 存储目录， 『mysql-bin』 binlog 文件名前缀，便如 mysql-bin.000001 mysql-bin.000002 log_bin = /data/mysql/binlog/mysql-bin # binlog 日志写入磁盘策略，双1参数中的其中一个 sync_binlog = 1 # binlog 日志记录格式为 row binlog_format = row # binlog 保存天数 expire_logs_days = 7 注意: binlog 日志最好和数据分开存储 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:2:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"记录内容 binlog 是 SQL 层的功能。记录的是变更 SQL 语句，不记录查询语句。 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"记录 SQL 语句种类 DDL：原封不动记录当前 DDL（statement 语句方式） DML：原封不动记录当前 DDL（statement 语句方式） DCL：只记录已提交事务的 DML (insert, update, delete) ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"DML 三种语句记录方式 statement (5.6 默认) SBR (statement based replication): 原封不动的记录当前DML ROW (5.7 默认) RBR (ROW based replication): 记录数据行的变化，用户看不懂（需要工具分析） mixed (混合) MBR (mixed based replication): 以上两模式的混合 STATEMENT 模式 可读性较高，日志量小，不够严谨 ROW 模式 可读性很低，日志量大，足够严谨 建议使用 ROW 模式 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:3:2","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"事件的简介 二进制日志的最小单元，对于 DML,DCL，一条语句就是一个事件(event) 对于 DML 语句来讲，只记录已提交的事务 例如: 以下列子，分为4个事件 begin; 101-320 DML; 320-630 DML; 630-740 commit; 740-810 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:4:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"event 的组成 三部分构成 事件的开始标识 事件的内容 事件的结束标识 Position 开始标记: at 1262 结束标记: end_log_pos 1312 作用: 为了方便截取日志 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:4:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"查看二进制日志 查看一共有几个日志文件 mysql\u003e show binary logs; +-----------------+-----------+ | Log_name | File_size | +-----------------+-----------+ | mybinlog.000001 | 177 | | mybinlog.000002 | 1403 | +-----------------+-----------+ 查看当前在用的日志文件 mysql\u003e show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | mybinlog.000002 | 1403 | | | | +-----------------+----------+--------------+------------------+-------------------+ 查看二进日志事件 mysql\u003e show binlog events in 'mybinlog.000002'; +-----------------+------+----------------+-----------+-------------+-----------------------------------------------------------------------------------------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +-----------------+------+----------------+-----------+-------------+-----------------------------------------------------------------------------------------------------------------------+ | mybinlog.000002 | 4 | Format_desc | 1 | 123 | Server ver: 5.7.28-log, Binlog ver: 4 | | mybinlog.000002 | 123 | Previous_gtids | 1 | 154 | | | mybinlog.000002 | 154 | Anonymous_Gtid | 1 | 219 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' | | mybinlog.000002 | 219 | Query | 1 | 407 | CREATE USER 'repl'@'10.10.1.%' IDENTIFIED WITH 'mysql_native_password' AS '*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9' | | mybinlog.000002 | 407 | Anonymous_Gtid | 1 | 472 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS' 导出查看二进制日志 root@db1:~# mysqlbinlog /data/mysql/3306/mybinlog.000002 \u003e /tmp/2.sql # 或者， --base64-outpu=decode-rows 解码日志信息 root@db1:~# mysqlbinlog --base64-outpu=decode-rows /data/mysql/3306/mybinlog.000002 \u003e /tmp/2.sql 通过 position 号截取日志 root@db1:~# mysqlbinlog --start-position=219 --stop-position=1403 /data/mysql/3306/mybinlog.000002 \u003e /tmp/r.sql ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:5:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志管理 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:6:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志滚动 每次重启 MySQL 时 BINLOG 日志会自动滚动生成并使用新的日志文件 bin-log 文件大小达到参数 max_binlog_size 限制； 手动滚动更新 mysql\u003e flush logs; root@db1:~# mysqladmin flush-logs mysqldump 的 -F 参数也会触发自动滚动更新 BINLOG 日志文件，不建议使用 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:6:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"日志清理 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:0","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"自动清理方法1：（修改配置文件和在mysql内设置参数可无需重启服务） root@db1:~# vim /etc/my.cnf [mysqld] expire_logs_days = 7 // 表示日志保留7天，超过7天则设置为过期的, 默认为 0 永不过期 root@db1:~# mysql -u root -p mysql\u003e show binary logs; mysql\u003e show variables like '%log%'; mysql\u003e set global expire_logs_days = 7; expire_logs_days 一般设置为一个全备周期 + 1，如果全备周期为7天，就设置为 7+1=8 一般生产中至少保留2个全备周期 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:1","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"手动清理方法2： 如果没有主从复制，可以通过下面的命令重置数据库日志，清除之前所有的日志文件： mysql\u003e reset master 注: 此操作危险，请谨慎操作！！！ 但是如果存在复制关系，应当通过 PURGE 的名来清理 bin-log 日志，语法如下： # mysql -u root -p mysql\u003e purge master logs to 'mysql-bin.010’; //清除 mysql-bin.010 之前所有的日志 mysql\u003e purge master logs before '2016-02-28 13:00:00'; //清除2016-02-28 13:00:00前的日志 mysql\u003e purge master logs before date_sub(now(), interval 3 day); //清除3天前的bin日志 注意，不要轻易手动去删除 binlog，会导致 binlog.index 和真实存在的 binlog 不匹配，而导致 expire_logs_day 失效 ","date":"2021-01-10","objectID":"/posts/mysql-binlog/:7:2","tags":["mysql","binlog"],"title":"MySQL bin-log 日志","uri":"/posts/mysql-binlog/"},{"categories":["mysql"],"content":"information_schema 库 ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:0","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"统计单表占用物理空间大小 查询表: information_schema.tables 计算公式: 方法一: 单表占用空间大小 = AVG_ROW_LENGTH * TABLE_ROWS + INDEX_LENGTH 方法二: 单表占用空间大小 = DATA_LENGTH 示例: 查看 employees 库中 salaries 表的占用空间大小 mysql\u003e select table_schema,table_name, -\u003e (avg_row_length * table_rows + index_length) / 1024 / 1024 as data_mb -\u003e from tables where table_schema='employees' and table_name = 'salaries'; +--------------+------------+-------------+ | table_schema | table_name | data_mb | +--------------+------------+-------------+ | employees | salaries | 94.74268913 | +--------------+------------+-------------+ ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:1","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"查看数据库碎片占用最大的表, 前 10 名 mysql\u003e select table_schema,table_name, data_free / 1024 / 1024 as data_free_mb from tables order by data_free_mb limit 10; ","date":"2021-01-07","objectID":"/posts/mysql-information_schema/:1:2","tags":["mysql","information_schema"],"title":"统计 MySQL 数据库信息","uri":"/posts/mysql-information_schema/"},{"categories":["mysql"],"content":"MyISAM 引擎默认是支持通过拷贝文件方式迁移数据，InnoDB 引擎不支持。 如果需要迁移 InnoDB 引擎数据可以先将数据表的引擎由 InnoDB 更改为 MyISAM。 也可以通过管理 MySQL 独立表空间文件实现数据库的迁移。操作步骤如下: ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:0:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"准备测试数据 可以使用 MySQL 官方提供的测试数据进行实验演示: https://github.com/datacharmer/test_db git clone https://github.com/datacharmer/test_db.git cd test_db mysql -t \u003c employees.sql ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:1:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"导出库中所有表结构 [root@10-13-90-34 ~]# mysqldump -d -B employees \u003e employees_schema.sql ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:2:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"在目标数据库中创建与源库一样的表文件 [root@10-13-90-34 ~]# mysql -S /data/mysql/3308/mysql.sock mysql\u003e source employees_schema.sql; mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | performance_schema | | sys | +--------------------+ ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:3:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"管理表空间文件，恢复数据 删除表空间文件 删除表空间文件时可能会由于外键约束导致失败，可以先暂时关闭外键约束 SET foreign_key_checks = 0;, 操作完成后在开启 SET foreign_key_checks = 1; alter table employees.departments discard tablespace; alter table employees.dept_emp discard tablespace; alter table employees.dept_manager discard tablespace; alter table employees.employees discard tablespace; alter table employees.salaries discard tablespace; alter table employees.titles discard tablespace; 导入表空间文件 将源库中所有表的 idb 文件拷贝到目标库中并修改权限 [root@10-13-90-34 employees]# cp -p /data/mysql/3306/employees/*.ibd /data/mysql/3308/employees 导入表空间文件 alter table employees.departments import tablespace; alter table employees.dept_emp import tablespace; alter table employees.dept_manager import tablespace; alter table employees.employees import tablespace; alter table employees.salaries import tablespace; alter table employees.titles import tablespace; 开启外键约束 SET foreign_key_checks = 1; 验证数据 注意: 此方法操作有风险，不到万不得已不建议使用 ","date":"2021-01-05","objectID":"/posts/mysql-tablespace-restore/:4:0","tags":["mysql"],"title":"使用 MySQL 表空间方式(迁移/恢复)数据","uri":"/posts/mysql-tablespace-restore/"},{"categories":["mysql"],"content":"mysqldump 参数说明 -A, –all-databases: 备份所有库 -B, –databases: 使用此参数可以同时备份多个库 单库备份可以加上 -B 参数，这样备份文件中加会加入 create database ... 及 use DATABASE 语句. –master-data=2: 加入此参数可以记录 binlog 日志文件位置和文件名 (生产中建议加上此参数) 备份时会自动记录 binlog 日志信息在备份文件中，值为2时以注释的形式记录，值为1时以语句形式记录。 会自动锁全表及解锁 加 –single-transaction 参数，可以减少锁表时间 --master-data 在生产中建议使用的值为 2 –single-transaction 对于 Innodb引擎表备份时，开启一个独立事务，获取一个一致性快照进行备份。 -R: 备份存储过程，函数 -E: 备份事件 -triggers: 备份触发器 -d, –no-data: 不备份数据，只备份数据结构 -n, –no-create-db: 不生成创建数据库语句 -t, –no-create-info: 不生成创建表语句 mysqldump 可选参数 --max_allowed_packet=64M 建议使用 mysqldump 命令参数: mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:1:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"数据库备份 ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份所有库 [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E -A \u003e all.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:1","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"只备份所有库的结构 [root@localhost ~]# mysqldump -uroot -p -A -d \u003e all.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:2","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份单个数据库 [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E -B DATABASENAME \u003e DATABASENAME.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:3","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"一次备份多个数据库 (-B, –databases) [root@localhost ~]# mysqldump -uroot -p --master-data=2 --single-transaction --triggers -R -E --databases db1 db2 \u003e dbs.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:4","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"备份数据库中指定的表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME TABLENAME \u003e DATABASENAME_TABLENAME.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:5","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"一次备份数据库中指定的多张表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME t1 t2 \u003e DATABASENAME_ts.sql ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:6","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"导出函数或者存储过程 mysqldump -h HOSTNAME -u USERNAME -p PASSWORD -ntd --triggers -R -E DATABASENAME \u003e DATABASENAME.sql -ntd 表示不导出数据及创建库和表的语句； ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:2:7","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql"],"content":"数据库恢复 当我们需要还原数据时可以通过以下命令进行还原 方法1 直接还原数据库，如果备份语句中没有禁用记录 binlog 会产生大量无用的 binlog 信息增加还原时长 [root@localhost ~]# mysql -uroot -p \u003c all.sql 方法2 先连接上数据库，然后临时禁止记录 binlog 日志，再还原数据库文件，最后在开启 binlog 日志记录功能(断开重连也会恢复) [root@localhost ~]# mysql -uroot -p mysql\u003e set sql_log_bin=0; mysql\u003e source /data/bak/backup.sql; mysql\u003e set sql_log_bin=0; ","date":"2021-01-04","objectID":"/posts/mysqldump-backup/:3:0","tags":["mysql","mysqldump"],"title":"Mysqldump 备份 MySQL","uri":"/posts/mysqldump-backup/"},{"categories":["mysql","xtrabackup"],"content":"percona-xtrabackup 是物理备份工具，拷贝数据文件。 原生态支持全备和增量备份。 会记录二进制日志文件及位置。 InnoDB 表: 热备份，业务正常发生时，影响较小的备份方式 非 InnoDB 表: 温备份，会锁表 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:0:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"安装 percona-xtrabackup 下载地址: https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/ ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:1:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"xtrabackup 使用 使用 xtrabackup 命令前提条件 数据库必须启动 能连接上数据库，指定用户名，密码，socket 配置文件 my.cnf 中必须配置 datadir 参数 配置 my.cnf [client] # 配置客户端工具连接 socket 文件路径，有此参数 xtrabackup 可以省略 -S 参数 socket = /data/mysql/3306/mysql.sock [mysqld] # 配置数据目录 datadir = /data/mysql/3306 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:0","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"全量备份 root@db1:/data/bak# xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf -u root -p --backup --target-dir=/data/bak/full-$(date +%F) –defaults-file: 指定 my.cnf 配置文件，此参数必须放在第一位 -u: 数据库用户名 -p: 用户密码 –target-dir: 指定备份存储目录 如果 my.cnf 配置文件的 [client] 配置项中没有指定 socket 参数，需要指定 -S 指定 mysql.sock 文件路径 ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:1","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql","xtrabackup"],"content":"还原数据 准备数据 此步操作主要应用那些还没有应用的事务，该回滚的事务回滚，该提交的事务提交。 该步骤可以使文件在单个时间点上完全一致。 root@db1:/data/bak# xtrabackup --prepare --target-dir=/data/bak/full-2020-12-25 注: 准备数据操作过程不能被中断，否则备份将不可用。 还原数据 可以直接修改 my.cnf 的 datadir 值，将路径修改为备份路径，然后修改备份目录的权限即可 直接拷贝文件到 MySQL 数据目录中，然后修改数据目录下所有文件的权限即可 可以直接使用 cp or rsync 命令，也可以使用 xtrabackup 命令。 root@db1:/data/bak# xtrabackup --defaults-file=/usr/local/mysql/etc/my.cnf --copy-back --target-dir=/data/bak/full-2020-12-25 root@db1:/data/bak# chown -R mysql.mysql /data/mysql/3306/ 也可以使用如下命令 cp -a /data/bak/full-2020-12-25 /data/mysql/3306 rsync -avrP /data/bak/full-2020-12-25/ /data/mysql/3306/ ","date":"2021-01-04","objectID":"/posts/xtrabackup-backup-mysql/:2:2","tags":["mysql","xtrabackup"],"title":"Xtrabackup 备份 MySQL (全备)","uri":"/posts/xtrabackup-backup-mysql/"},{"categories":["mysql"],"content":"适用于 MySQL5.6 及之前的版本 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:0","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"1. 停止MySQL服务 执行： /etc/init.d/mysql stop，你的机器上不一定是 /etc/init.d/mysql 也可能是 /etc/init.d/mysqld ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:1","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"2. 跳过验证启动MySQL /usr/local/mysql/bin/mysqld_safe --skip-grant-tables \u003e/dev/null 2\u003e\u00261 \u0026 注：如果 mysqld_safe 命令所在的路径和上面不一样需要修改成你的，如果不清楚可以用find命令查找。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:2","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"3. 重置密码 等一会儿，然后执行： /usr/local/mysql/bin/mysql -u root 出现mysql提示符后输入：update mysql.user set password=password('要设置的密码') where user='root'; 回车后执行：flush privileges; 刷新 MySQL 系统权限相关的表。再执行：exit; 退出。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:3","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"4. 重启MySQL 杀死 MySQL 进程： killall mysqld 重启 MySQL： /etc/init.d/mysql start ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:1:4","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"MySQL5.7 重置 root 密码 编辑 my.cnf 文件加入以下配置 [mysqld] skip-grant-tables 重启 mysql，正常连接 mysql 使用如下命令修改 root 密码 update mysql.user set authentication_string=PASSWORD('123456') where user='root'; flush privilegs; 最后在去除 my.cnf 配置文件中的 skip-grant-tables 配置项,重启 mysql 即可。 ","date":"2021-01-03","objectID":"/posts/mysql-reset-root-password/:2:0","tags":["mysql"],"title":"重置 MySQL root 密码","uri":"/posts/mysql-reset-root-password/"},{"categories":["mysql"],"content":"一、用户权限管理 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 查看帮助信息 使用 mysql 命令连接上 MySQL 服务后可以使用 help 命令查看帮助信息，例如: mysql\u003e help For information about MySQL products and services, visit: http://www.mysql.com/ For developer information, including the MySQL Reference Manual, visit: http://dev.mysql.com/ To buy MySQL Enterprise support, training, or other products, visit: https://shop.mysql.com/ List of all MySQL commands: Note that all text commands must be first on line and end with ';' ? (\\?) Synonym for `help'. clear (\\c) Clear the current input statement. connect (\\r) Reconnect to the server. Optional arguments are db and host. delimiter (\\d) Set statement delimiter. edit (\\e) Edit command with $EDITOR. ego (\\G) Send command to mysql server, display result vertically. exit (\\q) Exit mysql. Same as quit. go (\\g) Send command to mysql server. help (\\h) Display this help. nopager (\\n) Disable pager, print to stdout. notee (\\t) Don't write into outfile. pager (\\P) Set PAGER [to_pager]. Print the query results via PAGER. print (\\p) Print current command. prompt (\\R) Change your mysql prompt. quit (\\q) Quit mysql. rehash (\\#) Rebuild completion hash. source (\\.) Execute an SQL script file. Takes a file name as an argument. status (\\s) Get status information from the server. system (\\!) Execute a system shell command. tee (\\T) Set outfile [to_outfile]. Append everything into given outfile. use (\\u) Use another database. Takes database name as argument. charset (\\C) Switch to another charset. Might be needed for processing binlog with multi-byte charsets. warnings (\\W) Show warnings after every statement. nowarning (\\w) Don't show warnings after every statement. resetconnection(\\x) Clean session context. For server side help, type 'help contents' 例如查看 select 语句的用法 可以使用 help select； 命令查看帮助信息 mysql\u003e help select; Name: 'SELECT' Description: Syntax: SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE 'file_name' [CHARACTER SET charset_name] export_options | INTO DUMPFILE 'file_name' | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] SELECT is used to retrieve rows selected from one or more tables, and can include UNION statements and subqueries. See [HELP UNION], and https://dev.mysql.com/doc/refman/5.7/en/subqueries.html. The most commonly used clauses of SELECT statements are these: o Each select_expr indicates a column that you want to retrieve. There must be at least one select_expr. o table_references indicates the table or tables from which to retrieve rows. Its syntax is described in [HELP JOIN]. o SELECT supports explicit partition selection using the PARTITION with a list of partitions or subpartitions (or both) following the name of the table in a table_reference (see [HELP JOIN]). In this case, rows are selected only from the partitions listed, and any other partitions of the table are ignored. For more information and examples, see https://dev.mysql.com/doc/refman/5.7/en/partitioning-selection.html. SELECT ... PARTITION from tables using storage engines such as MyISAM that perform table-level locks (and thus partition locks) lock only the partitions or subpartitions named by the PARTITION option. For more information, see https://dev.mysql.com/doc/refman/5.7/en/partitioning-limitations-lock ing.html. o The WHERE clause, if given, indicates the condition or conditions that rows must satisfy to be selected. where_condition is an expression that evaluates to true for each row to be selected. The statement selects all rows if there is no WHERE clause. In the WHERE expression, you can us","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 用户创建 # 创建用户（默认密码为空） mysql\u003e create user 'username'@'host'; # 创建用户并设置密码 mysql\u003e create user 'username'@'host' identified by 'password'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"3. 删除用户 mysql\u003e drop user 'username'@'host'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:3","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"4. 更改密码 # 更改密码 （只对当前登录账号有效） mysql\u003e set password=password('123456'); # 2. 更改指定用户的密码 mysql\u003e set password for 'username'@'host'=password('123456'); ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:4","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"5. 查询用户权限 # 查询当前账号的权限 mysql\u003e show grants; # 查询指定账号的权限 mysql\u003e show grants for 'user'@'host'; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:5","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"6. 用户授权 # 对用户授权（如果用户存在就增加权限，不存在就创建用户不过密码为空） mysql\u003e grant privileges on databasename.tablename to 'username'@'host'; # 对用户授权并设置密码（如果用户存在就增加权限，不存在就创建用户） # mysql 8.0 版本以后需要先创建用户在授权 mysql\u003e grant privileges on databasename.tablename -\u003e to 'username'@'host' identified by 'password'; privileges: 权限列表以逗号隔开，例如： select, insert, update 注意: 进行数据库基本信息相关更改后请使用 flush privileges; 刷新数据库信息 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:6","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"7. 用户权限回收 mysql\u003e revoke privilege on databasename.tablename from 'user'@'host'; 注：数据库名要用反撇号引起，或者不用 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:1:7","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"二、数据库 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 数据库的基本操作 # 显示数据库 mysql\u003e show databases; # 创建数据库 mysql\u003e create database DATABASENAME charset utf8mb4;; # 查看数据库创建语句 mysql\u003e show create database DATABASENAME; # 删除数据库 mysql\u003e drop database DATABASENAME; ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 备份数据库数据及表结构 # 备份整个数据库 [root@localhost ~]# mysqldump -uroot -p -A \u003e all.sql # 备份整个数据库的结构 [root@localhost ~]# mysqldump -uroot -p -A -d \u003e all.sql # 备份单个数据库 [root@localhost ~]# mysqldump -uroot -p DATABASENAME \u003e DATABASENAME.sql # 一次备份多个数据库, 同时备份 db1, db2 二个库的数据 (-B, --databases) [root@localhost ~]# mysqldump -uroot -p --databases db1 db2 \u003e dbs.sql # 备份数据库中指定的表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME TABLENAME \u003e DATABASENAME_TABLENAME.sql # 一次备份数据库中指定的多张表 [root@localhost ~]# mysqldump -uroot -p DATABASENAME t1 t2 \u003e DATABASENAME_ts.sql -B, --databases: 单库备份可以加上 -B 参数，这样备份文件中加会加入 create database ... 及 use DATABASE 语句. -A, --all-databases : 备份所有数据库 -d, --no-data ：只导出表结构 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"3. 导出函数或者存储过程 mysqldump -hHOSTNAME -uUSERNAME -pPASSWORD -ntd -R DATABASENAME \u003e DATABASENAME.sql -ntd 是表示导出存储过程； -R 是表示导出函数 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:3","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"4. 恢复数据库数据 ** 使用系统命令** [root@localhost ~]# mysql -uroot DATABASENAME \u003c DATABASENAME.sql 使用 source 命令 # 禁止记录 binlog 日志，恢复数据就没必要记录 binlog 了 mysql\u003e set sql_log_bin=0 mysql\u003e use lwg; mysql\u003e source /root/lwg.sql; 注意: 恢复数据时，如果数据库不存在需要先创建 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:2:4","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"三、数据表 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:0","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"1. 表的基本操作 # 查看数据库下所有的表 mysql\u003e show tables; # 创建表 mysql\u003e CREATE TABLE `TABLENAME` ( `id` int(10) NOT NULL PRIMARY KEY AUTO_INCREMENT, `user` varchar(30) NOT NULL, `password` varchar(30) NOT NULL ) ENGINE=MyISAM DEFAULT CHARSET=utf8; # 显示表结构 mysql\u003e desc TABLENAME; # 显示表创建语句 mysql\u003e show create table TABLENAME; # 清空表数据 mysql\u003e truncate table TABLENAME; mysql\u003e delete from TABLENAME; 不带 where 参数的 delete 语句可以删除 mysql 表中所有内容 使用 truncate table 也可以清空 mysql 表中所有内容。 效率上 truncate 比 delete 快，但 truncate 删除后不记录 mysql 日志，不可以恢复数据。 delete 的效果有点像将 mysql 表中所有记录一条一条删除到删完， 而 truncate 相当于保留 mysql 表的结构，重新创建了这个表，所有的状态都相当于新表。 所以 delete 不会重置 ID 列，而 truncat 会重置。 delete 删除是逻辑上的删除，并不会真正的释放硬盘空间，而 truncat 是物理上的删除操作会真正的释放硬盘空间 ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:1","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"2. 表 alter 的相关操作 # 增加一个字段(一列),并放到第一列的位置 (first) mysql\u003e desc users; +------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+----------+------+-----+---------+-------+ | username | char(30) | NO | PRI | NULL | | | userpasswd | char(20) | NO | | 123456 | | +------------+----------+------+-----+---------+-------+ 2 rows in set (0.00 sec) mysql\u003e alter table users add column id int not null first; Query OK, 0 rows affected (0.08 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+----------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | char(30) | NO | PRI | NULL | | | userpasswd | char(20) | NO | | 123456 | | +------------+----------+------+-----+---------+-------+ 3 rows in set (0.00 sec) # 删除一个字段 mysql\u003e alter table users drop userpasswd; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +----------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+----------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | char(30) | NO | PRI | NULL | | +----------+----------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 更改列的字段类型 mysql\u003e alter table users modify username varchar(100); Query OK, 2 rows affected (0.14 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +----------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+--------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | username | varchar(100) | NO | PRI | | | +----------+--------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 更改列名及字段类型 mysql\u003e alter table users change username user varchar(20); Query OK, 2 rows affected (0.03 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e desc users; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | user | varchar(20) | NO | PRI | | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) # 修改表的存储引擎 mysql\u003e show create table users; +-------+---------------------------------------+ | Table | Create Table | +-------+---------------------------------------+ | users | CREATE TABLE `users` ( `id` int(11) NOT NULL, `user` varchar(20) NOT NULL DEFAULT '', PRIMARY KEY (`user`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +-------+---------------------------------------+ 1 row in set (0.00 sec) mysql\u003e alter table users ENGINE=myisam; Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 # 这里我们使用另一种方法查询表的默认引擎 mysql\u003e show table status from lwg where name='users'\\G *************************** 1. row *************************** Name: users Engine: MyISAM Version: 10 Row_format: Dynamic Rows: 2 Avg_row_length: 20 Data_length: 40 Max_data_length: 281474976710655 Index_length: 2048 Data_free: 0 Auto_increment: NULL Create_time: 2017-08-25 04:15:46 Update_time: 2017-08-25 04:15:46 Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec) ","date":"2021-01-01","objectID":"/posts/mysql-manage/:3:2","tags":["mysql"],"title":"MySQL 基础管理命令","uri":"/posts/mysql-manage/"},{"categories":["mysql"],"content":"当数据库服务器资源有剩余时，为了充分利用剩余资源可以通过部署 MySQL 多实例提升资源利用率， 下面演示如何在一台机上安装 MySQL 多实例 ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:0:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"下载 MySQL 5.7 二进制包 [root@10-13-90-34 src]# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:1:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"解压并建立软链接(/usr/local/mysql) [root@10-13-90-34 src]# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ [root@10-13-90-34 local]# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:2:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"配置环境变量 echo 'export PATH=/urs/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh source /etc/profile ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:3:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"准备多实例环境 创建用户 [root@10-13-90-34 mysql]# useradd -r -s /sbin/nologin mysql 创建数据目录 [root@10-13-90-34 mysql]# mkdir -p /data/mysql/{3306,3307} [root@10-13-90-34 mysql]# chown -R mysql.mysql /data/mysql 准备多实例配置文件 实例1：3307 cat \u003e /usr/local/mysql/etc/my-3307.cnf \u003c\u003cEOF [client] port = 3307 socket = /data/mysql/3307/mysql.sock [mysqld] user = mysql port = 3307 basedir = /usr/local/mysql datadir = /data/mysql/3307 socket = /data/mysql/3307/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/3307/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/3307/mybinlog #sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 EOF 实例2：3308 cat \u003e /usr/local/mysql/etc/my-3308.cnf \u003c\u003cEOF [client] port = 3308 socket = /data/mysql/3307/mysql.sock [mysqld] user = mysql port = 3308 basedir = /usr/local/mysql datadir = /data/mysql/3308 socket = /data/mysql/3308/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/3308/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/3308/mybinlog #sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 EOF ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:4:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"多实例初始化 [root@10-13-90-34 3307]# mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3307 [root@10-13-90-34 3307]# mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/3308 --defaults-file= 参数必须放在最前面或者初始化不会成功 ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:5:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"使用 systemd 管理多实例服务 3307 cat \u003e /usr/lib/systemd/system/mysqld-3307.service \u003c\u003c EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf LimitNOFILE = 5000 EOF 3308 cat \u003e /usr/lib/systemd/system/mysqld-3308.service \u003c\u003c EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf LimitNOFILE = 5000 EOF ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:6:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"启动 MySQL 多实例 [root@10-13-90-34 ~]# systemctl start mysqld-3307 [root@10-13-90-34 ~]# systemctl start mysqld-3308 [root@10-13-90-34 ~]# systemctl status mysqld-3307.service ● mysqld-3307.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld-3307.service; disabled; vendor preset: disabled) Active: active (running) since 四 2020-12-24 13:44:18 CST; 14s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 40145 (mysqld) CGroup: /system.slice/mysqld-3307.service └─40145 /usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3307.cnf 12月 24 13:44:18 10-13-90-34 systemd[1]: Started MySQL Server. [root@10-13-90-34 ~]# systemctl status mysqld-3308.service ● mysqld-3308.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld-3308.service; disabled; vendor preset: disabled) Active: active (running) since 四 2020-12-24 13:44:20 CST; 16s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 40179 (mysqld) CGroup: /system.slice/mysqld-3308.service └─40179 /usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my-3308.cnf 12月 24 13:44:20 10-13-90-34 systemd[1]: Started MySQL Server. ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:7:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"连接 MySQL 多实例 [root@10-13-90-34 ~]# mysql -S /data/mysql/3307/mysql.sock ","date":"2020-12-24","objectID":"/posts/mysql-multi-instance/:8:0","tags":["mysql"],"title":"安装 MySQL 多实例","uri":"/posts/mysql-multi-instance/"},{"categories":["mysql"],"content":"下载 MySQL 5.7 二进制包 [root@10-13-90-34 src]# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz ","date":"2020-12-20","objectID":"/posts/mysql-install/:1:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"解压并建立软链接(/usr/local/mysql) [root@10-13-90-34 src]# tar xzf mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ [root@10-13-90-34 local]# ln -s /usr/local/mysql-5.7.28-linux-glibc2.12-x86_64/ /usr/local/mysql ","date":"2020-12-20","objectID":"/posts/mysql-install/:2:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"配置环境变量 echo 'export PATH=/usr/local/mysql/bin:$PATH' \u003e /etc/profile.d/mysql.sh source /etc/profile ","date":"2020-12-20","objectID":"/posts/mysql-install/:3:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"初始化前准备工作 # 安装依赖 [root@10-13-90-34 mysql]# yum install libaio # 创建 mysql 用户 [root@10-13-90-34 mysql]# useradd -r -s /sbin/nologin mysql # 创建数据存储目录 [root@10-13-90-34 mysql]# mkdir -p /data/mysql [root@10-13-90-34 mysql]# chown -R mysql.mysql /data/mysql/ # 生成配置文件 my.cnf [root@10-13-90-34 mysql]# mkdir etc [root@10-13-90-34 mysql]# vim etc/my.cnf [client] port = 3306 socket = /data/mysql/3306/mysql.sock [mysqld] user = mysql port = 3306 basedir = /usr/local/mysql datadir = /data/mysql socket = /data/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 log-error = /data/mysql/error.log server-id = 1 # binlog 配置 log-bin = /data/mysql/mybinlog sync_binlog = 1 binlog_cache_size = 4M max_binlog_cache_size = 2G max_binlog_size = 1G expire_logs_days = 7 binlog_format = row binlog_checksum = 1 # 事务模式 transaction_isolation = REPEATABLE-READ # InnoDB 配置 innodb_buffer_pool_size = 128M innodb_buffer_pool_instances = 4 innodb_data_file_path = ibdata1:1G:autoextend innodb_flush_log_at_trx_commit = 0 ","date":"2020-12-20","objectID":"/posts/mysql-install/:4:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"初始化数据库 初始化参数 --initialize # 初始化时会提供12位的 root 临时密码，使用mysql前必须重置此密码，密码管理使用严格模式。 --initialize-insecure # 不会为 root 用户生成临时密码 [root@10-13-90-34 mysql]# mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql ","date":"2020-12-20","objectID":"/posts/mysql-install/:5:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"管理 mysql 服务 使用自带脚本 MySQL 默认提供服务管理脚本 support-files/mysql.server 使用方法 [root@10-13-90-34 mysql]# cp support-files/mysql.server /etc/init.d/mysqld [root@10-13-90-34 mysql]# /etc/init.d/mysqld start 使用 systemd 管理 MySQL 服务 [root@10-13-90-34 mysql]# vim /usr/lib/systemd/system/mysqld.service [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/usr/local/mysql/bin/mysqld --defaults-file=/usr/local/mysql/etc/my.cnf LimitNOFILE = 5000 以上方法二选一即可 ","date":"2020-12-20","objectID":"/posts/mysql-install/:6:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["mysql"],"content":"扩展部署多实例 MySQL 方法1: 多份 MySQL 程序，不同的配置文件，不同的数据存储目录 方法2: 一份 MySQL 程序，不同的配置文件，不同的数据存储目录 （推荐） 实现方法 在 MySQL 服务启动命令 mysqld 使用参数（–defaults-file）指定默认使用的配置文件(my.cnf)即可实现，数据存储目录在配置文件中配置. 查看 mysqld 参数方法: mysqld --verbose --help ","date":"2020-12-20","objectID":"/posts/mysql-install/:7:0","tags":["mysql"],"title":"MySQL 5.7 安装","uri":"/posts/mysql-install/"},{"categories":["centos","php"],"content":"众所周知，PHP 是 LAMP 应用程序（WordPress，Joomla，Drupal和Media Wiki等）中最重要的部分。 现在，大多数这些应用程序都需要PHP 7进行安装和配置。 PHP 7.x的主要优点在于，它可以更快地加载Web应用程序，并且消耗更少的服务器资源（例如CPU和RAM）。 默认情况下，PHP 5.4 在 CentOS 7 和 RHEL 7 YUM 存储库中可用。 在本文中，我们将演示如何在 CentOS 7 和 RHEL 7 服务器上安装最新版本的 PHP。 文档出处: https://www.linuxtechi.com/install-php-7-centos-7-rhel-7-server/ ","date":"2019-08-26","objectID":"/posts/php-install/:0:0","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"CentOS 7服务器上PHP 7.0、7.1和7.2的安装步骤 ","date":"2019-08-26","objectID":"/posts/php-install/:1:0","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"1）安装 yum-utils 并启用 EPEL 存储库 登录到您的服务器并使用以下 yum 命令安装 yum-utils 并启用 epel 存储库 [root@linuxtechi ~]# yum install epel-release yum-utils -y ","date":"2019-08-26","objectID":"/posts/php-install/:1:1","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"2) 使用yum命令下载并安装remirepo [root@linuxtechi ~]# yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm ","date":"2019-08-26","objectID":"/posts/php-install/:1:2","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"3) 根据您的要求，配置PHP 7.x存储库 要配置PHP 7.0存储库，请使用以下命令， [root@linuxtechi ~]# yum-config-manager --enable remi-php70 要配置PHP 7.1存储库，请使用以下命令， [root@linuxtechi ~]# yum-config-manager --enable remi-php71 要配置PHP 7.2 存储库，请使用以下命令， [root@linuxtechi ~]# yum-config-manager --enable remi-php72 ","date":"2019-08-26","objectID":"/posts/php-install/:1:3","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"4) 安装PHP 7.2及其依赖项。 在本教程中，我将安装最新版本的PHP 7.2及其模块，在yum命令下运行 [root@linuxtechi ~]# yum install php php-common php-opcache php-mcrypt php-cli php-gd php-curl php-mysql -y 注意：要搜索所有PHP模块，请使用以下命令： [root@linuxtechi ~]# yum search php | more ","date":"2019-08-26","objectID":"/posts/php-install/:1:4","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"5）验证PHP版本 在步骤 4 中安装完所有PHP 7.2及其依赖项之后，请使用以下命令验证已安装的PHP版本， [root@linuxtechi ~]# php -v PHP 7.2.7 (cli) (built: Jun 20 2018 08:21:26) ( NTS ) Copyright (c) 1997-2018 The PHP Group Zend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.2.7, Copyright (c) 1999-2018, by Zend Technologies ","date":"2019-08-26","objectID":"/posts/php-install/:1:5","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"PHP 7.x在RHEL 7 Server上的安装步骤 ","date":"2019-08-26","objectID":"/posts/php-install/:2:0","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"1）启用EPEL，RHEL 7 Server可选存储库并安装remirepo rpm 登录到RHEL 7 Server并依次运行以下命令以启用EPEL存储库，安装remirepo并启用RHEL 7 Server可选存储库 [root@linuxtechi ~]# rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm [root@linuxtechi ~]# wget http://rpms.remirepo.net/enterprise/remi-release-7.rpm [root@linuxtechi ~]# rpm -Uvh remi-release-7.rpm epel-release-latest-7.noarch.rpm [root@linuxtechi ~]# subscription-manager repos --enable=rhel-7-server-optional-rpms ","date":"2019-08-26","objectID":"/posts/php-install/:2:1","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"2）配置PHP 7.x存储库 [root@linuxtechi ~]# yum install yum-utils [root@linuxtechi ~]# yum-config-manager --enable remi-php72 ","date":"2019-08-26","objectID":"/posts/php-install/:2:2","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"3）安装PHP 7.2及其依赖项 [root@linuxtechi ~]# yum install php php-common php-opcache php-mcrypt php-cli php-gd php-curl php-mysql -y ","date":"2019-08-26","objectID":"/posts/php-install/:2:3","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["centos","php"],"content":"4）验证PHP版本 [root@linuxtechi ~]# php -v ","date":"2019-08-26","objectID":"/posts/php-install/:2:4","tags":["php"],"title":"如何在 CentOS 7 和 RHEL 7 服务器上安装 PHP 7.x","uri":"/posts/php-install/"},{"categories":["php"],"content":"redis 插件 wget http://pecl.php.net/get/redis-4.2.0.tgz tar xzf redis-4.2.0.tgz cd redis-4.2.0 /usr/local/php/bin/phpize ./configure --with-php-config=/usr/local/php/bin/php-config make \u0026\u0026 make install cat \u003e\u003e /etc/php/php.ini \u003c\u003c EOF [redis] extension=redis.so EOF ","date":"2019-08-26","objectID":"/posts/php-plugin/:1:0","tags":["php"],"title":"PHP 插件安装","uri":"/posts/php-plugin/"},{"categories":["php"],"content":"RabbitMQ 插件 安装 rabbitmq-c wget -O rabbitmq-c-0.9.0.tar.gz https://github.com/alanxz/rabbitmq-c/archive/v0.9.0.tar.gz tar xzf rabbitmq-c-0.9.0.tar.gz cd rabbitmq-c-0.9.0 mkdir build \u0026\u0026 cd build cmake -DCMAKE_INSTALL_PREFIX=/usr/local/rabbitmq-c .. cmake --build . --target install 到这里就已经安装完成了。不过这里有一个坑。你可以看一下/usr/local/rabbitmq-c下的目录只有include和lib64。因为后面编译安装amqp扩展的时候系统会到/usr/local/rabbitmq-c/lib目录下搜索依赖库，导致错误。所以这里需要加一步 cd /usr/local/rabbitmq-c ln -s lib64 lib 安装 amqp wget https://pecl.php.net/get/amqp-1.9.4.tgz tar xzf amqp-1.9.4.tgz cd amqp-1.9.4 /usr/local/php/bin/phpize ./configure --with-php-config=/usr/local/php/bin/php-config / --with-amqp --with-librabbitmq-dir=/usr/local/rabbitmq-c make \u0026\u0026 make install cat \u003e\u003e /etc/php/php.ini \u003c\u003c EOF [amqp] extension=amqp.so EOF 安装 mcrypt yum install libmcrypt libmcrypt-devel mcrypt mhash wget https://pecl.php.net/get/mcrypt-1.0.2.tgz tar xzf mcrypt-1.0.2.tgz cd mcrypt-1.0.2 /usr/local/php/bin/phpize ./configure --with-php-config=/usr/local/php/bin/php-config make \u0026\u0026 make install cat \u003e\u003e /etc/php/php.ini \u003c\u003c EOF [amqp] extension=mcrypt.so EOF ","date":"2019-08-26","objectID":"/posts/php-plugin/:2:0","tags":["php"],"title":"PHP 插件安装","uri":"/posts/php-plugin/"},{"categories":["php"],"content":"环境准备 准备编译环境 yum install -y gcc gcc-c++ make cmak autoconf 安装相关依赖 yum install -y libmcrypt libmcrypt-devel mcrypt mhash yum install -y gmp-devel libxml2-devel openssl-devel bzip2-devel / libcurl-devel libjpeg-devel libpng-devel freetype-devel / libmcrypt-devel readline-devel libxslt-devel libicu-devel / gettext-devel libc-client-devel pam-devel ","date":"2019-08-26","objectID":"/posts/php-install/:1:0","tags":["php"],"title":"PHP 源码编译安装","uri":"/posts/php-install/"},{"categories":["php"],"content":"编译安装 ./configure --prefix=/usr/local/php / --sysconfdir=/etc/php / --with-config-file-path=/etc/php / --with-config-file-scan-dir=/etc/php/conf.d / --enable-fpm / --with-fpm-user=www / --with-fpm-group=www / --enable-mysqlnd / --with-mysqli=mysqlnd / --with-pdo-mysql=mysqlnd / --with-iconv-dir / --with-freetype-dir / --with-jpeg-dir / --with-png-dir / --with-zlib / --with-curl / --with-gettext / --with-imap / --enable-exif / --with-libxml-dir / --enable-xml / --disable-rpath / --enable-bcmath / --enable-shmop / --enable-sysvsem / --enable-inline-optimization / --enable-mbregex / --enable-mbstring / --enable-intl / --enable-pcntl / --enable-ftp / --with-gd / --with-openssl / --with-mhash / --enable-pcntl / --enable-sockets / --with-xmlrpc / --enable-zip / --enable-soap / --with-gettext / --enable-opcache / --with-xsl make make install ","date":"2019-08-26","objectID":"/posts/php-install/:2:0","tags":["php"],"title":"PHP 源码编译安装","uri":"/posts/php-install/"},{"categories":["php"],"content":"配置 PHP MemTotal=`free -m | grep Mem | awk '{print $2}'` /bin/cp -f php.ini-production /etc/php/php.ini sed -i 's/post_max_size =.*/post_max_size = 50M/g' /etc/php/php.ini sed -i 's/upload_max_filesize =.*/upload_max_filesize = 50M/g' /etc/php/php.ini sed -i 's/;date.timezone =.*/date.timezone = PRC/g' /etc/php/php.ini sed -i 's/short_open_tag =.*/short_open_tag = On/g' /etc/php/php.ini sed -i 's/;cgi.fix_pathinfo=.*/cgi.fix_pathinfo=0/g' /etc/php/php.ini sed -i 's/max_execution_time =.*/max_execution_time = 300/g' /etc/php/php.ini sed -i 's/disable_functions =.*/disable_functions = passthru,exec,system,chroot,chgrp,chown,shell_exec,proc_open,proc_get_status,popen,ini_alter,ini_restore,dl,openlog,syslog,readlink,symlink,popepassthru,stream_socket_server/g' /etc/php/php.ini /bin/cp -f /etc/php/php-fpm.conf.default /etc/php/php-fpm.conf sed -i \"s#^;pid.*#pid = /var/run/php-fpm.pid#\" /etc/php/php-fpm.conf sed -i \"s#^;error_log.*#error_log = /var/log/php-fpm.log#\" /etc/php/php-fpm.conf sed -i \"s#^;log_level.*#log_level = notice#\" /etc/php/php-fpm.conf cat \u003e/etc/php/php-fpm.d/www.conf\u003c\u003cEOF [www] listen = /var/run/php-cgi.sock listen.backlog = -1 listen.allowed_clients = 127.0.0.1 listen.owner = www listen.group = www listen.mode = 0666 user = www group = www pm = dynamic pm.max_children = 10 pm.start_servers = 2 pm.min_spare_servers = 1 pm.max_spare_servers = 6 request_terminate_timeout = 100 request_slowlog_timeout = 0 slowlog = /var/log/slow.log EOF if [[ ${MemTotal} -gt 1024 \u0026\u0026 ${MemTotal} -le 2048 ]]; then sed -i \"s#pm.max_children.*#pm.max_children = 20#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.start_servers.*#pm.start_servers = 10#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.min_spare_servers.*#pm.min_spare_servers = 10#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.max_spare_servers.*#pm.max_spare_servers = 20#\" /etc/php/php-fpm.d/www.conf elif [[ ${MemTotal} -gt 2048 \u0026\u0026 ${MemTotal} -le 4096 ]]; then sed -i \"s#pm.max_children.*#pm.max_children = 40#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.start_servers.*#pm.start_servers = 20#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.min_spare_servers.*#pm.min_spare_servers = 20#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.max_spare_servers.*#pm.max_spare_servers = 40#\" /etc/php/php-fpm.d/www.conf elif [[ ${MemTotal} -gt 4096 \u0026\u0026 ${MemTotal} -le 8192 ]]; then sed -i \"s#pm.max_children.*#pm.max_children = 60#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.start_servers.*#pm.start_servers = 30#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.min_spare_servers.*#pm.min_spare_servers = 30#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.max_spare_servers.*#pm.max_spare_servers = 60#\" /etc/php/php-fpm.d/www.conf elif [[ ${MemTotal} -gt 8192 ]]; then sed -i \"s#pm.max_children.*#pm.max_children = 80#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.start_servers.*#pm.start_servers = 40#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.min_spare_servers.*#pm.min_spare_servers = 40#\" /etc/php/php-fpm.d/www.conf sed -i \"s#pm.max_spare_servers.*#pm.max_spare_servers = 80#\" /etc/php/php-fpm.d/www.conf fi 复制服务启动文件 CentOS 6.x /bin/cp -f sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm chmod +x /etc/init.d/php-fpm sed -i \"s#php_fpm_PID=.*#php_fpm_PID=/var/run/php-fpm.pid#\" /etc/init.d/php-fpm CentOS 7.x /bin/cp -f sapi/fpm/php-fpm.service /usr/lib/systemd/system/php-fpm.service ","date":"2019-08-26","objectID":"/posts/php-install/:3:0","tags":["php"],"title":"PHP 源码编译安装","uri":"/posts/php-install/"},{"categories":["rabbitmq"],"content":"由于 rabbitmq 是 erlang 开发的所以依赖 erlang, 准备三台服务器并配置好主机名, 并以下信息写入三台服务器的 hosts. mq1 192.168.1.11 mq2 192.168.1.12 mq3 192.168.1.13 ","date":"2019-08-12","objectID":"/posts/rabbitmq/:0:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"1. 配置 erlang, rabbitmq YUM 仓库 curl -s https://packagecloud.io/install/repositories/rabbitmq/erlang/script.rpm.sh | bash curl -s https://packagecloud.io/install/repositories/rabbitmq/rabbitmq-server/script.rpm.sh | bash ","date":"2019-08-12","objectID":"/posts/rabbitmq/:1:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"2. 安装 eple 扩展 yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm ","date":"2019-08-12","objectID":"/posts/rabbitmq/:2:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"3. 安装 rabbitmq yum install -y erlang rabbitmq-server ","date":"2019-08-12","objectID":"/posts/rabbitmq/:3:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"4. 开启 rabbitmq WEB 管理 rabbitmq-plugins enable rabbitmq_management ","date":"2019-08-12","objectID":"/posts/rabbitmq/:4:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"5. 后台启动 rabbitmq rabbitmq-server -detached ","date":"2019-08-12","objectID":"/posts/rabbitmq/:5:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"6. 配置 rabbitmq 集群 rabbitmqctl stop_app rabbitmqctl join_cluster --disc rabbit@mq1 rabbitmqctl start_app –ram 为内存节点 ","date":"2019-08-12","objectID":"/posts/rabbitmq/:6:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"7. 修改 rabbitmq 用户 rabbitmqctl add_user jpuser 'G6JzIC3ifipGIMa' rabbitmqctl set_user_tags jpuser administrator rabbitmqctl set_permissions -p '/' jpuser '.*' '.*' '.*' ","date":"2019-08-12","objectID":"/posts/rabbitmq/:7:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rabbitmq"],"content":"8. 配置为高可用集群 rabbitmqctl set_policy ha-all-queue \"^\" '{\"ha-mode\":\"all\",\"ha-sync-mode\":\"automatic\"}' 设置 policy，以 ha. 开头的队列将会被镜像到集群其他所有节点,一个节点挂掉然后重启后会自动同步队列消息（生产环境采用这个方式） ","date":"2019-08-12","objectID":"/posts/rabbitmq/:8:0","tags":["rabbitmq"],"title":"部署 rabbitmq 集群","uri":"/posts/rabbitmq/"},{"categories":["rsync"],"content":"任务需求 需要将一台服务器指定目录中的文件实时同步到另一台服务器上，目录路径 /data/www/pic, 此时我们通过 Rsync + Inotify 来实现。 服务器如下 192.168.145.131: 文件发布推送，客户端 192.168.145.132: 文件同步接收，服务端 由于我们需要将 192.168.145.131 上的文件实时同步至 192.168.145.132 服务器，所以 192.168.145.132 为服务端，192.168.145.131 为客户端 ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:1:0","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"配置服务端 ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:2:0","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"安装 rsync yum install rsync ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:2:1","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"配置 rsync 编辑文件 /etc/rsyncd.conf，写入如下配置信息 # /etc/rsyncd: configuration file for rsync daemon mode # See rsyncd.conf man page for more options. # configuration example: uid = nobody gid = nobody use chroot = yes max connections = 10 pid file = /var/run/rsyncd.pid # exclude = lost+found/ transfer logging = yes timeout = 900 fake super = yes # 不要漏了这项配置，否则会报 Operation not permitted 错误 # ignore nonreadable = yes # dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 [www] # 同步文件存放目录 path = /data/www/ comment = Files synced in real time ignore errors read only = no write only = no # 允许连接的IP地址 hosts allow = 192.168.145.131 # 拒绝连接的 IP 地址，* 号匹配所有 hosts deny = * list = false # 同步过来文件的属主和属组，记得修改同步目录的的权限： chown -R www.www /data/www uid = www gid = www # 客户端连接使用的用户名 auth users = www # 客户端连接使用的用户名及密码，格式 \u003cusername\u003e:\u003cpassword\u003e 以普通文本文件存放，注意权限 secrets file = /etc/www.pass 生成 rsync 客户端连接使用的验证文件 echo 'www:123456' \u003e /etc/www.pass chmod 600 /etc/www.pass 创建同步目录，并授权（根据配置文件的批定的uid，gid）于相应的权限 mkdir -p /data/www chown -R www.www /data/www ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:2:2","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"启动 rsync systemctl start rsyncd.service systemctl enable rsyncd.service ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:2:3","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"配置客户端 由于客户端是文件推送端，需要实时检测文件目录的变化并同步至服务端，所以需要安装 rsync 和 inotify-tools 工具 ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:0","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"安装 rsync yum install rsync ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:1","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"安装 inotify-tools 由于 inotify 特性需要 Linux 内核的支持，在安装 inotify-tools 前要先确认 Linux 系统内核是否达到了 2.6.13 以上，如果 Linux 内核低于 2.6.13 版本，就需要重新编译内核加入 inotify 的支持，也可以用如下方法判断，内核是否支持 inotify [root@localhost ~]# uname -r 3.10.0-862.el7.x86_64 [root@localhost ~]# ll /proc/sys/fs/inotify total 0 -rw-r--r-- 1 root root 0 Jun 12 21:10 max_queued_events -rw-r--r-- 1 root root 0 Jun 12 21:10 max_user_instances -rw-r--r-- 1 root root 0 Jun 12 21:10 max_user_watches 如果有上面三项输出，表示系统已经默认支持 inotify，接着就可以开始安装 inotify-tools 了 yum install inotify-tools inotify-tools 安装完成后，会生成 inotifywait 和 inotifywatch 两个指令。 inotifywait 用于等待文件或文件集上的一个特定事件，它可以监控任何文件和目录设置，并且可以递归地监控整个目录树。 inotifywatch 用于收集被监控的文件系统统计数据，包括每个 inotify 事件发生多少次等信息。 ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:2","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"inotify 相关参数 inotify 定义了下列的接口参数，可以用来限制 inotify 消耗 kernel memory 的大小。由于这些参数都是内存参数，因此，可以根据应用需求，实时的调节其大小。下面分别做简单介绍。 /proc/sys/fs/inotify/max_queued_evnets：表示调用 inotify_init 时分配给 inotify instance 中可排队的 event 的数目的最大值，超出这个值的事件被丢弃，但会触发 IN_Q_OVERFLOW 事件。 /proc/sys/fs/inotify/max_user_instances: 表示每一个 real user ID可创建的 inotify instatnces 的数量上限。 /proc/sys/fs/inotify/max_user_watches: 表示每个 inotify instatnces 可监控的最大目录数量。 如果监控的文件数目巨大，需要根据情况，适当增加此值的大小, 例如: echo 30000000 \u003e /proc/sys/fs/inotify/max_user_watches ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:3","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"inotifywait 相关参数 inotifywait 是一个监控等待事件，可以配合 shell 脚本使用它，下面介绍一下常用的一些参数： -m， 即 --monitor，表示始终保持事件监听状态。 -r， 即 --recursive，表示递归查询目录。 -q， 即 --quiet，表示打印出监控事件。 -e， 即 --event，通过此参数可以指定要监控的事件，常见的事件有 modify、delete、create、attrib 等。 更详细的请参看 man inotifywait ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:4","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"配置 inotify 在 /data/scripts 目录中写入以下脚本文件，文件名为 inotify-rsync.sh #!/bin/bash # #*************************************************************************** # Author: liwanggui # Date: 2017-03-24 # FileName: inotify-rsync.sh # Description: Real-time file synchronization # Copyright (C): 2021 All rights reserved #*************************************************************************** # host=192.168.145.132 src=/data/www/pic # rsyncd.conf 配置项名称 dst=www # 连接验证的用户名 user=www /usr/bin/inotifywait -mrq --timefmt '%y/%m/%d %H:%M:%S ' --format '%T %w%f %e ' -e close_write,delete,create,attrib $src \\ | while read files do /usr/bin/rsync -vzrtopg --delete --progress --password-file=/etc/www.pass $src $user@$host::$dst echo \"${files}was rsynced\" \u0026\u003e\u003e/tmp/rsync.log done ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:5","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["rsync"],"content":"启动测试 使用 nohup 将脚本以守护进程的方式的运行在后台 nohup bash /data/scripts/inotify-rsync.sh \u0026 在 /data/www/pic 目录生成一些文件 cp /etc/yum.repos.d/* /data/www/pic/ 查看脚本日志, 使用 cat /tmp/rsync.log 命令查看 17/03/24 22:15:43 /data/www/pic/CentOS-Base.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Base.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-CR.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-CR.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Debuginfo.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Debuginfo.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-fasttrack.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-fasttrack.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Media.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Media.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Sources.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Sources.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Vault.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/CentOS-Vault.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/epel.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/epel.repo CLOSE_WRITE,CLOSE was rsynced 17/03/24 22:15:43 /data/www/pic/epel-testing.repo CREATE was rsynced 17/03/24 22:15:43 /data/www/pic/epel-testing.repo CLOSE_WRITE,CLOSE was rsynced 在服务端查看文件同步情况, ls -l /data/www/pic -rw-r--r-- 1 www www 1572 Jun 12 22:15 CentOS-Base.repo -rw-r--r-- 1 www www 1309 Jun 12 22:15 CentOS-CR.repo -rw-r--r-- 1 www www 649 Jun 12 22:15 CentOS-Debuginfo.repo -rw-r--r-- 1 www www 314 Jun 12 22:15 CentOS-fasttrack.repo -rw-r--r-- 1 www www 630 Jun 12 22:15 CentOS-Media.repo -rw-r--r-- 1 www www 1331 Jun 12 22:15 CentOS-Sources.repo -rw-r--r-- 1 www www 4768 Jun 12 22:15 CentOS-Vault.repo -rw-r--r-- 1 www www 951 Jun 12 22:15 epel.repo -rw-r--r-- 1 www www 1050 Jun 12 22:15 epel-testing.repo 我们可以看到文件同步过来了，而且文件 UID 和 GID 也是配置文件设置的 www 参考网址：http://ixdba.blog.51cto.com/2895551/580280 ","date":"2017-03-24","objectID":"/posts/rsync-inotify/:3:6","tags":["rsync","inotify"],"title":"通过 rsync + inotify 搭建实时文件同步系统","uri":"/posts/rsync-inotify/"},{"categories":["dhcp"],"content":"原因：由于公司使用 CentOS6.5 充当网关，为了避免麻烦给同事们一个个地去配置 IP 地址， 所以决定安装使用 dhcp 来自动分配 IP（之前是真一个一个去给他们配置 IP 地址，真他妈的累，烦，卖力而且不讨好~~） ","date":"2017-03-15","objectID":"/posts/dhcp/:0:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"1.安装DHCP [root@localhost ~]# yum install -y dhcp ","date":"2017-03-15","objectID":"/posts/dhcp/:1:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"2.配置dhcpd.conf [root@localhost ~]# vim /etc/dhcp/dhcpd.conf # # DHCP Server Configuration file. # see /usr/share/doc/dhcp*/dhcpd.conf.sample # see 'man 5 dhcpd.conf' # # 全局配置 # 设置客户端域名 option domain-name \"wgsc.tv\"; option domain-name-servers 192.168.1.60, 192.168.1.61; # 默认租约12h default-lease-time 43200; # 最大租约24h max-lease-time 86400; # 不要ddns设定 ddns-update-style none; # Use this to send dhcp log messages to a different log file (you also # have to hack syslog.conf to complete the redirection). log-facility local7; # 设置DHCP子网段 subnet 192.168.1.0 netmask 255.255.255.0 { range 192.168.1.181 192.168.1.230; option routers 192.168.1.1; } ################### 服务器(设备)静态IP配置 ################### # Java_bug_6 host device_1 { hardware ethernet fc:aa:14:3a:42:f0; fixed-address 192.168.1.6; } # k3-25 host device_2 { hardware ethernet fc:aa:14:48:62:98; fixed-address 192.168.1.25; } # ....省略一大串 ","date":"2017-03-15","objectID":"/posts/dhcp/:2:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"3.配置dhcp监听接口 [root@localhost ~]# vim /etc/sysconfig/dhcpd # Command line options here DHCPDARGS=eth0 Tips: CentOS7 以后不需要设置 ","date":"2017-03-15","objectID":"/posts/dhcp/:3:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"4.关闭iptables [root@localhost ~]# /etc/inin.d/iptables stop [root@localhost ~]# chkconfig iptables off ","date":"2017-03-15","objectID":"/posts/dhcp/:4:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"5.启动dhcp服务 [root@localhost ~]# /etc/init.d/dhcpd start 注： 启动日志信息可以查看 /var/log/message 租约信息可查看 /var/lib/dhcpd/dhcpd.leases ","date":"2017-03-15","objectID":"/posts/dhcp/:5:0","tags":["dhcp"],"title":"配置 DHCP 服务 实现 ip 地址自动分配","uri":"/posts/dhcp/"},{"categories":["dhcp"],"content":"有时我们的网络中可能划分了好几个网段，需要同时为多个网段内的主机分配 ip 地址，这时候就需要用到 dhcp 中继了。 在连接多个子网的路由器上启用 dhcp 中继功能允许有针对性地转发 dhcp 广播。 学过网络的都应该知道不同子网段是不允许广播通过的。所有 dhcp 是无法正常在多网段内提供服务的 安装好 dhcpd 软件，通过修改 /etc/sysconfig/dhcrelay 文件来配置 dhcp 中继。 具体操作步骤如下： 1. 开启服务器的路由转发功能。 vim /etc/sysctl.conf net.ipv4.ip_forward = 1 sysctl -p 2. 设置允许dhcp中继数据的接口及dhcp服务器的ip地址 vim /etc/sysconfig/dhcrelay INTERFACES=\"eth0 eth1\" DHCPSERVERS=\"192.168.1.2\" 3. 启动 dhcrelay 中继服务程序 service dhcrelay start chkconfig --level 35 dhcrelay on ","date":"2017-03-14","objectID":"/posts/dhcp-relay/:0:0","tags":["dhcp"],"title":"配置 DHCP 中继为多个网段分配 ip 地址","uri":"/posts/dhcp-relay/"},{"categories":["vpn"],"content":"实验环境 VPC1:192.168.1.1 VPC2:192.168.2.2 ","date":"2015-09-24","objectID":"/posts/openswan/:1:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"安装 openswan [root@wglee ~]# yum install openswan ","date":"2015-09-24","objectID":"/posts/openswan/:2:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"编辑 /etc/ipsec.conf 文件，启用 /etc/ipsec.d/*.conf [root@wglee ~]# sudo vi /etc/ipsec.conf # /etc/ipsec.conf - Openswan IPsec configuration file # # Manual: ipsec.conf.5 # # Please place your own config files in /etc/ipsec.d/ ending in .conf version 2.0 # conforms to second version of ipsec.conf specification # basic configuration config setup # Debug-logging controls: \"none\" for (almost) none, \"all\" for lots. # klipsdebug=none # plutodebug=\"control parsing\" # For Red Hat Enterprise Linux and Fedora, leave protostack=netkey protostack=netkey nat_traversal=yes virtual_private= oe=off # Enable this if you see \"failed to find any available worker\" # nhelpers=0 #You may put your configuration (.conf) file in the \"/etc/ipsec.d/\" and uncomment this. include /etc/ipsec.d/*.conf ","date":"2015-09-24","objectID":"/posts/openswan/:3:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"在 /etc/ipsec.d 目录创建以下文件 配置VPC1 [root@wglee ~]# sudo vi /etc/ipsec.d/vpc1-to-vpc2.conf conn vpc1-to-vpc2 type=tunnel authby=secret left=%defaultroute leftid=\u003cVPC1的外网IP\u003e leftnexthop=%defaultroute leftsubnet=\u003cVPC1 子网地址\u003e right=\u003cVPC2的外网IP\u003e rightsubnet=\u003cVPC2 子网地址\u003e pfs=yes auto=start [root@wglee ~]# sudo vi /etc/ipsec.d/vpc1-to-vpc2.secrets \u003cVPC1 子网地址\u003e \u003cVPC1 子网地址\u003e: PSK \"Put a Preshared Key here!!\" 配置VPC2 [root@wglee ~]# sudo vi /etc/ipsec.d/vpc2-to-vpc1.conf conn vpc2-to-vpc1 type=tunnel authby=secret left=%defaultroute leftid=\u003cVPC2的外网IP\u003e leftnexthop=%defaultroute leftsubnet=\u003cVPC2 的子网地址\u003e right=\u003cEIP1\u003e rightsubnet=\u003cVPC1 的子网地址\u003e pfs=yes auto=start [root@wglee ~]# sudo vi /etc/ipsec.d/vpc2-to-vpc1.secrets \u003cVPC2 的子网地址\u003e \u003cVPC1 的子网地址\u003e: PSK \"Put a Preshared Key here!!\" ","date":"2015-09-24","objectID":"/posts/openswan/:4:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"启动 IPSec/Openswan [root@wglee ~]# sudo service ipsec start # Configure IPSec/Openswan to always start on boot [root@wglee ~]# sudo chkconfig ipsec on ","date":"2015-09-24","objectID":"/posts/openswan/:5:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"编辑 /etc/sysctl.conf [root@wglee ~]# sudo vi /etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 ","date":"2015-09-24","objectID":"/posts/openswan/:6:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"重启网络 [root@wglee ~]# service network restart ","date":"2015-09-24","objectID":"/posts/openswan/:7:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":["vpn"],"content":"检查 VPN 状态 #下面的命令可以在检查或故障排除VPN状态有所帮助： [root@wglee ~]# sudo ipsec verify #会检查所需的OpenSWAN的服务状态正常运行 [root@wglee ~]# sudo service ipsec status #检查OpenSWAN服务的状态和VPN隧道 来源： https://aws.amazon.com/articles/5472675506466066 ","date":"2015-09-24","objectID":"/posts/openswan/:8:0","tags":["openswan"],"title":"配置 openswan ipsecvpn","uri":"/posts/openswan/"},{"categories":null,"content":" 冬夜读书示子聿 -宋.陆游 古人学问无遗力，少壮工夫老始成。 纸上得来终觉浅，绝知此事要躬行。 有勇气去改变可以改变的事情；有胸怀去接纳不可以改变的事情；用智慧去区分两者的不同。 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"}]